# PCDN vs Normal Traffic Classification using Decision Tree
# ä½¿ç”¨ ip.proto å’Œ tcp.srcport ç‰¹å¾è¿›è¡Œå†³ç­–æ ‘äºŒåˆ†ç±»

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
from sklearn.preprocessing import StandardScaler
import os
import glob
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®å›¾è¡¨æ ·å¼
try:
    plt.style.use('seaborn-v0_8')
except:
    plt.style.use('default')
sns.set_palette("husl")

print("ğŸŒ³ å¼€å§‹PCDNæµé‡å†³ç­–æ ‘åˆ†ç±»ä»»åŠ¡")
print("ğŸ“‹ ä½¿ç”¨ç‰¹å¾: ip.proto, tcp.srcport")
print("ğŸ¯ æ¨¡å‹: Decision Tree Classifier")
print("=" * 60)

# å®šä¹‰æ•°æ®è·¯å¾„
base_path = "pcdn_32_pkts_2class_feature_enhance_v17.4_dataset"
train_path = os.path.join(base_path, "Training_set")
val_path = os.path.join(base_path, "Validation_set") 
test_path = os.path.join(base_path, "Testing_set")

# é€‰æ‹©çš„ç‰¹å¾
selected_features = ['ip.proto', 'tcp.srcport']

def load_data_from_directory(directory_path, label):
    """åŠ è½½æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = glob.glob(os.path.join(directory_path, "*.csv"))
    dataframes = []
    
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            df['label'] = label  # æ·»åŠ æ ‡ç­¾
            df['source_file'] = os.path.basename(file)  # è®°å½•æ¥æºæ–‡ä»¶
            dataframes.append(df)
            print(f"âœ… åŠ è½½æ–‡ä»¶: {file} (æ ·æœ¬æ•°: {len(df)})")
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file}, é”™è¯¯: {e}")
    
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"ğŸ“Š {directory_path} æ€»æ ·æœ¬æ•°: {len(combined_df)}")
        return combined_df
    else:
        print(f"âš ï¸  {directory_path} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return pd.DataFrame()

# æ£€æŸ¥æ˜¯å¦æœ‰ç©ºçš„æ•°æ®é›†
def safe_concat(dataframes, set_name):
    """å®‰å…¨åˆå¹¶æ•°æ®é›†ï¼Œå¤„ç†ç©ºæ•°æ®é›†çš„æƒ…å†µ"""
    non_empty_dfs = [df for df in dataframes if not df.empty]
    if not non_empty_dfs:
        print(f"âš ï¸  {set_name} æ²¡æœ‰æœ‰æ•ˆæ•°æ®!")
        return pd.DataFrame()
    return pd.concat(non_empty_dfs, ignore_index=True)

def preprocess_features(data, features):
    """é¢„å¤„ç†ç‰¹å¾æ•°æ®"""
    processed_data = data.copy()
    
    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
    missing_features = [f for f in features if f not in processed_data.columns]
    if missing_features:
        print(f"âš ï¸  ç¼ºå¤±ç‰¹å¾: {missing_features}")
        return None
    
    # æå–é€‰æ‹©çš„ç‰¹å¾
    feature_data = processed_data[features].copy()
    
    # å¤„ç†ç¼ºå¤±å€¼
    print(f"ğŸ“Š ç‰¹å¾ç¼ºå¤±å€¼ç»Ÿè®¡:")
    for feature in features:
        missing_count = feature_data[feature].isna().sum()
        print(f"  {feature}: {missing_count} ä¸ªç¼ºå¤±å€¼")
        
        # å¯¹äºæ•°å€¼å‹ç‰¹å¾ï¼Œç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
        if missing_count > 0:
            median_val = feature_data[feature].median()
            if pd.isna(median_val):  # å¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaNï¼ˆå…¨éƒ¨éƒ½æ˜¯ç¼ºå¤±å€¼ï¼‰
                # ä½¿ç”¨0å¡«å……æˆ–è€…ç‰¹å¾çš„å…¸å‹å€¼
                if feature == 'ip.proto':
                    fill_val = 6  # TCPåè®®
                elif feature == 'tcp.srcport':
                    fill_val = 0  # é»˜è®¤ç«¯å£
                else:
                    fill_val = 0  # é€šç”¨é»˜è®¤å€¼
                print(f"    ç‰¹å¾å…¨éƒ¨ç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤å€¼ {fill_val} å¡«å……")
            else:
                fill_val = median_val
                print(f"    å·²ç”¨ä¸­ä½æ•° {fill_val} å¡«å……")
            
            feature_data[feature].fillna(fill_val, inplace=True)
    
    return feature_data

# 1. åŠ è½½è®­ç»ƒé›†æ•°æ®
print("\n1ï¸âƒ£ åŠ è½½è®­ç»ƒé›†æ•°æ®...")
train_normal = load_data_from_directory(os.path.join(train_path, "APP_0"), 0)  # æ­£å¸¸æµé‡
train_pcdn = load_data_from_directory(os.path.join(train_path, "APP_1"), 1)    # PCDNæµé‡

# 2. åŠ è½½éªŒè¯é›†æ•°æ®
print("\n2ï¸âƒ£ åŠ è½½éªŒè¯é›†æ•°æ®...")
val_normal = load_data_from_directory(os.path.join(val_path, "APP_0"), 0)
val_pcdn = load_data_from_directory(os.path.join(val_path, "APP_1"), 1)

# 3. åŠ è½½æµ‹è¯•é›†æ•°æ®
print("\n3ï¸âƒ£ åŠ è½½æµ‹è¯•é›†æ•°æ®...")
test_normal = load_data_from_directory(os.path.join(test_path, "APP_0"), 0)
test_pcdn = load_data_from_directory(os.path.join(test_path, "APP_1"), 1)

# 4. åˆå¹¶æ•°æ®é›†
print("\n4ï¸âƒ£ åˆå¹¶æ•°æ®é›†...")

train_data = safe_concat([train_normal, train_pcdn], "è®­ç»ƒé›†")
val_data = safe_concat([val_normal, val_pcdn], "éªŒè¯é›†")
test_data = safe_concat([test_normal, test_pcdn], "æµ‹è¯•é›†")

# æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
if train_data.empty or val_data.empty or test_data.empty:
    print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ–‡ä»¶")
    exit()

print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬ (æ­£å¸¸: {len(train_normal)}, PCDN: {len(train_pcdn)})")
print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬ (æ­£å¸¸: {len(val_normal)}, PCDN: {len(val_pcdn)})")
print(f"æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬ (æ­£å¸¸: {len(test_normal)}, PCDN: {len(test_pcdn)})")

# 5. ç‰¹å¾æå–å’Œé¢„å¤„ç†
print(f"\n5ï¸âƒ£ ç‰¹å¾æå–å’Œé¢„å¤„ç†...")
print(f"é€‰æ‹©çš„ç‰¹å¾: {selected_features}")

# é¢„å¤„ç†å„æ•°æ®é›†çš„ç‰¹å¾
X_train = preprocess_features(train_data, selected_features)
X_val = preprocess_features(val_data, selected_features)
X_test = preprocess_features(test_data, selected_features)

if X_train is None or X_val is None or X_test is None:
    print("âŒ ç‰¹å¾é¢„å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç‰¹å¾åç§°")
    exit()

# æå–æ ‡ç­¾
y_train = train_data['label'].values
y_val = val_data['label'].values  
y_test = test_data['label'].values

# 6. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯¹äºå†³ç­–æ ‘å¯é€‰ï¼Œä½†ä¸ºäº†ä¸€è‡´æ€§ä¿ç•™ï¼‰
print(f"\n6ï¸âƒ£ ç‰¹å¾æ ‡å‡†åŒ–...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # åªåœ¨è®­ç»ƒé›†ä¸Šfit
X_val_scaled = scaler.transform(X_val)          # éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°
X_test_scaled = scaler.transform(X_test)        # æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°

print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ")
print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train_scaled.shape}")
print(f"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val_scaled.shape}")
print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test_scaled.shape}")

# 7. æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡
print(f"\n7ï¸âƒ£ æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_train)}")
print(f"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_val)}")
print(f"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_test)}")

# æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡ (è®­ç»ƒé›†):")
feature_stats = pd.DataFrame(X_train, columns=selected_features).describe()
print(feature_stats)

print(f"\nğŸ¯ æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¼€å§‹å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒ...")

# 8. å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒï¼ˆå¸¦è¶…å‚æ•°ä¼˜åŒ–ï¼‰
print(f"\n8ï¸âƒ£ å¼€å§‹å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒ...")
print("=" * 80)

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = "output"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

# å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'criterion': ['gini', 'entropy']
}

print("ğŸ” å¼€å§‹è¶…å‚æ•°ç½‘æ ¼æœç´¢...")
print(f"ğŸ”§ æœç´¢ç©ºé—´: {len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['criterion'])} ç»„åˆ")

# åˆ›å»ºå†³ç­–æ ‘åˆ†ç±»å™¨
dt_base = DecisionTreeClassifier(random_state=42)

# ä½¿ç”¨ç½‘æ ¼æœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°
grid_search = GridSearchCV(
    dt_base, 
    param_grid, 
    cv=3,  # 3æŠ˜äº¤å‰éªŒè¯
    scoring='accuracy',
    n_jobs=-1,
    verbose=0
)

# åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œç½‘æ ¼æœç´¢
grid_search.fit(X_train_scaled, y_train)

# è·å–æœ€ä½³æ¨¡å‹
dt_model = grid_search.best_estimator_

print(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆ!")
print(f"ğŸ† æœ€ä½³å‚æ•°:")
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")
print(f"ğŸ“Š æœ€ä½³äº¤å‰éªŒè¯å¾—åˆ†: {grid_search.best_score_:.4f}")

print("=" * 80)

# 9. æ¨¡å‹é¢„æµ‹
print(f"\n9ï¸âƒ£ æ¨¡å‹é¢„æµ‹...")

# åœ¨å„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹
y_train_pred = dt_model.predict(X_train_scaled)
y_train_prob = dt_model.predict_proba(X_train_scaled)[:, 1]

y_val_pred = dt_model.predict(X_val_scaled)
y_val_prob = dt_model.predict_proba(X_val_scaled)[:, 1]

y_test_pred = dt_model.predict(X_test_scaled)
y_test_prob = dt_model.predict_proba(X_test_scaled)[:, 1]

# 10. æ¨¡å‹è¯„ä¼°
print(f"\nğŸ”Ÿ æ¨¡å‹è¯„ä¼°ç»“æœ...")

# å‡†ç¡®ç‡
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# AUC (å®‰å…¨è®¡ç®—ï¼Œå¤„ç†å•ç±»åˆ«æƒ…å†µ)
def safe_auc_score(y_true, y_prob, set_name):
    """å®‰å…¨è®¡ç®—AUCï¼Œå¤„ç†å•ç±»åˆ«æƒ…å†µ"""
    if len(np.unique(y_true)) < 2:
        print(f"âš ï¸  {set_name} åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è®¡ç®—AUC")
        return 0.5  # è¿”å›é»˜è®¤å€¼
    return roc_auc_score(y_true, y_prob)

train_auc = safe_auc_score(y_train, y_train_prob, "è®­ç»ƒé›†")
val_auc = safe_auc_score(y_val, y_val_prob, "éªŒè¯é›†")
test_auc = safe_auc_score(y_test, y_test_prob, "æµ‹è¯•é›†")

print(f"ğŸ“Š å‡†ç¡®ç‡ (Accuracy):")
print(f"  è®­ç»ƒé›†: {train_acc:.4f}")
print(f"  éªŒè¯é›†: {val_acc:.4f}")
print(f"  æµ‹è¯•é›†: {test_acc:.4f}")

print(f"\nğŸ“Š AUCå€¼:")
print(f"  è®­ç»ƒé›†: {train_auc:.4f}")
print(f"  éªŒè¯é›†: {val_auc:.4f}")
print(f"  æµ‹è¯•é›†: {test_auc:.4f}")

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print(f"\nğŸ“‹ æµ‹è¯•é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_test_pred, target_names=['Normal Traffic', 'PCDN Traffic']))

# 11. ç‰¹å¾é‡è¦æ€§åˆ†æ
print(f"\n1ï¸âƒ£1ï¸âƒ£ ç‰¹å¾é‡è¦æ€§åˆ†æ...")

# è·å–ç‰¹å¾é‡è¦æ€§
feature_importance = dt_model.feature_importances_
feature_names = selected_features

# åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åº:")
for idx, row in importance_df.iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

# 12. å†³ç­–æ ‘ç»“æ„åˆ†æ
print(f"\n1ï¸âƒ£2ï¸âƒ£ å†³ç­–æ ‘ç»“æ„åˆ†æ...")

print(f"ğŸŒ³ å†³ç­–æ ‘ä¿¡æ¯:")
print(f"  æ ‘çš„æ·±åº¦: {dt_model.get_depth()}")
print(f"  å¶å­èŠ‚ç‚¹æ•°: {dt_model.get_n_leaves()}")
print(f"  æ€»èŠ‚ç‚¹æ•°: {dt_model.tree_.node_count}")

# æ‰“å°å†³ç­–æ ‘è§„åˆ™ï¼ˆç®€åŒ–ç‰ˆï¼‰
print(f"\nğŸ“‹ å†³ç­–æ ‘è§„åˆ™ (å‰10æ¡):")
tree_rules = export_text(dt_model, feature_names=selected_features, max_depth=3)
print(tree_rules)

print(f"\nğŸ¯ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# 13. å¯è§†åŒ–ç»“æœ
print(f"\n1ï¸âƒ£3ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# åˆ›å»ºå›¾è¡¨ (3x2å¸ƒå±€ï¼ŒåŒ…å«å†³ç­–æ ‘å›¾)
fig = plt.figure(figsize=(20, 15))
fig.suptitle('Decision Tree PCDN Traffic Classification Results', fontsize=16, fontweight='bold')

# 1. å†³ç­–æ ‘å¯è§†åŒ– (å ç”¨ä¸¤ä¸ªä½ç½®)
ax1 = plt.subplot(3, 2, (1, 2))
plot_tree(dt_model, 
          feature_names=selected_features,
          class_names=['Normal Traffic', 'PCDN Traffic'],
          filled=True,
          max_depth=3,  # é™åˆ¶æ˜¾ç¤ºæ·±åº¦ä»¥ä¿æŒæ¸…æ™°
          fontsize=10,
          ax=ax1)
ax1.set_title('Decision Tree Visualization (max_depth=3)', fontweight='bold', fontsize=14)

# 2. ç‰¹å¾é‡è¦æ€§å›¾
ax2 = plt.subplot(3, 2, 3)
colors = plt.cm.Set3(np.linspace(0, 1, len(importance_df)))
bars = ax2.bar(importance_df['feature'], importance_df['importance'], 
               color=colors)
ax2.set_title('Feature Importance Analysis', fontweight='bold')
ax2.set_xlabel('Feature Names')
ax2.set_ylabel('Importance Score')
ax2.tick_params(axis='x', rotation=45)

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, importance in zip(bars, importance_df['importance']):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{importance:.3f}', ha='center', va='bottom', fontweight='bold')

# 3. ROCæ›²çº¿
ax3 = plt.subplot(3, 2, 4)

# å®‰å…¨ç»˜åˆ¶ROCæ›²çº¿
def safe_plot_roc(y_true, y_prob, label, ax):
    """å®‰å…¨ç»˜åˆ¶ROCæ›²çº¿"""
    if len(np.unique(y_true)) < 2:
        return  # è·³è¿‡å•ç±»åˆ«æƒ…å†µ
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    ax.plot(fpr, tpr, label=label, linewidth=2)

safe_plot_roc(y_train, y_train_prob, f'Training Set (AUC = {train_auc:.3f})', ax3)
safe_plot_roc(y_val, y_val_prob, f'Validation Set (AUC = {val_auc:.3f})', ax3)
safe_plot_roc(y_test, y_test_prob, f'Test Set (AUC = {test_auc:.3f})', ax3)

ax3.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
ax3.set_title('ROC Curve Comparison', fontweight='bold')
ax3.set_xlabel('False Positive Rate (FPR)')
ax3.set_ylabel('True Positive Rate (TPR)')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. æ··æ·†çŸ©é˜µ
ax4 = plt.subplot(3, 2, 5)
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4,
            xticklabels=['Normal Traffic', 'PCDN Traffic'],
            yticklabels=['Normal Traffic', 'PCDN Traffic'])
ax4.set_title('Test Set Confusion Matrix', fontweight='bold')
ax4.set_xlabel('Predicted Label')
ax4.set_ylabel('True Label')

# 5. å‡†ç¡®ç‡å¯¹æ¯”
ax5 = plt.subplot(3, 2, 6)
datasets = ['Training Set', 'Validation Set', 'Test Set']
accuracies = [train_acc, val_acc, test_acc]
colors_acc = ['#FF9999', '#66B2FF', '#99FF99']

bars = ax5.bar(datasets, accuracies, color=colors_acc, alpha=0.8)
ax5.set_title('Accuracy Comparison Across Datasets', fontweight='bold')
ax5.set_ylabel('Accuracy')
ax5.set_ylim(0, 1.1)

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height + 0.02,
             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()

# å®‰å…¨ä¿å­˜å›¾è¡¨
try:
    plt.savefig(os.path.join(output_dir, 'decision_tree_classification_results.png'), 
                dpi=300, bbox_inches='tight')
    print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜ä¸º: output/decision_tree_classification_results.png")
except Exception as e:
    print(f"âš ï¸  å›¾è¡¨ä¿å­˜å¤±è´¥: {e}")
    print("ğŸ“Š å›¾è¡¨ä»åœ¨å†…å­˜ä¸­æ˜¾ç¤º")

plt.show()

# 14. å†³ç­–æ ‘è¯¦ç»†å¯è§†åŒ–ï¼ˆå•ç‹¬ä¿å­˜ï¼‰
print(f"\n1ï¸âƒ£4ï¸âƒ£ ç”Ÿæˆè¯¦ç»†å†³ç­–æ ‘å›¾...")

fig_tree, ax_tree = plt.subplots(1, 1, figsize=(25, 15))
plot_tree(dt_model, 
          feature_names=selected_features,
          class_names=['Normal Traffic', 'PCDN Traffic'],
          filled=True,
          rounded=True,
          fontsize=12,
          ax=ax_tree)
ax_tree.set_title(f'Complete Decision Tree (depth={dt_model.get_depth()}, nodes={dt_model.tree_.node_count})', 
                  fontweight='bold', fontsize=16)

try:
    plt.savefig(os.path.join(output_dir, 'decision_tree_detailed.png'), 
                dpi=300, bbox_inches='tight')
    print("ğŸ“Š è¯¦ç»†å†³ç­–æ ‘å›¾å·²ä¿å­˜ä¸º: output/decision_tree_detailed.png")
except Exception as e:
    print(f"âš ï¸  è¯¦ç»†å†³ç­–æ ‘å›¾ä¿å­˜å¤±è´¥: {e}")

plt.show()

# 15. æ€»ç»“æŠ¥å‘Š
print(f"\n1ï¸âƒ£5ï¸âƒ£ å†³ç­–æ ‘åˆ†ç±»ä»»åŠ¡æ€»ç»“æŠ¥å‘Š")
print("=" * 60)
print(f"ğŸ¯ ä»»åŠ¡: PCDNæµé‡ä¸æ­£å¸¸æµé‡äºŒåˆ†ç±»")
print(f"ğŸŒ³ æ¨¡å‹: Decision Tree Classifier")
print(f"ğŸ”§ ä½¿ç”¨ç‰¹å¾: {', '.join(selected_features)}")
print(f"ğŸ“Š æ•°æ®è§„æ¨¡:")
print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

print(f"\nğŸ† æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:")
print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"  æµ‹è¯•é›†AUCå€¼: {test_auc:.4f}")

print(f"\nğŸŒ³ å†³ç­–æ ‘æ¨¡å‹å‚æ•°:")
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")

print(f"\nğŸŒ³ å†³ç­–æ ‘ç»“æ„:")
print(f"  æ ‘çš„æ·±åº¦: {dt_model.get_depth()}")
print(f"  å¶å­èŠ‚ç‚¹æ•°: {dt_model.get_n_leaves()}")
print(f"  æ€»èŠ‚ç‚¹æ•°: {dt_model.tree_.node_count}")

print(f"\nğŸ“ˆ ç‰¹å¾é‡è¦æ€§:")
for idx, row in importance_df.iterrows():
    percentage = (row['importance'] / importance_df['importance'].sum()) * 100
    print(f"  {row['feature']}: {row['importance']:.4f} ({percentage:.1f}%)")

print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"  ä¸»è¦ç»“æœå›¾: output/decision_tree_classification_results.png")
print(f"  è¯¦ç»†å†³ç­–æ ‘: output/decision_tree_detailed.png")

print(f"\nâœ… å†³ç­–æ ‘åˆ†æå®Œæˆ!")
print("=" * 60)
