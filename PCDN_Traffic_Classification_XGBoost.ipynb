{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDNæµé‡ä¸æ­£å¸¸æµé‡äºŒåˆ†ç±»ä»»åŠ¡\n",
        "\n",
        "## é¡¹ç›®æ¦‚è¿°\n",
        "æœ¬é¡¹ç›®ä½¿ç”¨XGBoostå¯¹ç½‘ç»œæµé‡è¿›è¡ŒäºŒåˆ†ç±»ï¼š\n",
        "- **APP_0**: æ­£å¸¸æµé‡ (æ ‡ç­¾: 0)\n",
        "- **APP_1**: PCDNæµé‡ (æ ‡ç­¾: 1)\n",
        "\n",
        "## æ•°æ®é›†ç»“æ„\n",
        "- `Training_set/`: è®­ç»ƒé›†\n",
        "- `Validation_set/`: éªŒè¯é›†  \n",
        "- `Testing_set/`: æµ‹è¯•é›†\n",
        "\n",
        "æ¯ä¸ªé›†åˆåŒ…å«APP_0ï¼ˆæ­£å¸¸æµé‡ï¼‰å’ŒAPP_1ï¼ˆPCDNæµé‡ï¼‰ä¸¤ä¸ªç±»åˆ«çš„CSVæ–‡ä»¶ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import xgboost as xgb\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "sns.set_style(\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. æ•°æ®åŠ è½½ä¸æ¢ç´¢\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®šä¹‰æ•°æ®åŠ è½½å‡½æ•°\n",
        "def load_dataset(base_path, dataset_type):\n",
        "    \"\"\"\n",
        "    åŠ è½½æŒ‡å®šç±»å‹çš„æ•°æ®é›†\n",
        "    \n",
        "    Args:\n",
        "        base_path: æ•°æ®é›†æ ¹ç›®å½•\n",
        "        dataset_type: æ•°æ®é›†ç±»å‹ ('Training_set', 'Validation_set', 'Testing_set')\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: åˆå¹¶åçš„æ•°æ®é›†\n",
        "    \"\"\"\n",
        "    data_list = []\n",
        "    \n",
        "    # åŠ è½½APP_0 (æ­£å¸¸æµé‡) æ•°æ®\n",
        "    app0_path = Path(base_path) / dataset_type / 'APP_0'\n",
        "    app0_files = list(app0_path.glob('*.csv'))\n",
        "    print(f\"ğŸ“ {dataset_type}/APP_0 æ‰¾åˆ° {len(app0_files)} ä¸ªæ–‡ä»¶\")\n",
        "    \n",
        "    for file in app0_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df['label'] = 0  # æ­£å¸¸æµé‡æ ‡ç­¾\n",
        "            df['source_file'] = str(file.name)\n",
        "            data_list.append(df)\n",
        "            print(f\"  âœ… {file.name}: {len(df)} è¡Œ\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ è¯»å– {file.name} å¤±è´¥: {e}\")\n",
        "    \n",
        "    # åŠ è½½APP_1 (PCDNæµé‡) æ•°æ®\n",
        "    app1_path = Path(base_path) / dataset_type / 'APP_1'\n",
        "    app1_files = list(app1_path.glob('*.csv'))\n",
        "    print(f\"ğŸ“ {dataset_type}/APP_1 æ‰¾åˆ° {len(app1_files)} ä¸ªæ–‡ä»¶\")\n",
        "    \n",
        "    for file in app1_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df['label'] = 1  # PCDNæµé‡æ ‡ç­¾\n",
        "            df['source_file'] = str(file.name)\n",
        "            data_list.append(df)\n",
        "            print(f\"  âœ… {file.name}: {len(df)} è¡Œ\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ è¯»å– {file.name} å¤±è´¥: {e}\")\n",
        "    \n",
        "    if data_list:\n",
        "        combined_df = pd.concat(data_list, ignore_index=True)\n",
        "        print(f\"ğŸ¯ {dataset_type} æ€»è®¡: {len(combined_df)} è¡Œæ•°æ®\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(f\"âš ï¸ {dataset_type} æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# åŠ è½½æ‰€æœ‰æ•°æ®é›†\n",
        "base_path = 'pcdn_32_pkts_2class_feature_enhance_v17.4_dataset'\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†...\\n\")\n",
        "train_df = load_dataset(base_path, 'Training_set')\n",
        "print()\n",
        "val_df = load_dataset(base_path, 'Validation_set')\n",
        "print()\n",
        "test_df = load_dataset(base_path, 'Testing_set')\n",
        "print(\"\\nğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®é›†åŸºæœ¬ä¿¡æ¯\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“ˆ æ•°æ®é›†æ¦‚è§ˆ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "datasets = {'è®­ç»ƒé›†': train_df, 'éªŒè¯é›†': val_df, 'æµ‹è¯•é›†': test_df}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if not df.empty:\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  ğŸ“ æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
        "        print(f\"  ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ:\")\n",
        "        label_counts = df['label'].value_counts().sort_index()\n",
        "        for label, count in label_counts.items():\n",
        "            label_name = \"æ­£å¸¸æµé‡\" if label == 0 else \"PCDNæµé‡\"\n",
        "            print(f\"    {label} ({label_name}): {count} æ ·æœ¬\")\n",
        "        print(f\"  ğŸ“‚ æ–‡ä»¶æ¥æº: {df['source_file'].unique()}\")\n",
        "    else:\n",
        "        print(f\"\\n{name}: ç©ºæ•°æ®é›†\")\n",
        "\n",
        "# æŸ¥çœ‹æ•°æ®å­—æ®µ\n",
        "if not train_df.empty:\n",
        "    print(\"\\nğŸ” æ•°æ®å­—æ®µåˆ†æ\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"æ€»å­—æ®µæ•°: {len(train_df.columns)}\")\n",
        "    print(f\"å­—æ®µåˆ—è¡¨: {list(train_df.columns)}\")\n",
        "    \n",
        "    # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®\n",
        "    print(\"\\nğŸ“‹ è®­ç»ƒé›†å‰3è¡Œæ•°æ®é¢„è§ˆ:\")\n",
        "    display(train_df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_features(df):\n",
        "    \"\"\"\n",
        "    æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    print(\"ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...\")\n",
        "    \n",
        "    # 1. å¤„ç†ç¼ºå¤±å€¼\n",
        "    print(f\"ğŸ“Š ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
        "    missing_stats = df_processed.isnull().sum()\n",
        "    missing_cols = missing_stats[missing_stats > 0]\n",
        "    if len(missing_cols) > 0:\n",
        "        print(missing_cols)\n",
        "        # å¡«å……æ•°å€¼å‹ç¼ºå¤±å€¼\n",
        "        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "        df_processed[numeric_cols] = df_processed[numeric_cols].fillna(0)\n",
        "        # å¡«å……å­—ç¬¦å‹ç¼ºå¤±å€¼\n",
        "        categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "        df_processed[categorical_cols] = df_processed[categorical_cols].fillna('unknown')\n",
        "    else:\n",
        "        print(\"âœ… æ— ç¼ºå¤±å€¼\")\n",
        "    \n",
        "    # 2. å¤„ç†ç‰¹æ®Šå­—æ®µ\n",
        "    special_fields = ['ip_direction', 'pkt_len', 'iat', 'payload']\n",
        "    \n",
        "    for field in special_fields:\n",
        "        if field in df_processed.columns:\n",
        "            print(f\"ğŸ”„ å¤„ç† {field} å­—æ®µ...\")\n",
        "            \n",
        "            if field == 'payload':\n",
        "                # è½½è·æ•°æ®ï¼šè®¡ç®—é•¿åº¦ç‰¹å¾\n",
        "                df_processed[f'{field}_length'] = df_processed[field].astype(str).str.len()\n",
        "                df_processed = df_processed.drop(columns=[field])\n",
        "            \n",
        "            elif field in ['ip_direction', 'pkt_len', 'iat']:\n",
        "                # è§£æåˆ—è¡¨å‹ç‰¹å¾\n",
        "                try:\n",
        "                    # å°è¯•è§£æä¸ºåˆ—è¡¨\n",
        "                    parsed_data = df_processed[field].apply(lambda x: ast.literal_eval(str(x)) if pd.notna(x) and str(x).strip() else [])\n",
        "                    \n",
        "                    # æå–ç»Ÿè®¡ç‰¹å¾\n",
        "                    df_processed[f'{field}_mean'] = parsed_data.apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_std'] = parsed_data.apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_max'] = parsed_data.apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_min'] = parsed_data.apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_sum'] = parsed_data.apply(lambda x: np.sum(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_count'] = parsed_data.apply(lambda x: len(x))\n",
        "                    \n",
        "                    # åˆ é™¤åŸå§‹å­—æ®µ\n",
        "                    df_processed = df_processed.drop(columns=[field])\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ å¤„ç† {field} æ—¶å‡ºé”™: {e}ï¼Œä¿æŒåŸå§‹æ•°æ®\")\n",
        "    \n",
        "    # 3. ç¼–ç åˆ†ç±»ç‰¹å¾\n",
        "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "    categorical_cols = [col for col in categorical_cols if col not in ['source_file']]  # æ’é™¤è¾…åŠ©å­—æ®µ\n",
        "    \n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        if col in df_processed.columns:\n",
        "            le = LabelEncoder()\n",
        "            df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "    \n",
        "    print(f\"ğŸ¯ é¢„å¤„ç†å®Œæˆï¼æœ€ç»ˆç‰¹å¾æ•°: {df_processed.shape[1]}\")\n",
        "    \n",
        "    return df_processed, label_encoders\n",
        "\n",
        "# å¤„ç†è®­ç»ƒæ•°æ®\n",
        "if not train_df.empty:\n",
        "    train_processed, encoders = preprocess_features(train_df)\n",
        "    print(\"\\nâœ… è®­ç»ƒé›†é¢„å¤„ç†å®Œæˆ\")\n",
        "    \n",
        "    # å¤„ç†éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨ç›¸åŒçš„ç¼–ç å™¨ï¼‰\n",
        "    if not val_df.empty:\n",
        "        val_processed, _ = preprocess_features(val_df)\n",
        "        print(\"âœ… éªŒè¯é›†é¢„å¤„ç†å®Œæˆ\")\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        test_processed, _ = preprocess_features(test_df)\n",
        "        print(\"âœ… æµ‹è¯•é›†é¢„å¤„ç†å®Œæˆ\")\n",
        "else:\n",
        "    print(\"âŒ è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé¢„å¤„ç†\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å‡†å¤‡å»ºæ¨¡æ•°æ®\n",
        "if not train_df.empty:\n",
        "    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾\n",
        "    feature_cols = [col for col in train_processed.columns if col not in ['label', 'source_file']]\n",
        "    \n",
        "    X_train = train_processed[feature_cols]\n",
        "    y_train = train_processed['label']\n",
        "    \n",
        "    if not val_df.empty:\n",
        "        X_val = val_processed[feature_cols]\n",
        "        y_val = val_processed['label']\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        X_test = test_processed[feature_cols]\n",
        "        y_test = test_processed['label']\n",
        "    \n",
        "    print(f\"ğŸ¯ ç‰¹å¾ç»´åº¦: {X_train.shape}\")\n",
        "    print(f\"ğŸ“Š è®­ç»ƒæ ‡ç­¾åˆ†å¸ƒ: {y_train.value_counts().to_dict()}\")\n",
        "    print(f\"ğŸ”§ ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
        "    print(f\"ğŸ“ ç‰¹å¾åç§°å‰10ä¸ª: {feature_cols[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. XGBoostæ¨¡å‹è®­ç»ƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoostæ¨¡å‹è®­ç»ƒ\n",
        "if not train_df.empty and len(X_train) > 0:\n",
        "    print(\"ğŸš€ å¼€å§‹XGBoostæ¨¡å‹è®­ç»ƒ...\")\n",
        "    \n",
        "    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼å‹\n",
        "    X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
        "    \n",
        "    if len(X_train_numeric.columns) == 0:\n",
        "        print(\"âŒ æ²¡æœ‰å¯ç”¨çš„æ•°å€¼å‹ç‰¹å¾\")\n",
        "    else:\n",
        "        print(f\"ğŸ“Š ä½¿ç”¨ {len(X_train_numeric.columns)} ä¸ªæ•°å€¼å‹ç‰¹å¾\")\n",
        "        \n",
        "        # é…ç½®XGBoostå‚æ•°\n",
        "        xgb_params = {\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'logloss',\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹\n",
        "        model = xgb.XGBClassifier(**xgb_params)\n",
        "        \n",
        "        # å‡†å¤‡éªŒè¯æ•°æ®\n",
        "        eval_set = []\n",
        "        if not val_df.empty:\n",
        "            X_val_numeric = val_processed[X_train_numeric.columns]\n",
        "            eval_set = [(X_train_numeric, y_train), (X_val_numeric, y_val)]\n",
        "        else:\n",
        "            eval_set = [(X_train_numeric, y_train)]\n",
        "        \n",
        "        # è®­ç»ƒæ¨¡å‹\n",
        "        model.fit(\n",
        "            X_train_numeric, y_train,\n",
        "            eval_set=eval_set,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        print(\"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n",
        "        \n",
        "        # æ›´æ–°ç‰¹å¾åˆ—è¡¨\n",
        "        final_feature_cols = X_train_numeric.columns.tolist()\n",
        "else:\n",
        "    print(\"âŒ æ— æ³•è®­ç»ƒæ¨¡å‹ï¼šæ•°æ®ä¸è¶³\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼°\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    print(\"ğŸ¯ æ¨¡å‹è¯„ä¼°å¼€å§‹...\")\n",
        "    \n",
        "    # è®­ç»ƒé›†é¢„æµ‹\n",
        "    y_train_pred = model.predict(X_train_numeric)\n",
        "    y_train_proba = model.predict_proba(X_train_numeric)[:, 1]\n",
        "    train_accuracy = (y_train_pred == y_train).mean()\n",
        "    \n",
        "    print(f\"ğŸ“Š è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f}\")\n",
        "    \n",
        "    # éªŒè¯é›†é¢„æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰\n",
        "    if not val_df.empty:\n",
        "        y_val_pred = model.predict(X_val_numeric)\n",
        "        y_val_proba = model.predict_proba(X_val_numeric)[:, 1]\n",
        "        val_accuracy = (y_val_pred == y_val).mean()\n",
        "        print(f\"ğŸ“Š éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}\")\n",
        "    \n",
        "    # æµ‹è¯•é›†é¢„æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰\n",
        "    if not test_df.empty:\n",
        "        X_test_numeric = test_processed[final_feature_cols]\n",
        "        y_test_pred = model.predict(X_test_numeric)\n",
        "        y_test_proba = model.predict_proba(X_test_numeric)[:, 1]\n",
        "        test_accuracy = (y_test_pred == y_test).mean()\n",
        "        print(f\"ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}\")\n",
        "    \n",
        "    # æ‰“å°è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    target_names = ['æ­£å¸¸æµé‡', 'PCDNæµé‡']\n",
        "    \n",
        "    print(\"\\nğŸ¯ è®­ç»ƒé›†åˆ†ç±»æŠ¥å‘Š:\")\n",
        "    print(classification_report(y_train, y_train_pred, target_names=target_names))\n",
        "    \n",
        "    if not val_df.empty:\n",
        "        print(\"\\nâœ… éªŒè¯é›†åˆ†ç±»æŠ¥å‘Š:\")\n",
        "        print(classification_report(y_val, y_val_pred, target_names=target_names))\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        print(\"\\nğŸ§ª æµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Š:\")\n",
        "        print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "else:\n",
        "    print(\"âŒ æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯è§†åŒ–è¯„ä¼°ç»“æœ\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # 1. æ··æ·†çŸ©é˜µ\n",
        "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
        "                xticklabels=['æ­£å¸¸æµé‡', 'PCDNæµé‡'], \n",
        "                yticklabels=['æ­£å¸¸æµé‡', 'PCDNæµé‡'])\n",
        "    axes[0,0].set_title('ğŸ”¥ è®­ç»ƒé›†æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')\n",
        "    axes[0,0].set_ylabel('å®é™…æ ‡ç­¾')\n",
        "    axes[0,0].set_xlabel('é¢„æµ‹æ ‡ç­¾')\n",
        "    \n",
        "    # 2. ROCæ›²çº¿\n",
        "    if len(np.unique(y_train)) > 1:\n",
        "        fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
        "        auc_train = roc_auc_score(y_train, y_train_proba)\n",
        "        axes[0,1].plot(fpr_train, tpr_train, label=f'è®­ç»ƒé›† (AUC = {auc_train:.3f})', linewidth=2)\n",
        "        \n",
        "        if not val_df.empty and len(np.unique(y_val)) > 1:\n",
        "            fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba)\n",
        "            auc_val = roc_auc_score(y_val, y_val_proba)\n",
        "            axes[0,1].plot(fpr_val, tpr_val, label=f'éªŒè¯é›† (AUC = {auc_val:.3f})', linewidth=2)\n",
        "        \n",
        "        if not test_df.empty and len(np.unique(y_test)) > 1:\n",
        "            fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
        "            auc_test = roc_auc_score(y_test, y_test_proba)\n",
        "            axes[0,1].plot(fpr_test, tpr_test, label=f'æµ‹è¯•é›† (AUC = {auc_test:.3f})', linewidth=2)\n",
        "        \n",
        "        axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "        axes[0,1].set_xlabel('å‡æ­£ç‡ (FPR)')\n",
        "        axes[0,1].set_ylabel('çœŸæ­£ç‡ (TPR)')\n",
        "        axes[0,1].set_title('ğŸ“ˆ ROCæ›²çº¿', fontsize=14, fontweight='bold')\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n",
        "    axes[1,0].hist(y_train_proba[y_train==0], bins=20, alpha=0.7, label='æ­£å¸¸æµé‡', color='blue')\n",
        "    axes[1,0].hist(y_train_proba[y_train==1], bins=20, alpha=0.7, label='PCDNæµé‡', color='red')\n",
        "    axes[1,0].set_xlabel('é¢„æµ‹æ¦‚ç‡')\n",
        "    axes[1,0].set_ylabel('é¢‘æ¬¡')\n",
        "    axes[1,0].set_title('ğŸ¯ é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. å‡†ç¡®ç‡å¯¹æ¯”\n",
        "    accuracies = [train_accuracy]\n",
        "    labels = ['è®­ç»ƒé›†']\n",
        "    colors = ['#3498db']\n",
        "    \n",
        "    if not val_df.empty:\n",
        "        accuracies.append(val_accuracy)\n",
        "        labels.append('éªŒè¯é›†')\n",
        "        colors.append('#2ecc71')\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        accuracies.append(test_accuracy)\n",
        "        labels.append('æµ‹è¯•é›†')\n",
        "        colors.append('#e74c3c')\n",
        "    \n",
        "    bars = axes[1,1].bar(labels, accuracies, color=colors)\n",
        "    axes[1,1].set_title('ğŸ“Š å„æ•°æ®é›†å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
        "    axes[1,1].set_ylabel('å‡†ç¡®ç‡')\n",
        "    axes[1,1].set_ylim(0, 1.1)\n",
        "    \n",
        "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                      f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âŒ æ— æ³•ç”Ÿæˆè¯„ä¼°å¯è§†åŒ–\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ç‰¹å¾é‡è¦æ€§åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    print(\"ğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ...\")\n",
        "    \n",
        "    # è·å–ç‰¹å¾é‡è¦æ€§\n",
        "    feature_importance = model.feature_importances_\n",
        "    feature_names = final_feature_cols\n",
        "    \n",
        "    # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"ğŸ“Š Top 10 æœ€é‡è¦ç‰¹å¾:\")\n",
        "    print(importance_df.head(10))\n",
        "    \n",
        "    # åˆ›å»ºç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "    \n",
        "    # 1. Top 20 ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾\n",
        "    top_20 = importance_df.head(20)\n",
        "    axes[0,0].barh(range(len(top_20)), top_20['importance'], color='skyblue')\n",
        "    axes[0,0].set_yticks(range(len(top_20)))\n",
        "    axes[0,0].set_yticklabels(top_20['feature'])\n",
        "    axes[0,0].set_xlabel('é‡è¦æ€§åˆ†æ•°')\n",
        "    axes[0,0].set_title('ğŸ† Top 20 ç‰¹å¾é‡è¦æ€§', fontsize=14, fontweight='bold')\n",
        "    axes[0,0].invert_yaxis()\n",
        "    \n",
        "    # 2. ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒç›´æ–¹å›¾\n",
        "    axes[0,1].hist(feature_importance, bins=30, color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].set_xlabel('é‡è¦æ€§åˆ†æ•°')\n",
        "    axes[0,1].set_ylabel('ç‰¹å¾æ•°é‡')\n",
        "    axes[0,1].set_title('ğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. ç´¯ç§¯é‡è¦æ€§è´¡çŒ®\n",
        "    cumsum_importance = np.cumsum(importance_df['importance'].values)\n",
        "    axes[1,0].plot(range(1, len(cumsum_importance)+1), cumsum_importance, 'b-', linewidth=2)\n",
        "    axes[1,0].fill_between(range(1, len(cumsum_importance)+1), cumsum_importance, alpha=0.3)\n",
        "    axes[1,0].set_xlabel('ç‰¹å¾æ•°é‡')\n",
        "    axes[1,0].set_ylabel('ç´¯ç§¯é‡è¦æ€§')\n",
        "    axes[1,0].set_title('ğŸ“ˆ ç´¯ç§¯ç‰¹å¾é‡è¦æ€§è´¡çŒ®', fontsize=14, fontweight='bold')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # æ‰¾åˆ°å‰80%é‡è¦æ€§å¯¹åº”çš„ç‰¹å¾æ•°é‡\n",
        "    threshold_80 = 0.8 * cumsum_importance[-1]\n",
        "    features_80 = np.where(cumsum_importance >= threshold_80)[0][0] + 1\n",
        "    axes[1,0].axhline(y=threshold_80, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[1,0].axvline(x=features_80, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[1,0].text(features_80+1, threshold_80, f'å‰{features_80}ä¸ªç‰¹å¾\\\\nè´¡çŒ®80%é‡è¦æ€§', \n",
        "                  bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "    \n",
        "    # 4. Top 10 ç‰¹å¾é‡è¦æ€§é¥¼å›¾\n",
        "    top_10 = importance_df.head(10)\n",
        "    other_importance = importance_df.iloc[10:]['importance'].sum()\n",
        "    \n",
        "    pie_data = top_10['importance'].tolist() + [other_importance]\n",
        "    pie_labels = top_10['feature'].tolist() + ['å…¶ä»–ç‰¹å¾']\n",
        "    \n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))\n",
        "    wedges, texts, autotexts = axes[1,1].pie(pie_data, labels=pie_labels, autopct='%1.1f%%', \n",
        "                                            colors=colors, startangle=90)\n",
        "    axes[1,1].set_title('ğŸ¥§ Top 10 ç‰¹å¾é‡è¦æ€§å æ¯”', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # è°ƒæ•´æ–‡å­—å¤§å°\n",
        "    for text in texts:\n",
        "        text.set_fontsize(8)\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_fontsize(8)\n",
        "        autotext.set_fontweight('bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ“ˆ ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"ğŸ”¢ æ€»ç‰¹å¾æ•°é‡: {len(feature_importance)}\")\n",
        "    print(f\"ğŸ“Š å¹³å‡é‡è¦æ€§: {np.mean(feature_importance):.6f}\")\n",
        "    print(f\"ğŸ“Š é‡è¦æ€§æ ‡å‡†å·®: {np.std(feature_importance):.6f}\")\n",
        "    print(f\"ğŸ† æœ€é«˜é‡è¦æ€§: {np.max(feature_importance):.6f} ({importance_df.iloc[0]['feature']})\")\n",
        "    print(f\"ğŸ”» æœ€ä½é‡è¦æ€§: {np.min(feature_importance):.6f}\")\n",
        "    print(f\"ğŸ¯ å‰{features_80}ä¸ªç‰¹å¾è´¡çŒ®80%é‡è¦æ€§\")\n",
        "    \n",
        "    # ä¿å­˜é‡è¦ç‰¹å¾åˆ—è¡¨\n",
        "    top_features = importance_df.head(20)['feature'].tolist()\n",
        "    print(f\"\\nğŸŒŸ å»ºè®®å…³æ³¨çš„Top 20ç‰¹å¾:\")\n",
        "    for i, feature in enumerate(top_features, 1):\n",
        "        importance_score = importance_df[importance_df['feature'] == feature]['importance'].iloc[0]\n",
        "        print(f\"  {i:2d}. {feature:<30} (é‡è¦æ€§: {importance_score:.6f})\")\n",
        "else:\n",
        "    print(\"âŒ æ— æ³•è¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. æ¨¡å‹æ€§èƒ½æ€»ç»“ä¸å»ºè®®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡å‹æ€§èƒ½æ€»ç»“\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ¯ PCDNæµé‡åˆ†ç±»æ¨¡å‹æ€§èƒ½æ€»ç»“\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:\")\n",
        "    print(f\"  ğŸ“ è®­ç»ƒæ ·æœ¬: {len(X_train)} ä¸ª (æ­£å¸¸æµé‡: {sum(y_train==0)}, PCDNæµé‡: {sum(y_train==1)})\")\n",
        "    if not val_df.empty:\n",
        "        print(f\"  âœ… éªŒè¯æ ·æœ¬: {len(X_val)} ä¸ª (æ­£å¸¸æµé‡: {sum(y_val==0)}, PCDNæµé‡: {sum(y_val==1)})\")\n",
        "    if not test_df.empty:\n",
        "        print(f\"  ğŸ§ª æµ‹è¯•æ ·æœ¬: {len(X_test)} ä¸ª (æ­£å¸¸æµé‡: {sum(y_test==0)}, PCDNæµé‡: {sum(y_test==1)})\")\n",
        "    print(f\"  ğŸ”§ ä½¿ç”¨ç‰¹å¾: {len(final_feature_cols)} ä¸ª\")\n",
        "    \n",
        "    print(f\"\\nğŸ† æ¨¡å‹æ€§èƒ½:\")\n",
        "    print(f\"  ğŸ“ˆ è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "    if not val_df.empty:\n",
        "        print(f\"  ğŸ“ˆ éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
        "    if not test_df.empty:\n",
        "        print(f\"  ğŸ“ˆ æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "    \n",
        "    if len(np.unique(y_train)) > 1:\n",
        "        print(f\"  ğŸ“Š è®­ç»ƒé›†AUC: {auc_train:.4f}\")\n",
        "        if not val_df.empty and len(np.unique(y_val)) > 1:\n",
        "            print(f\"  ğŸ“Š éªŒè¯é›†AUC: {auc_val:.4f}\")\n",
        "        if not test_df.empty and len(np.unique(y_test)) > 1:\n",
        "            print(f\"  ğŸ“Š æµ‹è¯•é›†AUC: {auc_test:.4f}\")\n",
        "    \n",
        "    print(f\"\\nğŸŒŸ å…³é”®ç‰¹å¾ (Top 5):\")\n",
        "    for i, (_, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
        "        print(f\"  {i}. {row['feature']:<25} (é‡è¦æ€§: {row['importance']:.6f})\")\n",
        "    \n",
        "    print(f\"\\nğŸ’¡ æ¨¡å‹å»ºè®®:\")\n",
        "    \n",
        "    # æ•°æ®é‡å»ºè®®\n",
        "    total_samples = len(X_train)\n",
        "    if total_samples < 100:\n",
        "        print(f\"  âš ï¸  æ•°æ®é‡è¾ƒå° ({total_samples}æ ·æœ¬)ï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥æé«˜æ¨¡å‹ç¨³å®šæ€§\")\n",
        "    elif total_samples < 1000:\n",
        "        print(f\"  ğŸ“Š æ•°æ®é‡é€‚ä¸­ ({total_samples}æ ·æœ¬)ï¼Œå¯è€ƒè™‘æ•°æ®å¢å¼ºæŠ€æœ¯\")\n",
        "    else:\n",
        "        print(f\"  âœ… æ•°æ®é‡å……è¶³ ({total_samples}æ ·æœ¬)\")\n",
        "    \n",
        "    # ç±»åˆ«å¹³è¡¡å»ºè®®\n",
        "    class_ratio = min(sum(y_train==0), sum(y_train==1)) / max(sum(y_train==0), sum(y_train==1))\n",
        "    if class_ratio < 0.5:\n",
        "        print(f\"  âš ï¸  ç±»åˆ«ä¸å¹³è¡¡ (æ¯”ä¾‹: {class_ratio:.2f})ï¼Œå»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡æˆ–é‡‡æ ·æŠ€æœ¯\")\n",
        "    else:\n",
        "        print(f\"  âœ… ç±»åˆ«ç›¸å¯¹å¹³è¡¡ (æ¯”ä¾‹: {class_ratio:.2f})\")\n",
        "    \n",
        "    # æ€§èƒ½å»ºè®®\n",
        "    if not val_df.empty:\n",
        "        overfitting = train_accuracy - val_accuracy\n",
        "        if overfitting > 0.1:\n",
        "            print(f\"  âš ï¸  å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (è®­ç»ƒ-éªŒè¯å·®è·: {overfitting:.3f})ï¼Œå»ºè®®è°ƒæ•´æ­£åˆ™åŒ–å‚æ•°\")\n",
        "        elif overfitting < -0.05:\n",
        "            print(f\"  ğŸ¤” éªŒè¯é›†æ€§èƒ½ä¼˜äºè®­ç»ƒé›†ï¼Œå¯èƒ½æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´\")\n",
        "        else:\n",
        "            print(f\"  âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½ (è®­ç»ƒ-éªŒè¯å·®è·: {overfitting:.3f})\")\n",
        "    \n",
        "    # ç‰¹å¾å»ºè®®\n",
        "    high_importance_features = len(importance_df[importance_df['importance'] > importance_df['importance'].mean()])\n",
        "    print(f\"  ğŸ”§ {high_importance_features}/{len(final_feature_cols)} ä¸ªç‰¹å¾é«˜äºå¹³å‡é‡è¦æ€§\")\n",
        "    \n",
        "    if features_80 < len(final_feature_cols) * 0.5:\n",
        "        print(f\"  ğŸ’¡ å¯è€ƒè™‘ç‰¹å¾é€‰æ‹©ï¼šå‰{features_80}ä¸ªç‰¹å¾å·²è´¡çŒ®80%é‡è¦æ€§\")\n",
        "    \n",
        "    print(f\"\\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:\")\n",
        "    print(f\"  1. ğŸ“ˆ æ”¶é›†æ›´å¤šæ ·æœ¬æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å°‘æ•°ç±»åˆ«\")\n",
        "    print(f\"  2. ğŸ”§ åŸºäºç‰¹å¾é‡è¦æ€§è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹ä¼˜åŒ–\")\n",
        "    print(f\"  3. âš™ï¸  å°è¯•å…¶ä»–ç®—æ³•å¯¹æ¯” (Random Forest, SVM, Neural Network)\")\n",
        "    print(f\"  4. ğŸ¯ è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½\")\n",
        "    print(f\"  5. ğŸ“Š åœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²å’Œç›‘æ§æ¨¡å‹æ€§èƒ½\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âœ… åˆ†æå®Œæˆï¼æ¨¡å‹å·²å‡†å¤‡å°±ç»ªã€‚\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"âŒ æ— æ³•ç”Ÿæˆæ¨¡å‹æ€»ç»“\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. æ¨¡å‹ä¿å­˜ä¸åŠ è½½\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹\n",
        "if 'model' in locals():\n",
        "    import joblib\n",
        "    \n",
        "    model_filename = 'pcdn_traffic_classifier.pkl'\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: {model_filename}\")\n",
        "    \n",
        "    # ä¿å­˜ç‰¹å¾åˆ—è¡¨\n",
        "    import json\n",
        "    features_filename = 'model_features.json'\n",
        "    with open(features_filename, 'w') as f:\n",
        "        json.dump(final_feature_cols, f, indent=2)\n",
        "    print(f\"ğŸ“ ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜ä¸º: {features_filename}\")\n",
        "    \n",
        "    # ä¿å­˜ç‰¹å¾é‡è¦æ€§\n",
        "    importance_filename = 'feature_importance.csv'\n",
        "    importance_df.to_csv(importance_filename, index=False, encoding='utf-8-sig')\n",
        "    print(f\"ğŸ“Š ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜ä¸º: {importance_filename}\")\n",
        "    \n",
        "    print(\"\\nğŸ‰ æ‰€æœ‰æ–‡ä»¶ä¿å­˜å®Œæˆï¼\")\n",
        "    \n",
        "    # æ¼”ç¤ºå¦‚ä½•åŠ è½½æ¨¡å‹\n",
        "    print(\"\\nğŸ“– æ¨¡å‹åŠ è½½ç¤ºä¾‹ä»£ç :\")\n",
        "    print(\"\"\"\n",
        "    import joblib\n",
        "    import json\n",
        "    \n",
        "    # åŠ è½½æ¨¡å‹\n",
        "    loaded_model = joblib.load('pcdn_traffic_classifier.pkl')\n",
        "    \n",
        "    # åŠ è½½ç‰¹å¾åˆ—è¡¨\n",
        "    with open('model_features.json', 'r') as f:\n",
        "        feature_columns = json.load(f)\n",
        "    \n",
        "    # å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹\n",
        "    # new_data = preprocess_new_data(raw_data)  # éœ€è¦ç›¸åŒçš„é¢„å¤„ç†\n",
        "    # predictions = loaded_model.predict(new_data[feature_columns])\n",
        "    \"\"\")\n",
        "else:\n",
        "    print(\"âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä¿å­˜\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
