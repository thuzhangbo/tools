# PCDN vs Normal Traffic Classification using Decision Tree
# ä½¿ç”¨ ip.proto å’Œ tcp.srcport ç‰¹å¾è¿›è¡Œå†³ç­–æ ‘äºŒåˆ†ç±»
#
# ğŸš€ æ•°æ®ç¼“å­˜åŠŸèƒ½è¯´æ˜:
# - é¦–æ¬¡è¿è¡Œ: åŠ è½½åŸå§‹CSVæ•°æ®ï¼Œé¢„å¤„ç†åè‡ªåŠ¨ä¿å­˜åˆ°ç¼“å­˜
# - åç»­è¿è¡Œ: è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½ç¼“å­˜æ•°æ®ï¼Œå¤§å¹…æå‡å¯åŠ¨é€Ÿåº¦
# - å¼ºåˆ¶é‡æ–°åŠ è½½: å°†ä¸‹æ–¹ force_reload è®¾ç½®ä¸º True
# - ç¼“å­˜ä½ç½®: data_cache/preprocessed_data.pkl

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
from sklearn.preprocessing import StandardScaler
import os
import glob
import pickle
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®å›¾è¡¨æ ·å¼
try:
    plt.style.use('seaborn-v0_8')
except:
    plt.style.use('default')
sns.set_palette("husl")

print("ğŸŒ³ å¼€å§‹PCDNæµé‡å†³ç­–æ ‘åˆ†ç±»ä»»åŠ¡")
print("ğŸ“‹ ä½¿ç”¨ç‰¹å¾: ip.proto, tcp.srcport")
print("ğŸ¯ æ¨¡å‹: Decision Tree Classifier")
print("=" * 60)

# å®šä¹‰æ•°æ®è·¯å¾„
base_path = "pcdn_32_pkts_2class_feature_enhance_v17.4_dataset"
train_path = os.path.join(base_path, "Training_set")
val_path = os.path.join(base_path, "Validation_set") 
test_path = os.path.join(base_path, "Testing_set")

# é€‰æ‹©çš„ç‰¹å¾
selected_features = ['ip.proto', 'tcp.srcport']

# æ•°æ®ç¼“å­˜è®¾ç½®
cache_dir = "data_cache"
cache_file = os.path.join(cache_dir, "preprocessed_data.pkl")
force_reload = False  # è®¾ç½®ä¸ºTrueå¼ºåˆ¶é‡æ–°åŠ è½½æ•°æ®

def load_data_from_directory(directory_path, label):
    """åŠ è½½æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = glob.glob(os.path.join(directory_path, "*.csv"))
    dataframes = []
    
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            df['label'] = label  # æ·»åŠ æ ‡ç­¾
            df['source_file'] = os.path.basename(file)  # è®°å½•æ¥æºæ–‡ä»¶
            dataframes.append(df)
            print(f"âœ… åŠ è½½æ–‡ä»¶: {file} (æ ·æœ¬æ•°: {len(df)})")
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file}, é”™è¯¯: {e}")
    
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"ğŸ“Š {directory_path} æ€»æ ·æœ¬æ•°: {len(combined_df)}")
        return combined_df
    else:
        print(f"âš ï¸  {directory_path} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return pd.DataFrame()

# æ£€æŸ¥æ˜¯å¦æœ‰ç©ºçš„æ•°æ®é›†
def safe_concat(dataframes, set_name):
    """å®‰å…¨åˆå¹¶æ•°æ®é›†ï¼Œå¤„ç†ç©ºæ•°æ®é›†çš„æƒ…å†µ"""
    non_empty_dfs = [df for df in dataframes if not df.empty]
    if not non_empty_dfs:
        print(f"âš ï¸  {set_name} æ²¡æœ‰æœ‰æ•ˆæ•°æ®!")
        return pd.DataFrame()
    return pd.concat(non_empty_dfs, ignore_index=True)

def preprocess_features(data, features):
    """é¢„å¤„ç†ç‰¹å¾æ•°æ®"""
    processed_data = data.copy()
    
    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
    missing_features = [f for f in features if f not in processed_data.columns]
    if missing_features:
        print(f"âš ï¸  ç¼ºå¤±ç‰¹å¾: {missing_features}")
        return None
    
    # æå–é€‰æ‹©çš„ç‰¹å¾
    feature_data = processed_data[features].copy()
    
    # å¤„ç†ç¼ºå¤±å€¼
    print(f"ğŸ“Š ç‰¹å¾ç¼ºå¤±å€¼ç»Ÿè®¡:")
    for feature in features:
        missing_count = feature_data[feature].isna().sum()
        print(f"  {feature}: {missing_count} ä¸ªç¼ºå¤±å€¼")
        
        # å¯¹äºæ•°å€¼å‹ç‰¹å¾ï¼Œç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
        if missing_count > 0:
            median_val = feature_data[feature].median()
            if pd.isna(median_val):  # å¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaNï¼ˆå…¨éƒ¨éƒ½æ˜¯ç¼ºå¤±å€¼ï¼‰
                # ä½¿ç”¨0å¡«å……æˆ–è€…ç‰¹å¾çš„å…¸å‹å€¼
                if feature == 'ip.proto':
                    fill_val = 6  # TCPåè®®
                elif feature == 'tcp.srcport':
                    fill_val = 0  # é»˜è®¤ç«¯å£
                else:
                    fill_val = 0  # é€šç”¨é»˜è®¤å€¼
                print(f"    ç‰¹å¾å…¨éƒ¨ç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤å€¼ {fill_val} å¡«å……")
            else:
                fill_val = median_val
                print(f"    å·²ç”¨ä¸­ä½æ•° {fill_val} å¡«å……")
            
            feature_data[feature].fillna(fill_val, inplace=True)
    
    return feature_data

# æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜æ•°æ®
if not force_reload and os.path.exists(cache_file):
    print(f"\nğŸš€ å‘ç°ç¼“å­˜æ•°æ®ï¼Œæ­£åœ¨å¿«é€ŸåŠ è½½...")
    try:
        with open(cache_file, 'rb') as f:
            cached_data = pickle.load(f)
        
        X_train_scaled = cached_data['X_train_scaled']
        X_val_scaled = cached_data['X_val_scaled']  
        X_test_scaled = cached_data['X_test_scaled']
        y_train = cached_data['y_train']
        y_val = cached_data['y_val']
        y_test = cached_data['y_test']
        scaler = cached_data['scaler']
        
        print(f"âœ… ç¼“å­˜æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"ğŸ“Š è®­ç»ƒé›†: {X_train_scaled.shape[0]} æ ·æœ¬, {X_train_scaled.shape[1]} ç‰¹å¾")
        print(f"ğŸ“Š éªŒè¯é›†: {X_val_scaled.shape[0]} æ ·æœ¬")
        print(f"ğŸ“Š æµ‹è¯•é›†: {X_test_scaled.shape[0]} æ ·æœ¬")
        print(f"ğŸ¯ è·³è½¬åˆ°æ¨¡å‹è®­ç»ƒ...")
        
        # è·³è½¬åˆ°æ¨¡å‹è®­ç»ƒéƒ¨åˆ†
        data_loaded_from_cache = True
        
    except Exception as e:
        print(f"âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ”„ å°†é‡æ–°åŠ è½½åŸå§‹æ•°æ®...")
        data_loaded_from_cache = False
else:
    data_loaded_from_cache = False

if not data_loaded_from_cache:
    # 1. åŠ è½½è®­ç»ƒé›†æ•°æ®
    print("\n1ï¸âƒ£ åŠ è½½è®­ç»ƒé›†æ•°æ®...")
    train_normal = load_data_from_directory(os.path.join(train_path, "APP_0"), 0)  # æ­£å¸¸æµé‡
    train_pcdn = load_data_from_directory(os.path.join(train_path, "APP_1"), 1)    # PCDNæµé‡

    # 2. åŠ è½½éªŒè¯é›†æ•°æ®
    print("\n2ï¸âƒ£ åŠ è½½éªŒè¯é›†æ•°æ®...")
    val_normal = load_data_from_directory(os.path.join(val_path, "APP_0"), 0)
    val_pcdn = load_data_from_directory(os.path.join(val_path, "APP_1"), 1)

    # 3. åŠ è½½æµ‹è¯•é›†æ•°æ®
    print("\n3ï¸âƒ£ åŠ è½½æµ‹è¯•é›†æ•°æ®...")
    test_normal = load_data_from_directory(os.path.join(test_path, "APP_0"), 0)
    test_pcdn = load_data_from_directory(os.path.join(test_path, "APP_1"), 1)

    # 4. åˆå¹¶æ•°æ®é›†
    print("\n4ï¸âƒ£ åˆå¹¶æ•°æ®é›†...")

    train_data = safe_concat([train_normal, train_pcdn], "è®­ç»ƒé›†")
    val_data = safe_concat([val_normal, val_pcdn], "éªŒè¯é›†")
    test_data = safe_concat([test_normal, test_pcdn], "æµ‹è¯•é›†")

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if train_data.empty or val_data.empty or test_data.empty:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ–‡ä»¶")
        exit()

    print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬ (æ­£å¸¸: {len(train_normal)}, PCDN: {len(train_pcdn)})")
    print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬ (æ­£å¸¸: {len(val_normal)}, PCDN: {len(val_pcdn)})")
    print(f"æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬ (æ­£å¸¸: {len(test_normal)}, PCDN: {len(test_pcdn)})")

    # 5. ç‰¹å¾æå–å’Œé¢„å¤„ç†
    print(f"\n5ï¸âƒ£ ç‰¹å¾æå–å’Œé¢„å¤„ç†...")
    print(f"é€‰æ‹©çš„ç‰¹å¾: {selected_features}")

    # é¢„å¤„ç†å„æ•°æ®é›†çš„ç‰¹å¾
    X_train = preprocess_features(train_data, selected_features)
    X_val = preprocess_features(val_data, selected_features)
    X_test = preprocess_features(test_data, selected_features)

    if X_train is None or X_val is None or X_test is None:
        print("âŒ ç‰¹å¾é¢„å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç‰¹å¾åç§°")
        exit()

    # æå–æ ‡ç­¾
    y_train = train_data['label'].values
    y_val = val_data['label'].values  
    y_test = test_data['label'].values

    # 6. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯¹äºå†³ç­–æ ‘å¯é€‰ï¼Œä½†ä¸ºäº†ä¸€è‡´æ€§ä¿ç•™ï¼‰
    print(f"\n6ï¸âƒ£ ç‰¹å¾æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)  # åªåœ¨è®­ç»ƒé›†ä¸Šfit
    X_val_scaled = scaler.transform(X_val)          # éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°
    X_test_scaled = scaler.transform(X_test)        # æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°

    print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ")
    print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train_scaled.shape}")
    print(f"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val_scaled.shape}")
    print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test_scaled.shape}")
    
    # 7. ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°ç¼“å­˜
    print(f"\nğŸ’¾ ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°ç¼“å­˜...")
    try:
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
            
        # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
        cache_data = {
            'X_train_scaled': X_train_scaled,
            'X_val_scaled': X_val_scaled,
            'X_test_scaled': X_test_scaled,
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test,
            'scaler': scaler,
            'selected_features': selected_features,
            'data_shapes': {
                'train': X_train_scaled.shape,
                'val': X_val_scaled.shape,
                'test': X_test_scaled.shape
            }
        }
        
        # ä¿å­˜åˆ°pickleæ–‡ä»¶
        with open(cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
            
        print(f"âœ… æ•°æ®ç¼“å­˜ä¿å­˜æˆåŠŸ: {cache_file}")
        print(f"ğŸ“ ç¼“å­˜å¤§å°: {os.path.getsize(cache_file) / 1024 / 1024:.2f} MB")
        
    except Exception as e:
        print(f"âš ï¸  æ•°æ®ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        print(f"ğŸ”„ ç»§ç»­æ­£å¸¸æµç¨‹...")

# æ— è®ºæ˜¯ä»ç¼“å­˜åŠ è½½è¿˜æ˜¯é‡æ–°å¤„ç†ï¼Œéƒ½ç»§ç»­æ‰§è¡Œæ•°æ®ç»Ÿè®¡
print(f"\n8ï¸âƒ£ æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train_scaled.shape}")
print(f"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val_scaled.shape}")
print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test_scaled.shape}")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_train)}")
print(f"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_val)}")
print(f"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_test)}")

# æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡ï¼ˆä»åŸå§‹æ•°æ®é‡æ–°åˆ›å»ºï¼‰
if not data_loaded_from_cache:
    print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡ (è®­ç»ƒé›†):")
    feature_stats = pd.DataFrame(X_train, columns=selected_features).describe()
    print(feature_stats)
else:
    print(f"\nğŸ“Š ä½¿ç”¨ç‰¹å¾: {selected_features}")
    print(f"ğŸš€ æ•°æ®å·²ä»ç¼“å­˜åŠ è½½ï¼Œè·³è¿‡è¯¦ç»†ç»Ÿè®¡")

print(f"\nğŸ¯ æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¼€å§‹å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒ...")

# 9. å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒï¼ˆå¸¦è¶…å‚æ•°ä¼˜åŒ–ï¼‰
print(f"\n9ï¸âƒ£ å¼€å§‹å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒ...")
print("=" * 80)

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = "output"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

# å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'criterion': ['gini', 'entropy']
}

print("ğŸ” å¼€å§‹è¶…å‚æ•°ç½‘æ ¼æœç´¢...")
print(f"ğŸ”§ æœç´¢ç©ºé—´: {len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['criterion'])} ç»„åˆ")

# åˆ›å»ºå†³ç­–æ ‘åˆ†ç±»å™¨
dt_base = DecisionTreeClassifier(random_state=42)

# ä½¿ç”¨ç½‘æ ¼æœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°
grid_search = GridSearchCV(
    dt_base, 
    param_grid, 
    cv=3,  # 3æŠ˜äº¤å‰éªŒè¯
    scoring='accuracy',
    n_jobs=-1,
    verbose=0
)

# åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œç½‘æ ¼æœç´¢
grid_search.fit(X_train_scaled, y_train)

# è·å–æœ€ä½³æ¨¡å‹
dt_model = grid_search.best_estimator_

print(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆ!")
print(f"ğŸ† æœ€ä½³å‚æ•°:")
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")
print(f"ğŸ“Š æœ€ä½³äº¤å‰éªŒè¯å¾—åˆ†: {grid_search.best_score_:.4f}")

print("=" * 80)

# 10. æ¨¡å‹é¢„æµ‹
print(f"\nğŸ”Ÿ æ¨¡å‹é¢„æµ‹...")

# åœ¨å„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹
y_train_pred = dt_model.predict(X_train_scaled)
y_train_prob = dt_model.predict_proba(X_train_scaled)[:, 1]

y_val_pred = dt_model.predict(X_val_scaled)
y_val_prob = dt_model.predict_proba(X_val_scaled)[:, 1]

y_test_pred = dt_model.predict(X_test_scaled)
y_test_prob = dt_model.predict_proba(X_test_scaled)[:, 1]

# 11. æ¨¡å‹è¯„ä¼°
print(f"\n1ï¸âƒ£1ï¸âƒ£ æ¨¡å‹è¯„ä¼°ç»“æœ...")

# å‡†ç¡®ç‡
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# AUC (å®‰å…¨è®¡ç®—ï¼Œå¤„ç†å•ç±»åˆ«æƒ…å†µ)
def safe_auc_score(y_true, y_prob, set_name):
    """å®‰å…¨è®¡ç®—AUCï¼Œå¤„ç†å•ç±»åˆ«æƒ…å†µ"""
    if len(np.unique(y_true)) < 2:
        print(f"âš ï¸  {set_name} åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è®¡ç®—AUC")
        return 0.5  # è¿”å›é»˜è®¤å€¼
    return roc_auc_score(y_true, y_prob)

train_auc = safe_auc_score(y_train, y_train_prob, "è®­ç»ƒé›†")
val_auc = safe_auc_score(y_val, y_val_prob, "éªŒè¯é›†")
test_auc = safe_auc_score(y_test, y_test_prob, "æµ‹è¯•é›†")

print(f"ğŸ“Š å‡†ç¡®ç‡ (Accuracy):")
print(f"  è®­ç»ƒé›†: {train_acc:.4f}")
print(f"  éªŒè¯é›†: {val_acc:.4f}")
print(f"  æµ‹è¯•é›†: {test_acc:.4f}")

print(f"\nğŸ“Š AUCå€¼:")
print(f"  è®­ç»ƒé›†: {train_auc:.4f}")
print(f"  éªŒè¯é›†: {val_auc:.4f}")
print(f"  æµ‹è¯•é›†: {test_auc:.4f}")

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print(f"\nğŸ“‹ æµ‹è¯•é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_test_pred, target_names=['Normal Traffic', 'PCDN Traffic']))

# 12. ç‰¹å¾é‡è¦æ€§åˆ†æ
print(f"\n1ï¸âƒ£2ï¸âƒ£ ç‰¹å¾é‡è¦æ€§åˆ†æ...")

# è·å–ç‰¹å¾é‡è¦æ€§
feature_importance = dt_model.feature_importances_
feature_names = selected_features

# åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åº:")
for idx, row in importance_df.iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

# 13. å†³ç­–æ ‘ç»“æ„åˆ†æ
print(f"\n1ï¸âƒ£3ï¸âƒ£ å†³ç­–æ ‘ç»“æ„åˆ†æ...")

print(f"ğŸŒ³ å†³ç­–æ ‘ä¿¡æ¯:")
print(f"  æ ‘çš„æ·±åº¦: {dt_model.get_depth()}")
print(f"  å¶å­èŠ‚ç‚¹æ•°: {dt_model.get_n_leaves()}")
print(f"  æ€»èŠ‚ç‚¹æ•°: {dt_model.tree_.node_count}")

# æ‰“å°å†³ç­–æ ‘è§„åˆ™ï¼ˆç®€åŒ–ç‰ˆï¼‰
print(f"\nğŸ“‹ å†³ç­–æ ‘è§„åˆ™ (å‰10æ¡):")
tree_rules = export_text(dt_model, feature_names=selected_features, max_depth=3)
print(tree_rules)

print(f"\nğŸ¯ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# 14. å¯è§†åŒ–ç»“æœ
print(f"\n1ï¸âƒ£4ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# é¦–å…ˆåˆ›å»ºå†³ç­–æ ‘çš„å•ç‹¬å¯è§†åŒ–å›¾
print("ğŸŒ³ ç”Ÿæˆå†³ç­–æ ‘å•ç‹¬å¯è§†åŒ–å›¾...")

# ç¡®å®šåˆé€‚çš„æ˜¾ç¤ºæ·±åº¦ï¼ˆå¹³è¡¡æ¸…æ™°åº¦å’Œä¿¡æ¯é‡ï¼‰
tree_depth = dt_model.get_depth()
display_depth = min(tree_depth, 4)  # æœ€å¤šæ˜¾ç¤º4å±‚ä»¥ä¿æŒæ¸…æ™°

fig_tree_single, ax_tree_single = plt.subplots(1, 1, figsize=(24, 16))

# ä½¿ç”¨æ›´å¥½çš„é…è‰²å’Œæ ·å¼
plot_tree(dt_model, 
          feature_names=selected_features,
          class_names=['Normal Traffic', 'PCDN Traffic'],
          filled=True,
          rounded=True,
          max_depth=display_depth,
          fontsize=14,
          proportion=True,  # æ˜¾ç¤ºæ¯”ä¾‹ä¿¡æ¯
          impurity=True,    # æ˜¾ç¤ºä¸çº¯åº¦
          ax=ax_tree_single)

# è®¾ç½®æ ‡é¢˜å’Œæ ·å¼
title_text = f'Decision Tree for PCDN Traffic Classification\n'
title_text += f'(Showing top {display_depth} levels, Total depth: {tree_depth}, Total nodes: {dt_model.tree_.node_count})'
ax_tree_single.set_title(title_text, fontweight='bold', fontsize=18, pad=20)

# ç§»é™¤åæ ‡è½´
ax_tree_single.set_xticks([])
ax_tree_single.set_yticks([])
ax_tree_single.spines['top'].set_visible(False)
ax_tree_single.spines['right'].set_visible(False)
ax_tree_single.spines['bottom'].set_visible(False)
ax_tree_single.spines['left'].set_visible(False)

# æ·»åŠ å›¾ä¾‹è¯´æ˜
legend_text = """
ğŸ“‹ How to Read This Decision Tree:

ğŸ”¹ Node Information:
   â€¢ Feature condition: [feature â‰¤ threshold]
   â€¢ gini: Impurity measure (0.0 = pure, 0.5 = mixed)
   â€¢ samples: Number of training samples reaching this node
   â€¢ value: [Normal Traffic count, PCDN Traffic count]
   â€¢ class: Final prediction for this node

ğŸ”¹ Colors:
   â€¢ Orange tones: Predominantly Normal Traffic
   â€¢ Blue tones: Predominantly PCDN Traffic
   â€¢ Darker = more confident, Lighter = more mixed

ğŸ”¹ Decision Path:
   â€¢ Follow Yes (True) â†’ Left branch
   â€¢ Follow No (False) â†’ Right branch
   â€¢ Leaf nodes show final classification
"""

ax_tree_single.text(0.02, 0.02, legend_text, transform=ax_tree_single.transAxes, 
                   fontsize=11, verticalalignment='bottom',
                   bbox=dict(boxstyle="round,pad=0.8", facecolor="lightyellow", alpha=0.9, edgecolor="gray"))

plt.tight_layout()

# ä¿å­˜é«˜æ¸…çš„å†³ç­–æ ‘å›¾
try:
    plt.savefig(os.path.join(output_dir, 'decision_tree_single_clear.png'), 
                dpi=300, bbox_inches='tight', facecolor='white')
    print("ğŸ“Š æ¸…æ™°å†³ç­–æ ‘å›¾å·²ä¿å­˜ä¸º: output/decision_tree_single_clear.png")
except Exception as e:
    print(f"âš ï¸  å†³ç­–æ ‘å›¾ä¿å­˜å¤±è´¥: {e}")

plt.show()

# åˆ›å»ºå…¶ä»–åˆ†æå›¾è¡¨ (2x2å¸ƒå±€ï¼Œä¸åŒ…å«å†³ç­–æ ‘)
fig = plt.figure(figsize=(16, 12))
fig.suptitle('Decision Tree PCDN Traffic Classification - Performance Analysis', fontsize=16, fontweight='bold')

# 1. ç‰¹å¾é‡è¦æ€§å›¾
ax1 = plt.subplot(2, 2, 1)
colors = plt.cm.Set3(np.linspace(0, 1, len(importance_df)))
bars = ax1.bar(importance_df['feature'], importance_df['importance'], 
               color=colors)
ax1.set_title('Feature Importance Analysis', fontweight='bold')
ax1.set_xlabel('Feature Names')
ax1.set_ylabel('Importance Score')
ax1.tick_params(axis='x', rotation=45)

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, importance in zip(bars, importance_df['importance']):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{importance:.3f}', ha='center', va='bottom', fontweight='bold')

# 2. ROCæ›²çº¿
ax2 = plt.subplot(2, 2, 2)

# å®‰å…¨ç»˜åˆ¶ROCæ›²çº¿
def safe_plot_roc(y_true, y_prob, label, ax):
    """å®‰å…¨ç»˜åˆ¶ROCæ›²çº¿"""
    if len(np.unique(y_true)) < 2:
        return  # è·³è¿‡å•ç±»åˆ«æƒ…å†µ
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    ax.plot(fpr, tpr, label=label, linewidth=2)

safe_plot_roc(y_train, y_train_prob, f'Training Set (AUC = {train_auc:.3f})', ax2)
safe_plot_roc(y_val, y_val_prob, f'Validation Set (AUC = {val_auc:.3f})', ax2)
safe_plot_roc(y_test, y_test_prob, f'Test Set (AUC = {test_auc:.3f})', ax2)

ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
ax2.set_title('ROC Curve Comparison', fontweight='bold')
ax2.set_xlabel('False Positive Rate (FPR)')
ax2.set_ylabel('True Positive Rate (TPR)')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. æ··æ·†çŸ©é˜µ
ax3 = plt.subplot(2, 2, 3)
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3,
            xticklabels=['Normal Traffic', 'PCDN Traffic'],
            yticklabels=['Normal Traffic', 'PCDN Traffic'])
ax3.set_title('Test Set Confusion Matrix', fontweight='bold')
ax3.set_xlabel('Predicted Label')
ax3.set_ylabel('True Label')

# 4. å‡†ç¡®ç‡å¯¹æ¯”
ax4 = plt.subplot(2, 2, 4)
datasets = ['Training Set', 'Validation Set', 'Test Set']
accuracies = [train_acc, val_acc, test_acc]
colors_acc = ['#FF9999', '#66B2FF', '#99FF99']

bars = ax4.bar(datasets, accuracies, color=colors_acc, alpha=0.8)
ax4.set_title('Accuracy Comparison Across Datasets', fontweight='bold')
ax4.set_ylabel('Accuracy')
ax4.set_ylim(0, 1.1)

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()

# å®‰å…¨ä¿å­˜å›¾è¡¨
try:
    plt.savefig(os.path.join(output_dir, 'decision_tree_performance_analysis.png'), 
                dpi=300, bbox_inches='tight')
    print("ğŸ“Š æ€§èƒ½åˆ†æå›¾å·²ä¿å­˜ä¸º: output/decision_tree_performance_analysis.png")
except Exception as e:
    print(f"âš ï¸  æ€§èƒ½åˆ†æå›¾ä¿å­˜å¤±è´¥: {e}")
    print("ğŸ“Š å›¾è¡¨ä»åœ¨å†…å­˜ä¸­æ˜¾ç¤º")

plt.show()

# 15. ç”Ÿæˆå®Œæ•´çš„å†³ç­–æ ‘æ–‡æœ¬è§„åˆ™
print(f"\n1ï¸âƒ£5ï¸âƒ£ ç”Ÿæˆå®Œæ•´å†³ç­–æ ‘è§„åˆ™...")

# å¯¼å‡ºå®Œæ•´çš„å†³ç­–æ ‘è§„åˆ™åˆ°æ–‡ä»¶
try:
    full_tree_rules = export_text(dt_model, feature_names=selected_features)
    rules_file = os.path.join(output_dir, 'decision_tree_rules.txt')
    with open(rules_file, 'w', encoding='utf-8') as f:
        f.write("Decision Tree Rules for PCDN Traffic Classification\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"Model Parameters:\n")
        for param, value in grid_search.best_params_.items():
            f.write(f"  {param}: {value}\n")
        f.write(f"\nTree Structure:\n")
        f.write(f"  Tree Depth: {dt_model.get_depth()}\n")
        f.write(f"  Number of Leaves: {dt_model.get_n_leaves()}\n")
        f.write(f"  Total Nodes: {dt_model.tree_.node_count}\n\n")
        f.write("Decision Rules:\n")
        f.write("-" * 40 + "\n")
        f.write(full_tree_rules)
    
    print(f"ğŸ“ å®Œæ•´å†³ç­–æ ‘è§„åˆ™å·²ä¿å­˜ä¸º: output/decision_tree_rules.txt")
except Exception as e:
    print(f"âš ï¸  å†³ç­–æ ‘è§„åˆ™ä¿å­˜å¤±è´¥: {e}")

# 16. æ€»ç»“æŠ¥å‘Š
print(f"\n1ï¸âƒ£6ï¸âƒ£ å†³ç­–æ ‘åˆ†ç±»ä»»åŠ¡æ€»ç»“æŠ¥å‘Š")
print("=" * 60)
print(f"ğŸ¯ ä»»åŠ¡: PCDNæµé‡ä¸æ­£å¸¸æµé‡äºŒåˆ†ç±»")
print(f"ğŸŒ³ æ¨¡å‹: Decision Tree Classifier")
print(f"ğŸ”§ ä½¿ç”¨ç‰¹å¾: {', '.join(selected_features)}")
print(f"ğŸ“Š æ•°æ®è§„æ¨¡:")
print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

print(f"\nğŸ† æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:")
print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"  æµ‹è¯•é›†AUCå€¼: {test_auc:.4f}")

print(f"\nğŸŒ³ å†³ç­–æ ‘æ¨¡å‹å‚æ•°:")
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")

print(f"\nğŸŒ³ å†³ç­–æ ‘ç»“æ„:")
print(f"  æ ‘çš„æ·±åº¦: {dt_model.get_depth()}")
print(f"  å¶å­èŠ‚ç‚¹æ•°: {dt_model.get_n_leaves()}")
print(f"  æ€»èŠ‚ç‚¹æ•°: {dt_model.tree_.node_count}")

print(f"\nğŸ“ˆ ç‰¹å¾é‡è¦æ€§:")
for idx, row in importance_df.iterrows():
    percentage = (row['importance'] / importance_df['importance'].sum()) * 100
    print(f"  {row['feature']}: {row['importance']:.4f} ({percentage:.1f}%)")

print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"  å†³ç­–æ ‘å¯è§†åŒ–: output/decision_tree_single_clear.png")
print(f"  æ€§èƒ½åˆ†æå›¾: output/decision_tree_performance_analysis.png")
print(f"  å†³ç­–è§„åˆ™æ–‡ä»¶: output/decision_tree_rules.txt")

print(f"\nâœ… å†³ç­–æ ‘åˆ†æå®Œæˆ!")
print("=" * 60)
