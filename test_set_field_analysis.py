#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Testing_setå­—æ®µåˆ†å¸ƒåˆ†æå·¥å…·
ç”¨äºæ£€æµ‹æ•°æ®æ³„éœ²å’Œå­—æ®µåˆ†å¸ƒå¼‚å¸¸
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

def normalize_data_types(df):
    """
    æ ‡å‡†åŒ–æ•°æ®ç±»å‹ï¼Œè§£å†³0å’Œ0.0ä¸ä¸€è‡´é—®é¢˜
    """
    df_normalized = df.copy()
    
    # æ•°å€¼å­—æ®µåˆ—è¡¨
    numeric_fields = [
        'frame.number', 'frame.time_relative', 'ip.version', 'ip.hdr_len', 
        'ip.dsfield', 'ip.len', 'ip.ttl', 'ip.proto', 'ipv6.plen', 'ipv6.nxt',
        'tcp.srcport', 'tcp.dstport', 'tcp.hdr_len', 'tcp.flags.syn', 
        'tcp.flags.ack', 'tcp.payload', 'udp.srcport', 'udp.dstport', 
        'udp.length', 'pcap_duration', 'srcport', 'dstport', 'ulProtoID',
        'dpi_rule_pkt', 'dpi_packets', 'dpi_bytes', 'flow_uplink_traffic',
        'flow_downlink_traffic', 'sum_pkt_len', 'total_pkts', 'srcport_cls',
        'dstport_cls', 'pkt_len_avg', 'pkt_len_max', 'pkt_len_min',
        'up_pkts', 'up_bytes', 'down_pkts', 'down_bytes', 'up_pkt_ratio',
        'down_pkt_ratio', 'up_byte_ratio', 'down_byte_ratio', 
        'up_down_pkt_ratio', 'up_down_byte_ratio', 'iat_avg', 'iat_max',
        'iat_min', 'avg_speed', 'avg_pkt_speed', 'max_burst'
    ]
    
    # ç»Ÿä¸€è½¬æ¢æ•°å€¼å­—æ®µä¸ºfloat64
    for field in numeric_fields:
        if field in df_normalized.columns:
            try:
                # å…ˆè½¬æ¢ä¸ºæ•°å€¼ï¼Œç„¶åç»Ÿä¸€ä¸ºfloat64
                df_normalized[field] = pd.to_numeric(df_normalized[field], errors='coerce').astype('float64')
            except Exception as e:
                print(f"âš ï¸ å­—æ®µ {field} ç±»å‹è½¬æ¢å¤±è´¥: {e}")
    
    return df_normalized

def check_data_type_consistency(df):
    """
    æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§
    """
    # å…³é”®æ•°å€¼å­—æ®µ
    key_fields = ['tcp.payload', 'tcp.srcport', 'tcp.dstport', 'ip.len', 'udp.length', 
                  'frame.number', 'ip.version', 'pkt_len_avg', 'up_bytes', 'down_bytes']
    
    inconsistent_fields = []
    
    for field in key_fields:
        if field in df.columns:
            app0_data = df[df['label'] == 0][field]
            app1_data = df[df['label'] == 1][field]
            
            if app0_data.dtype != app1_data.dtype:
                inconsistent_fields.append(field)
                print(f"  âŒ {field}: APP_0={app0_data.dtype}, APP_1={app1_data.dtype}")
            else:
                print(f"  âœ… {field}: {app0_data.dtype}")
    
    if not inconsistent_fields:
        print("  ğŸ‰ æ‰€æœ‰å…³é”®å­—æ®µæ•°æ®ç±»å‹ä¸€è‡´ï¼")
    else:
        print(f"  âš ï¸ å‘ç° {len(inconsistent_fields)} ä¸ªä¸ä¸€è‡´å­—æ®µ")
    
    return inconsistent_fields

def load_testing_data():
    """
    åŠ è½½Testing_setæ•°æ®å¹¶æ ‡å‡†åŒ–æ•°æ®ç±»å‹
    """
    base_path = "pcdn_32_pkts_2class_feature_enhance_v17.4_dataset/Testing_set"
    
    dataframes = []
    file_info = []
    
    for app_dir in ['APP_0', 'APP_1']:
        app_path = os.path.join(base_path, app_dir)
        if os.path.exists(app_path):
            csv_files = glob.glob(os.path.join(app_path, '*.csv'))
            
            for file_path in csv_files:
                try:
                    df = pd.read_csv(file_path)
                    
                    # æ ‡å‡†åŒ–æ•°æ®ç±»å‹
                    df = normalize_data_types(df)
                    
                    df['source_file'] = os.path.basename(file_path)
                    df['app_category'] = app_dir
                    df['label'] = 0 if app_dir == 'APP_0' else 1
                    
                    dataframes.append(df)
                    file_info.append({
                        'file': os.path.basename(file_path),
                        'category': app_dir,
                        'rows': len(df),
                        'columns': len(df.columns)
                    })
                    print(f"âœ… åŠ è½½: {os.path.basename(file_path)} ({app_dir}) - {len(df)} è¡Œ")
                    
                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥: {file_path} - {e}")
    
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"\nğŸ“Š Testing_setæ€»è§ˆ:")
        print(f"  æ€»æ–‡ä»¶æ•°: {len(file_info)}")
        print(f"  æ€»è¡Œæ•°: {len(combined_df)}")
        print(f"  åˆ—æ•°: {len(combined_df.columns)}")
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: {combined_df['label'].value_counts().to_dict()}")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§
        print(f"\nğŸ” æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥:")
        check_data_type_consistency(combined_df)
        
        return combined_df, file_info
    else:
        print("âŒ æœªæ‰¾åˆ°Testing_setæ•°æ®")
        return None, []

def analyze_field_distribution(df, field_name, top_n=20):
    """
    åˆ†ææŒ‡å®šå­—æ®µçš„åˆ†å¸ƒæƒ…å†µ
    """
    if field_name not in df.columns:
        print(f"âŒ å­—æ®µ '{field_name}' ä¸å­˜åœ¨")
        print(f"å¯ç”¨å­—æ®µ: {list(df.columns)}")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ” å­—æ®µåˆ†æ: {field_name}")
    print(f"{'='*60}")
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ•°æ®ç±»å‹: {df[field_name].dtype}")
    print(f"  éç©ºå€¼æ•°é‡: {df[field_name].notna().sum()}")
    print(f"  ç©ºå€¼æ•°é‡: {df[field_name].isna().sum()}")
    print(f"  å”¯ä¸€å€¼æ•°é‡: {df[field_name].nunique()}")
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†æ
    print(f"\nğŸ·ï¸ æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†æ:")
    for label in sorted(df['label'].unique()):
        label_name = "æ­£å¸¸æµé‡" if label == 0 else "PCDNæµé‡"
        subset = df[df['label'] == label]
        print(f"\n  {label_name} (æ ‡ç­¾={label}):")
        print(f"    æ ·æœ¬æ•°: {len(subset)}")
        print(f"    å”¯ä¸€å€¼æ•°: {subset[field_name].nunique()}")
        
        if df[field_name].dtype in ['object', 'string']:
            # å­—ç¬¦ä¸²ç±»å‹ - æ˜¾ç¤ºæœ€é¢‘ç¹çš„å€¼
            value_counts = subset[field_name].value_counts()
            print(f"    æœ€é¢‘ç¹çš„å€¼ (å‰{min(top_n, len(value_counts))}ä¸ª):")
            for i, (value, count) in enumerate(value_counts.head(top_n).items()):
                percentage = (count / len(subset)) * 100
                print(f"      {i+1:2d}. '{value}' - {count}æ¬¡ ({percentage:.1f}%)")
        else:
            # æ•°å€¼ç±»å‹ - æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"    ç»Ÿè®¡ä¿¡æ¯:")
            print(f"      æœ€å°å€¼: {subset[field_name].min()}")
            print(f"      æœ€å¤§å€¼: {subset[field_name].max()}")
            print(f"      å¹³å‡å€¼: {subset[field_name].mean():.4f}")
            print(f"      ä¸­ä½æ•°: {subset[field_name].median():.4f}")
            print(f"      æ ‡å‡†å·®: {subset[field_name].std():.4f}")
    
    # æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§
    print(f"\nğŸ” æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥:")
    app0_data = df[df['label'] == 0][field_name]
    app1_data = df[df['label'] == 1][field_name]
    
    if app0_data.dtype != app1_data.dtype:
        print(f"  âŒ æ•°æ®ç±»å‹ä¸ä¸€è‡´: APP_0={app0_data.dtype}, APP_1={app1_data.dtype}")
        print(f"  ğŸ’¡ å»ºè®®: ä½¿ç”¨ normalize_data_types() å‡½æ•°ç»Ÿä¸€æ•°æ®ç±»å‹")
    else:
        print(f"  âœ… æ•°æ®ç±»å‹ä¸€è‡´: {app0_data.dtype}")
    
    # æ£€æŸ¥æ•°æ®æ³„éœ²é£é™©
    print(f"\nğŸš¨ æ•°æ®æ³„éœ²é£é™©æ£€æŸ¥:")
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„å€¼åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­
    if df[field_name].dtype in ['object', 'string']:
        # å­—ç¬¦ä¸²ç±»å‹ - æ£€æŸ¥å€¼é‡å 
        app0_values = set(df[df['label'] == 0][field_name].dropna().unique())
        app1_values = set(df[df['label'] == 1][field_name].dropna().unique())
        
        overlap = app0_values & app1_values
        print(f"  å€¼é‡å æ£€æŸ¥:")
        print(f"    APP_0å”¯ä¸€å€¼æ•°: {len(app0_values)}")
        print(f"    APP_1å”¯ä¸€å€¼æ•°: {len(app1_values)}")
        print(f"    é‡å å€¼æ•°: {len(overlap)}")
        
        if overlap:
            print(f"    âš ï¸ å‘ç°é‡å å€¼ (å‰10ä¸ª): {list(overlap)[:10]}")
        else:
            print(f"    âœ… æ— é‡å å€¼")
    else:
        # æ•°å€¼ç±»å‹ - æ£€æŸ¥å€¼åˆ†å¸ƒå·®å¼‚
        print(f"  æ•°å€¼åˆ†å¸ƒæ£€æŸ¥:")
        app0_mean = app0_data.mean()
        app1_mean = app1_data.mean()
        app0_std = app0_data.std()
        app1_std = app1_data.std()
        
        print(f"    APP_0: å‡å€¼={app0_mean:.4f}, æ ‡å‡†å·®={app0_std:.4f}")
        print(f"    APP_1: å‡å€¼={app1_mean:.4f}, æ ‡å‡†å·®={app1_std:.4f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„åˆ†å¸ƒå·®å¼‚
        if abs(app0_mean - app1_mean) < 1e-10 and abs(app0_std - app1_std) < 1e-10:
            print(f"    âš ï¸ åˆ†å¸ƒå‡ ä¹å®Œå…¨ç›¸åŒï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²")
        else:
            print(f"    âœ… åˆ†å¸ƒå­˜åœ¨åˆç†å·®å¼‚")
    
    # 2. æ£€æŸ¥å­—æ®µæ˜¯å¦åŒ…å«æ˜æ˜¾çš„åˆ†ç±»ä¿¡æ¯
    suspicious_patterns = []
    if df[field_name].dtype in ['object', 'string']:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«IPåœ°å€ã€URLç­‰å¯èƒ½æ³„éœ²ä¿¡æ¯çš„å†…å®¹
        sample_values = df[field_name].dropna().astype(str).head(100)
        for value in sample_values:
            if any(pattern in str(value).lower() for pattern in ['http', 'www', 'com', 'net', 'org']):
                suspicious_patterns.append('URLæ¨¡å¼')
                break
            if '.' in str(value) and str(value).count('.') >= 3:
                suspicious_patterns.append('IPåœ°å€æ¨¡å¼')
                break
    
    if suspicious_patterns:
        print(f"    âš ï¸ å‘ç°å¯ç–‘æ¨¡å¼: {suspicious_patterns}")
    else:
        print(f"    âœ… æœªå‘ç°æ˜æ˜¾å¯ç–‘æ¨¡å¼")
    
    # 3. æ£€æŸ¥å­—æ®µåˆ†å¸ƒæ˜¯å¦è¿‡äºç®€å•
    if df[field_name].nunique() <= 2:
        print(f"    âš ï¸ å­—æ®µåªæœ‰{df[field_name].nunique()}ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½è¿‡äºç®€å•")
    elif df[field_name].nunique() > len(df) * 0.9:
        print(f"    âš ï¸ å­—æ®µå”¯ä¸€å€¼è¿‡å¤š({df[field_name].nunique()}/{len(df)})ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©")
    else:
        print(f"    âœ… å­—æ®µå”¯ä¸€å€¼æ•°é‡åˆç†")

def visualize_field_distribution(df, field_name, max_categories=20):
    """
    å¯è§†åŒ–å­—æ®µåˆ†å¸ƒ
    """
    if field_name not in df.columns:
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'å­—æ®µåˆ†å¸ƒåˆ†æ: {field_name}', fontsize=16)
    
    # 1. æ•´ä½“åˆ†å¸ƒ
    ax1 = axes[0, 0]
    if df[field_name].dtype in ['object', 'string']:
        value_counts = df[field_name].value_counts().head(max_categories)
        value_counts.plot(kind='bar', ax=ax1)
        ax1.set_title('æ•´ä½“åˆ†å¸ƒ (å‰20ä¸ªå€¼)')
        ax1.tick_params(axis='x', rotation=45)
    else:
        df[field_name].hist(bins=30, ax=ax1)
        ax1.set_title('æ•´ä½“åˆ†å¸ƒ')
    ax1.set_ylabel('é¢‘æ¬¡')
    
    # 2. æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†å¸ƒ
    ax2 = axes[0, 1]
    for label in sorted(df['label'].unique()):
        label_name = "æ­£å¸¸æµé‡" if label == 0 else "PCDNæµé‡"
        subset = df[df['label'] == label]
        
        if df[field_name].dtype in ['object', 'string']:
            value_counts = subset[field_name].value_counts().head(max_categories)
            ax2.plot(value_counts.index, value_counts.values, 'o-', label=label_name, alpha=0.7)
        else:
            ax2.hist(subset[field_name].dropna(), alpha=0.7, label=label_name, bins=20)
    
    ax2.set_title('æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†å¸ƒ')
    ax2.legend()
    ax2.tick_params(axis='x', rotation=45)
    
    # 3. æ ‡ç­¾åˆ†å¸ƒé¥¼å›¾
    ax3 = axes[1, 0]
    label_counts = df['label'].value_counts()
    labels = ['æ­£å¸¸æµé‡' if x == 0 else 'PCDNæµé‡' for x in label_counts.index]
    ax3.pie(label_counts.values, labels=labels, autopct='%1.1f%%')
    ax3.set_title('æ ‡ç­¾åˆ†å¸ƒ')
    
    # 4. å­—æ®µå€¼åˆ†å¸ƒé¥¼å›¾ (ä»…å¯¹åˆ†ç±»å­—æ®µ)
    ax4 = axes[1, 1]
    if df[field_name].dtype in ['object', 'string'] and df[field_name].nunique() <= 10:
        value_counts = df[field_name].value_counts()
        ax4.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')
        ax4.set_title(f'{field_name} å€¼åˆ†å¸ƒ')
    else:
        ax4.text(0.5, 0.5, f'å­—æ®µæœ‰{df[field_name].nunique()}ä¸ªå”¯ä¸€å€¼\næ— æ³•æ˜¾ç¤ºé¥¼å›¾', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title(f'{field_name} å€¼åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.show()

def fix_data_type_issues(df):
    """
    ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜
    """
    print("\nğŸ”§ ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜")
    print("=" * 50)
    
    # æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®ç±»å‹ä¸ä¸€è‡´
    inconsistent_fields = check_data_type_consistency(df)
    
    if inconsistent_fields:
        print(f"\nå‘ç° {len(inconsistent_fields)} ä¸ªä¸ä¸€è‡´å­—æ®µï¼Œå¼€å§‹ä¿®å¤...")
        df_fixed = normalize_data_types(df)
        
        # éªŒè¯ä¿®å¤æ•ˆæœ
        print("\néªŒè¯ä¿®å¤æ•ˆæœ:")
        check_data_type_consistency(df_fixed)
        
        return df_fixed
    else:
        print("\nâœ… æ•°æ®ç±»å‹å·²ä¸€è‡´ï¼Œæ— éœ€ä¿®å¤")
        return df

def comprehensive_analysis(df):
    """
    ç»¼åˆåˆ†ææ‰€æœ‰å­—æ®µ
    """
    print("\nğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    # 1. æ•°æ®ç±»å‹ç»Ÿè®¡
    print("\n1. æ•°æ®ç±»å‹ç»Ÿè®¡:")
    dtype_counts = df.dtypes.value_counts()
    for dtype, count in dtype_counts.items():
        print(f"   {dtype}: {count} ä¸ªå­—æ®µ")
    
    # 2. å…³é”®å­—æ®µåˆ†æ
    print("\n2. å…³é”®å­—æ®µåˆ†æ:")
    key_fields = ['tcp.payload', 'tcp.srcport', 'tcp.dstport', 'ip.len', 'udp.length']
    
    for field in key_fields:
        if field in df.columns:
            app0_data = df[df['label'] == 0][field]
            app1_data = df[df['label'] == 1][field]
            
            print(f"\n   {field}:")
            print(f"     æ•°æ®ç±»å‹: APP_0={app0_data.dtype}, APP_1={app1_data.dtype}")
            print(f"     å€¼èŒƒå›´: APP_0=[{app0_data.min():.2f}, {app0_data.max():.2f}], APP_1=[{app1_data.min():.2f}, {app1_data.max():.2f}]")
            print(f"     å‡å€¼: APP_0={app0_data.mean():.4f}, APP_1={app1_data.mean():.4f}")
    
    # 3. æ•°æ®æ³„éœ²é£é™©è¯„ä¼°
    print("\n3. æ•°æ®æ³„éœ²é£é™©è¯„ä¼°:")
    risk_fields = []
    
    for field in df.columns:
        if field in ['source_file', 'app_category', 'label']:
            continue
            
        app0_values = set(df[df['label'] == 0][field].dropna().unique())
        app1_values = set(df[df['label'] == 1][field].dropna().unique())
        
        if field in ['flow_id', 'dpi_file_name', 'dpi_five_tuple']:
            overlap = app0_values & app1_values
            if overlap:
                risk_fields.append(field)
                print(f"   âš ï¸ {field}: å‘ç° {len(overlap)} ä¸ªé‡å å€¼")
    
    if not risk_fields:
        print("   âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®æ³„éœ²é£é™©")
    else:
        print(f"   ğŸš¨ å‘ç° {len(risk_fields)} ä¸ªé«˜é£é™©å­—æ®µ: {', '.join(risk_fields)}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ” Testing_setå­—æ®µåˆ†å¸ƒåˆ†æå·¥å…· (ä¼˜åŒ–ç‰ˆ)")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    df, file_info = load_testing_data()
    if df is None:
        return
    
    # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜
    df = fix_data_type_issues(df)
    
    # ç»¼åˆåˆ†æ
    comprehensive_analysis(df)
    
    # æ˜¾ç¤ºå¯ç”¨å­—æ®µ
    print(f"\nğŸ“‹ å¯ç”¨å­—æ®µåˆ—è¡¨:")
    for i, col in enumerate(df.columns, 1):
        print(f"  {i:2d}. {col}")
    
    # äº¤äº’å¼åˆ†æ
    while True:
        print(f"\n{'='*60}")
        print("é€‰æ‹©æ“ä½œ:")
        print("1. åˆ†æç‰¹å®šå­—æ®µ")
        print("2. é‡æ–°æ£€æŸ¥æ•°æ®ç±»å‹")
        print("3. æŸ¥çœ‹ç»¼åˆåˆ†ææŠ¥å‘Š")
        print("4. é€€å‡º")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            field_name = input("è¯·è¾“å…¥è¦åˆ†æçš„å­—æ®µå: ").strip()
            
            if field_name not in df.columns:
                print(f"âŒ å­—æ®µ '{field_name}' ä¸å­˜åœ¨")
                continue
            
            # åˆ†æå­—æ®µ
            analyze_field_distribution(df, field_name)
            
            # è¯¢é—®æ˜¯å¦å¯è§†åŒ–
            show_plot = input("\næ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨? (y/n): ").strip().lower()
            if show_plot == 'y':
                visualize_field_distribution(df, field_name)
                
        elif choice == '2':
            check_data_type_consistency(df)
            
        elif choice == '3':
            comprehensive_analysis(df)
            
        elif choice == '4':
            break
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    print("\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
