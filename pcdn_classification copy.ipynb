{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'bysj (Python 3.10.18)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n bysj ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDN vs Normal Traffic Classification using XGBoost\n",
        "\n",
        "è¿™ä¸ªé¡¹ç›®ä½¿ç”¨XGBoostå¯¹æ­£å¸¸æµé‡å’ŒPCDNæµé‡è¿›è¡ŒäºŒåˆ†ç±»ã€‚\n",
        "\n",
        "## æ•°æ®é›†ç»“æ„\n",
        "- Training_set/APP_0: æ­£å¸¸æµé‡\n",
        "- Training_set/APP_1: PCDNæµé‡\n",
        "- Validation_set: éªŒè¯é›†\n",
        "- Testing_set: æµ‹è¯•é›†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from scipy import stats  # ç”¨äºåºåˆ—ç‰¹å¾çš„ååº¦å’Œå³°åº¦è®¡ç®—\n",
        "import ast  # ç”¨äºå®‰å…¨è§£ææ•°ç»„å­—ç¬¦ä¸²\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"åº“å¯¼å…¥å®Œæˆï¼\")\n",
        "\n",
        "# ===== é…ç½®å‚æ•° =====\n",
        "# æ§åˆ¶æ˜¯å¦å¯¹åºåˆ—ç‰¹å¾è¿›è¡Œç‰¹æ®Šå¤„ç†\n",
        "ENABLE_SEQUENCE_FEATURES = True  # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹: True=ç‰¹å¾å·¥ç¨‹, False=åˆ é™¤åºåˆ—ç‰¹å¾\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ”§ åºåˆ—ç‰¹å¾å¤„ç†æ¨¡å¼: {'å¯ç”¨ âœ…' if ENABLE_SEQUENCE_FEATURES else 'ç¦ç”¨ âŒ'}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(\"ğŸ“Š å¯ç”¨æ¨¡å¼ - å°†æ‰§è¡Œå¤æ‚çš„åºåˆ—ç‰¹å¾å·¥ç¨‹:\")\n",
        "    print(\"   â€¢ ip_direction â†’ 15ä¸ªç»Ÿè®¡ç‰¹å¾ (æ–¹å‘æ¨¡å¼åˆ†æ)\")\n",
        "    print(\"   â€¢ pkt_len â†’ 15ä¸ªç»Ÿè®¡ç‰¹å¾ (åŒ…å¤§å°æ¨¡å¼åˆ†æ)\")  \n",
        "    print(\"   â€¢ iat â†’ 15ä¸ªç»Ÿè®¡ç‰¹å¾ (æ—¶é—´é—´éš”æ¨¡å¼åˆ†æ)\")\n",
        "    print(\"   â€¢ æ€»è®¡ç”Ÿæˆ 45+ ä¸ªæ–°ç‰¹å¾\")\n",
        "    print(\"   ğŸ’¡ é€‚åˆ: è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œå……åˆ†åˆ©ç”¨æ—¶åºä¿¡æ¯\")\n",
        "else:\n",
        "    print(\"ğŸ—‘ï¸ ç¦ç”¨æ¨¡å¼ - å°†åˆ é™¤åºåˆ—ç‰¹å¾:\")\n",
        "    print(\"   â€¢ ç›´æ¥åˆ é™¤ ip_direction, pkt_len, iat\")\n",
        "    print(\"   â€¢ ä»…ä½¿ç”¨å…¶ä»–ç½‘ç»œæµé‡ç‰¹å¾è®­ç»ƒ\")\n",
        "    print(\"   â€¢ æ¨¡å‹æ›´ç®€å•ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«\")\n",
        "    print(\"   ğŸ’¡ é€‚åˆ: èµ„æºæœ‰é™ï¼Œæˆ–å¸Œæœ›ç®€åŒ–æ¨¡å‹çš„åœºæ™¯\")\n",
        "\n",
        "print(\"\\nğŸ’­ è¦åˆ‡æ¢æ¨¡å¼ï¼Œè¯·ä¿®æ”¹ä¸Šæ–¹ ENABLE_SEQUENCE_FEATURES çš„å€¼\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®åŠ è½½å‡½æ•°\n",
        "def load_data_from_directory(base_path, label):\n",
        "    \"\"\"\n",
        "    ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰CSVæ–‡ä»¶å¹¶æ·»åŠ æ ‡ç­¾ - å¢å¼ºç‰ˆï¼šç¡®ä¿åˆ—ä¸€è‡´æ€§\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(base_path, '*.csv'))\n",
        "    print(f\"åœ¨ {base_path} ä¸­æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶\")\n",
        "    \n",
        "    dataframes = []\n",
        "    all_columns_info = []  # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„åˆ—ä¿¡æ¯\n",
        "    \n",
        "    # ğŸ” ç¬¬ä¸€éæ‰«æï¼šæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„åˆ—ç»“æ„\n",
        "    print(f\"\\nğŸ” æ£€æŸ¥CSVæ–‡ä»¶åˆ—ç»“æ„ ({os.path.basename(base_path)}):\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            # åªè¯»å–ç¬¬ä¸€è¡Œæ¥è·å–åˆ—åï¼Œé¿å…åŠ è½½æ•´ä¸ªæ–‡ä»¶\n",
        "            df_sample = pd.read_csv(file, nrows=1)\n",
        "            file_columns = list(df_sample.columns)\n",
        "            all_columns_info.append({\n",
        "                'file': file,\n",
        "                'columns': file_columns,\n",
        "                'count': len(file_columns)\n",
        "            })\n",
        "            \n",
        "            print(f\"  æ–‡ä»¶{i+1}: {os.path.basename(file)} -> {len(file_columns)}åˆ—\")\n",
        "            \n",
        "            # æ˜¾ç¤ºå‰å‡ ä¸ªå’Œåå‡ ä¸ªåˆ—å\n",
        "            if len(file_columns) <= 6:\n",
        "                print(f\"    åˆ—å: {file_columns}\")\n",
        "            else:\n",
        "                print(f\"    åˆ—å: {file_columns[:3]} ... {file_columns[-3:]}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ æ£€æŸ¥æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}\")\n",
        "    \n",
        "    # ğŸ” åˆ†æåˆ—æ•°å·®å¼‚\n",
        "    column_counts = [info['count'] for info in all_columns_info]\n",
        "    unique_counts = sorted(set(column_counts))\n",
        "    \n",
        "    if len(unique_counts) > 1:\n",
        "        print(f\"\\nâŒ å‘ç°åˆ—æ•°ä¸ä¸€è‡´é—®é¢˜ï¼\")\n",
        "        for count in unique_counts:\n",
        "            files_with_count = [f\"æ–‡ä»¶{i+1}({os.path.basename(info['file'])})\" \n",
        "                              for i, info in enumerate(all_columns_info) \n",
        "                              if info['count'] == count]\n",
        "            print(f\"  ğŸ“Š {count}åˆ—: {', '.join(files_with_count)}\")\n",
        "        \n",
        "        # ğŸ”§ è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„å…±åŒåˆ—\n",
        "        if all_columns_info:\n",
        "            common_columns = set(all_columns_info[0]['columns'])\n",
        "            for info in all_columns_info[1:]:\n",
        "                common_columns = common_columns.intersection(set(info['columns']))\n",
        "            \n",
        "            common_columns_list = sorted(list(common_columns))\n",
        "            print(f\"\\nğŸ”§ æ‰€æœ‰æ–‡ä»¶çš„å…±åŒåˆ—æ•°: {len(common_columns_list)}\")\n",
        "            \n",
        "            # æ˜¾ç¤ºå„æ–‡ä»¶å°†è¢«æ’é™¤çš„åˆ—\n",
        "            print(\"ğŸ“ å„æ–‡ä»¶ç‹¬æœ‰çš„åˆ—:\")\n",
        "            for i, info in enumerate(all_columns_info):\n",
        "                excluded = set(info['columns']) - common_columns\n",
        "                if excluded:\n",
        "                    print(f\"  æ–‡ä»¶{i+1}: {sorted(list(excluded))}\")\n",
        "                else:\n",
        "                    print(f\"  æ–‡ä»¶{i+1}: æ— ç‹¬æœ‰åˆ—\")\n",
        "                    \n",
        "            print(f\"âœ… å°†ç»Ÿä¸€ä½¿ç”¨ {len(common_columns_list)} ä¸ªå…±åŒåˆ—\")\n",
        "            target_columns = common_columns_list\n",
        "        else:\n",
        "            target_columns = None\n",
        "            print(\"âŒ æ— æ³•ç¡®å®šå…±åŒåˆ—\")\n",
        "    else:\n",
        "        print(f\"âœ… æ‰€æœ‰æ–‡ä»¶åˆ—æ•°ä¸€è‡´: {column_counts[0]}åˆ—\")\n",
        "        target_columns = None\n",
        "    \n",
        "    # ğŸ“Š ç¬¬äºŒéï¼šä½¿ç”¨ç»Ÿä¸€åˆ—ç»“æ„åŠ è½½æ•°æ®\n",
        "    print(f\"\\nğŸ“Š æ­£å¼åŠ è½½æ•°æ®:\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            original_shape = df.shape\n",
        "            \n",
        "            # å¦‚æœéœ€è¦ï¼Œåªä¿ç•™å…±åŒåˆ—\n",
        "            if target_columns is not None:\n",
        "                df = df[target_columns]\n",
        "                print(f\"  ğŸ”§ æ–‡ä»¶{i+1}: {original_shape} -> {df.shape} (åˆ—å¯¹é½)\")\n",
        "            \n",
        "            df['label'] = label  # æ·»åŠ æ ‡ç­¾åˆ—\n",
        "            df['source_file'] = os.path.basename(file)  # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯\n",
        "            dataframes.append(df)\n",
        "            print(f\"  âœ… {os.path.basename(file)}: {len(df)}è¡Œ x {len(df.columns)}åˆ—\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ åŠ è½½æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}\")\n",
        "    \n",
        "    # ğŸ¯ æœ€ç»ˆåˆå¹¶å’ŒéªŒè¯\n",
        "    if dataframes:\n",
        "        # æœ€åæ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰dataframeåˆ—æ•°ä¸€è‡´\n",
        "        final_shapes = [df.shape for df in dataframes]\n",
        "        final_col_counts = [shape[1] for shape in final_shapes]\n",
        "        \n",
        "        if len(set(final_col_counts)) == 1:\n",
        "            print(f\"âœ… åˆå¹¶å‰æœ€ç»ˆæ£€æŸ¥é€šè¿‡: æ‰€æœ‰æ–‡ä»¶å‡ä¸º {final_col_counts[0]} åˆ—\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ åˆå¹¶å‰å‘ç°åˆ—æ•°å·®å¼‚: {final_col_counts}\")\n",
        "        \n",
        "        result = pd.concat(dataframes, ignore_index=True)\n",
        "        \n",
        "        print(f\"ğŸ‰ æ•°æ®åŠ è½½å®Œæˆ:\")\n",
        "        print(f\"  ğŸ“ æ–‡ä»¶æ•°: {len(csv_files)}\")\n",
        "        print(f\"  ğŸ“Š æœ€ç»ˆå½¢çŠ¶: {result.shape}\")\n",
        "        print(f\"  ğŸ·ï¸ æ ‡ç­¾å€¼: {label}\")\n",
        "        \n",
        "        # ğŸ“‹ è¿”å›ç»“æœå’Œåˆ—ä¿¡æ¯\n",
        "        columns_meta = {\n",
        "            'final_columns': list(result.columns),\n",
        "            'original_columns_info': all_columns_info,\n",
        "            'target_columns': target_columns,\n",
        "            'dataset_path': base_path\n",
        "        }\n",
        "        \n",
        "        return result, columns_meta\n",
        "    else:\n",
        "        print(\"âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡ä»¶\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "\n",
        "def analyze_cross_dataset_columns(datasets_info):\n",
        "    \"\"\"\n",
        "    åˆ†æä¸åŒæ•°æ®é›†ä¹‹é—´çš„åˆ—å·®å¼‚\n",
        "    datasets_info: å­—å…¸ï¼Œæ ¼å¼ä¸º {'æ•°æ®é›†åç§°': columns_meta}\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ” è·¨æ•°æ®é›†åˆ—ç»“æ„ä¸€è‡´æ€§åˆ†æ\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„åˆ—ä¿¡æ¯\n",
        "    dataset_columns = {}\n",
        "    for dataset_name, meta in datasets_info.items():\n",
        "        if 'final_columns' in meta and meta['final_columns']:\n",
        "            # æ’é™¤æˆ‘ä»¬æ·»åŠ çš„è¾…åŠ©åˆ—\n",
        "            original_cols = [col for col in meta['final_columns'] \n",
        "                           if col not in ['label', 'source_file']]\n",
        "            dataset_columns[dataset_name] = set(original_cols)\n",
        "            print(f\"ğŸ“Š {dataset_name}: {len(original_cols)}ä¸ªåŸå§‹åˆ—\")\n",
        "    \n",
        "    if len(dataset_columns) < 2:\n",
        "        print(\"âš ï¸ æ•°æ®é›†æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè·¨æ•°æ®é›†æ¯”è¾ƒ\")\n",
        "        return\n",
        "    \n",
        "    # è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„åˆ—ç»Ÿè®¡\n",
        "    all_datasets = list(dataset_columns.keys())\n",
        "    column_counts = {name: len(cols) for name, cols in dataset_columns.items()}\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ åˆ—æ•°ç»Ÿè®¡:\")\n",
        "    for dataset, count in column_counts.items():\n",
        "        print(f\"  {dataset}: {count}åˆ—\")\n",
        "    \n",
        "    # æ£€æŸ¥åˆ—æ•°æ˜¯å¦ä¸€è‡´\n",
        "    unique_counts = set(column_counts.values())\n",
        "    if len(unique_counts) == 1:\n",
        "        print(\"âœ… æ‰€æœ‰æ•°æ®é›†åˆ—æ•°ä¸€è‡´\")\n",
        "    else:\n",
        "        print(f\"âŒ å‘ç°åˆ—æ•°ä¸ä¸€è‡´: {sorted(unique_counts)}\")\n",
        "    \n",
        "    # è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„å…±åŒåˆ—å’Œç‹¬æœ‰åˆ—\n",
        "    all_columns = set()\n",
        "    for cols in dataset_columns.values():\n",
        "        all_columns = all_columns.union(cols)\n",
        "    \n",
        "    common_columns = set.intersection(*dataset_columns.values()) if dataset_columns else set()\n",
        "    \n",
        "    print(f\"\\nğŸ”§ åˆ—åˆ†æç»“æœ:\")\n",
        "    print(f\"  æ‰€æœ‰å”¯ä¸€åˆ—æ•°: {len(all_columns)}\")\n",
        "    print(f\"  å…±åŒåˆ—æ•°: {len(common_columns)}\")\n",
        "    print(f\"  å·®å¼‚åˆ—æ•°: {len(all_columns) - len(common_columns)}\")\n",
        "    \n",
        "    # è¯¦ç»†åˆ†ææ¯ä¸ªæ•°æ®é›†çš„ç‹¬æœ‰åˆ—\n",
        "    print(f\"\\nğŸ“ å„æ•°æ®é›†ç‹¬æœ‰åˆ—åˆ†æ:\")\n",
        "    for dataset, cols in dataset_columns.items():\n",
        "        unique_to_dataset = cols - common_columns\n",
        "        shared_with_others = cols - unique_to_dataset\n",
        "        \n",
        "        print(f\"\\n  ğŸ” {dataset}:\")\n",
        "        print(f\"    æ€»åˆ—æ•°: {len(cols)}\")\n",
        "        print(f\"    ä¸å…¶ä»–æ•°æ®é›†å…±åŒçš„åˆ—: {len(shared_with_others)}\")\n",
        "        print(f\"    ç‹¬æœ‰åˆ—æ•°: {len(unique_to_dataset)}\")\n",
        "        \n",
        "        if unique_to_dataset:\n",
        "            print(f\"    ç‹¬æœ‰åˆ—å: {sorted(list(unique_to_dataset))}\")\n",
        "        \n",
        "        # æ‰¾å‡ºè¯¥æ•°æ®é›†ç¼ºå¤±ä½†å…¶ä»–æ•°æ®é›†æœ‰çš„åˆ—\n",
        "        missing_columns = common_columns - cols\n",
        "        if missing_columns:\n",
        "            print(f\"    ç¼ºå¤±çš„å…±åŒåˆ—: {sorted(list(missing_columns))}\")\n",
        "    \n",
        "    # ç”Ÿæˆä¸¤ä¸¤å¯¹æ¯”\n",
        "    print(f\"\\nğŸ”„ ä¸¤ä¸¤æ•°æ®é›†å¯¹æ¯”:\")\n",
        "    datasets_list = list(dataset_columns.keys())\n",
        "    for i in range(len(datasets_list)):\n",
        "        for j in range(i+1, len(datasets_list)):\n",
        "            dataset1, dataset2 = datasets_list[i], datasets_list[j]\n",
        "            cols1, cols2 = dataset_columns[dataset1], dataset_columns[dataset2]\n",
        "            \n",
        "            common = cols1.intersection(cols2)\n",
        "            only_in_1 = cols1 - cols2\n",
        "            only_in_2 = cols2 - cols1\n",
        "            \n",
        "            print(f\"\\n  ğŸ“Š {dataset1} vs {dataset2}:\")\n",
        "            print(f\"    å…±åŒåˆ—: {len(common)}\")\n",
        "            print(f\"    ä»…{dataset1}æœ‰: {len(only_in_1)}\")\n",
        "            print(f\"    ä»…{dataset2}æœ‰: {len(only_in_2)}\")\n",
        "            \n",
        "            if only_in_1:\n",
        "                print(f\"    ä»…{dataset1}æœ‰çš„åˆ—: {sorted(list(only_in_1))}\")\n",
        "            if only_in_2:\n",
        "                print(f\"    ä»…{dataset2}æœ‰çš„åˆ—: {sorted(list(only_in_2))}\")\n",
        "    \n",
        "    # æ¨èçš„ç»Ÿä¸€ç­–ç•¥\n",
        "    print(f\"\\nğŸ’¡ ç»Ÿä¸€ç­–ç•¥å»ºè®®:\")\n",
        "    if len(common_columns) == len(all_columns):\n",
        "        print(\"âœ… æ‰€æœ‰æ•°æ®é›†åˆ—å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€è°ƒæ•´\")\n",
        "    else:\n",
        "        print(f\"ğŸ”§ å»ºè®®ç»Ÿä¸€åˆ°å…±åŒåˆ—é›†åˆ ({len(common_columns)}åˆ—)\")\n",
        "        print(f\"   è¿™å°†ç¡®ä¿æ‰€æœ‰æ•°æ®é›†å…·æœ‰ç›¸åŒçš„ç‰¹å¾ç»“æ„\")\n",
        "        \n",
        "        lost_columns = all_columns - common_columns\n",
        "        if lost_columns:\n",
        "            print(f\"   âš ï¸ å°†ä¸¢å¤±çš„åˆ—: {sorted(list(lost_columns))}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "\n",
        "# ä½¿ç”¨ç›¸å¯¹è·¯å¾„å®šä¹‰æ•°æ®ç›®å½•\n",
        "# æ•°æ®é›†åº”è¯¥ä¸æ­¤notebookåœ¨åŒä¸€ç›®å½•ä¸‹\n",
        "data_dir = './pcdn_32_pkts_2class_feature_enhance_v17.4_dataset'\n",
        "\n",
        "# æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨\n",
        "if not os.path.exists(data_dir):\n",
        "    print(f\"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}\")\n",
        "    print(\"è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å¤¹ä¸notebookåœ¨åŒä¸€ç›®å½•ä¸‹\")\n",
        "    print(\"å½“å‰å·¥ä½œç›®å½•:\", os.getcwd())\n",
        "    print(\"å½“å‰ç›®å½•å†…å®¹:\", [f for f in os.listdir('.') if not f.startswith('.')])\n",
        "    \n",
        "    # å°è¯•æŸ¥æ‰¾æ•°æ®ç›®å½•\n",
        "    possible_dirs = [d for d in os.listdir('.') if 'pcdn' in d.lower() and os.path.isdir(d)]\n",
        "    if possible_dirs:\n",
        "        print(f\"å‘ç°å¯èƒ½çš„æ•°æ®ç›®å½•: {possible_dirs}\")\n",
        "        data_dir = possible_dirs[0]\n",
        "        print(f\"ä½¿ç”¨æ•°æ®ç›®å½•: {data_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"æ‰¾ä¸åˆ°æ•°æ®ç›®å½•ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ä½ç½®\")\n",
        "else:\n",
        "    print(f\"âœ… æ‰¾åˆ°æ•°æ®ç›®å½•: {data_dir}\")\n",
        "\n",
        "# åŠ è½½è®­ç»ƒæ•°æ®\n",
        "print(\"\\nå¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®...\")\n",
        "train_normal = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)  # æ­£å¸¸æµé‡æ ‡ç­¾ä¸º0\n",
        "train_pcdn = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)    # PCDNæµé‡æ ‡ç­¾ä¸º1\n",
        "\n",
        "# åˆå¹¶è®­ç»ƒæ•°æ®\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "print(f\"\\nè®­ç»ƒæ•°æ®åŠ è½½å®Œæˆï¼\")\n",
        "print(f\"æ­£å¸¸æµé‡æ ·æœ¬æ•°: {len(train_normal)}\")\n",
        "print(f\"PCDNæµé‡æ ·æœ¬æ•°: {len(train_pcdn)}\")\n",
        "print(f\"æ€»è®­ç»ƒæ ·æœ¬æ•°: {len(train_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” æ•°æ®åŠ è½½é˜¶æ®µçš„æ ¹æœ¬é—®é¢˜å‘ç°\n",
        "\n",
        "### ğŸš¨ é—®é¢˜åˆ†æ\n",
        "æ‚¨çš„åˆ†æå®Œå…¨æ­£ç¡®ï¼**\"è®­ç»ƒé›†47ä¸ªç‰¹å¾ï¼Œæµ‹è¯•é›†48ä¸ªç‰¹å¾\"é—®é¢˜çš„æ ¹æºå¾ˆå¯èƒ½åœ¨æ•°æ®åŠ è½½é˜¶æ®µ**ã€‚\n",
        "\n",
        "### ğŸ” æ½œåœ¨åŸå› \n",
        "å½“ä½¿ç”¨ `pd.concat()` åˆå¹¶å¤šä¸ªCSVæ–‡ä»¶æ—¶ï¼Œå¦‚æœä¸åŒCSVæ–‡ä»¶çš„åˆ—æ•°ä¸ä¸€è‡´ï¼š\n",
        "\n",
        "```python\n",
        "# åŸæ¥çš„é—®é¢˜ä»£ç \n",
        "dataframes = []\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)  # ä¸åŒfileå¯èƒ½æœ‰ä¸åŒåˆ—æ•°ï¼\n",
        "    dataframes.append(df)\n",
        "return pd.concat(dataframes, ignore_index=True)  # ä¼šå–æ‰€æœ‰åˆ—çš„å¹¶é›†ï¼\n",
        "```\n",
        "\n",
        "**å…·ä½“åœºæ™¯ï¼š**\n",
        "- ğŸ“ è®­ç»ƒé›†CSVæ–‡ä»¶ï¼šéƒ½æœ‰47ä¸ªåŸå§‹åˆ—\n",
        "- ğŸ“ éªŒè¯é›†CSVæ–‡ä»¶ï¼šéƒ½æœ‰47ä¸ªåŸå§‹åˆ—  \n",
        "- ğŸ“ æµ‹è¯•é›†CSVæ–‡ä»¶ï¼šæŸä¸ªæ–‡ä»¶æœ‰48ä¸ªåˆ—ï¼ˆå¤šäº†1ä¸ªé¢å¤–åˆ—ï¼‰\n",
        "\n",
        "**ç»“æœï¼š**\n",
        "- ğŸ”§ è®­ç»ƒé›†åˆå¹¶åï¼š47ä¸ªç‰¹å¾ + NaNå¡«å……\n",
        "- ğŸ”§ éªŒè¯é›†åˆå¹¶åï¼š47ä¸ªç‰¹å¾ + NaNå¡«å……\n",
        "- ğŸ”§ æµ‹è¯•é›†åˆå¹¶åï¼š48ä¸ªç‰¹å¾ï¼ˆ47ä¸ªå…±åŒ + 1ä¸ªç‹¬æœ‰ï¼‰\n",
        "\n",
        "### âœ… è§£å†³æ–¹æ¡ˆ\n",
        "**æ–°çš„æ•°æ®åŠ è½½å‡½æ•°**å®ç°äº†ï¼š\n",
        "\n",
        "#### ğŸ” ä¸¤é˜¶æ®µæ£€æŸ¥\n",
        "1. **ç¬¬ä¸€é˜¶æ®µ**ï¼šå¿«é€Ÿæ‰«ææ‰€æœ‰CSVæ–‡ä»¶çš„åˆ—ç»“æ„ï¼ˆåªè¯»ç¬¬1è¡Œï¼‰\n",
        "2. **ç¬¬äºŒé˜¶æ®µ**ï¼šä½¿ç”¨ç»Ÿä¸€çš„åˆ—é›†åˆåŠ è½½æ‰€æœ‰æ•°æ®\n",
        "\n",
        "#### ğŸ“Š è¯¦ç»†è¯Šæ–­\n",
        "- æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„ç¡®åˆ‡åˆ—æ•°\n",
        "- æ ‡è¯†å“ªäº›æ–‡ä»¶æœ‰é¢å¤–çš„åˆ—\n",
        "- è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„**å…±åŒåˆ—é›†**\n",
        "\n",
        "#### ğŸ”§ è‡ªåŠ¨ä¿®å¤\n",
        "- è‡ªåŠ¨å¯¹é½åˆ°å…±åŒåˆ—é›†\n",
        "- æ’é™¤å„æ–‡ä»¶çš„ç‹¬æœ‰åˆ—\n",
        "- ç¡®ä¿æ‰€æœ‰æ•°æ®é›†å…·æœ‰ç›¸åŒçš„åˆ—ç»“æ„\n",
        "\n",
        "#### ğŸ’¡ è¿è¡Œæ•ˆæœ\n",
        "```\n",
        "ğŸ” æ£€æŸ¥CSVæ–‡ä»¶åˆ—ç»“æ„ (Testing_set):\n",
        "  æ–‡ä»¶1: test1.csv -> 47åˆ—\n",
        "  æ–‡ä»¶2: test2.csv -> 48åˆ—  â† å‘ç°é—®é¢˜ï¼\n",
        "  æ–‡ä»¶3: test3.csv -> 47åˆ—\n",
        "\n",
        "âŒ å‘ç°åˆ—æ•°ä¸ä¸€è‡´é—®é¢˜ï¼\n",
        "  ğŸ“Š 47åˆ—: æ–‡ä»¶1, æ–‡ä»¶3\n",
        "  ğŸ“Š 48åˆ—: æ–‡ä»¶2\n",
        "\n",
        "ğŸ”§ æ‰€æœ‰æ–‡ä»¶çš„å…±åŒåˆ—æ•°: 47\n",
        "ğŸ“ å„æ–‡ä»¶ç‹¬æœ‰çš„åˆ—:\n",
        "  æ–‡ä»¶1: æ— ç‹¬æœ‰åˆ—\n",
        "  æ–‡ä»¶2: ['é¢å¤–çš„åˆ—å']  â† æ‰¾åˆ°ç½ªé­ç¥¸é¦–ï¼\n",
        "  æ–‡ä»¶3: æ— ç‹¬æœ‰åˆ—\n",
        "\n",
        "âœ… å°†ç»Ÿä¸€ä½¿ç”¨ 47 ä¸ªå…±åŒåˆ—\n",
        "```\n",
        "\n",
        "è¿™æ ·å°±ä»æ ¹æºä¸Šè§£å†³äº†ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´çš„é—®é¢˜ï¼ğŸ¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½éªŒè¯å’Œæµ‹è¯•æ•°æ®\n",
        "print(\"å¼€å§‹åŠ è½½éªŒè¯æ•°æ®...\")\n",
        "val_normal = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥éªŒè¯æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"âš ï¸ è­¦å‘Š: éªŒè¯é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†\")\n",
        "    val_data = pd.DataFrame()\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "\n",
        "print(\"\\nå¼€å§‹åŠ è½½æµ‹è¯•æ•°æ®...\")\n",
        "test_normal = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"âš ï¸ è­¦å‘Š: æµ‹è¯•é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†\")\n",
        "    test_data = pd.DataFrame()\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "\n",
        "print(f\"\\néªŒè¯é›†æ ·æœ¬æ•°: {len(val_data)} (æ­£å¸¸: {len(val_normal)}, PCDN: {len(val_pcdn)})\")\n",
        "print(f\"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)} (æ­£å¸¸: {len(test_normal)}, PCDN: {len(test_pcdn)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ å¢å¼ºç‰ˆæ•°æ®åŠ è½½ï¼šæ”¯æŒè·¨æ•°æ®é›†åˆ—åˆ†æ\n",
        "print(\"ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ•°æ®åŠ è½½...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„åˆ—ä¿¡æ¯\n",
        "datasets_meta = {}\n",
        "\n",
        "# åŠ è½½è®­ç»ƒæ•°æ®\n",
        "print(\"\\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...\")\n",
        "train_normal, train_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)\n",
        "train_pcdn, train_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)\n",
        "\n",
        "# åˆå¹¶è®­ç»ƒæ•°æ®å¹¶è®°å½•å…ƒä¿¡æ¯\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "datasets_meta['Training_set'] = {\n",
        "    'final_columns': list(train_data.columns),\n",
        "    'dataset_path': 'Training_set',\n",
        "    'shape': train_data.shape,\n",
        "    'normal_meta': train_normal_meta,\n",
        "    'pcdn_meta': train_pcdn_meta\n",
        "}\n",
        "\n",
        "print(f\"âœ… è®­ç»ƒæ•°æ®: {len(train_normal)}ä¸ªæ­£å¸¸ + {len(train_pcdn)}ä¸ªPCDN = {len(train_data)}æ€»æ ·æœ¬\")\n",
        "\n",
        "# åŠ è½½éªŒè¯æ•°æ®\n",
        "print(\"\\nğŸ“‚ åŠ è½½éªŒè¯æ•°æ®...\")\n",
        "val_normal, val_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn, val_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥éªŒè¯æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"âš ï¸ éªŒè¯é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†\")\n",
        "    val_data = pd.DataFrame()\n",
        "    datasets_meta['Validation_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "    datasets_meta['Validation_set'] = {\n",
        "        'final_columns': list(val_data.columns),\n",
        "        'dataset_path': 'Validation_set',\n",
        "        'shape': val_data.shape,\n",
        "        'normal_meta': val_normal_meta,\n",
        "        'pcdn_meta': val_pcdn_meta\n",
        "    }\n",
        "    print(f\"âœ… éªŒè¯æ•°æ®: {len(val_normal)}ä¸ªæ­£å¸¸ + {len(val_pcdn)}ä¸ªPCDN = {len(val_data)}æ€»æ ·æœ¬\")\n",
        "\n",
        "# åŠ è½½æµ‹è¯•æ•°æ®\n",
        "print(\"\\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...\")\n",
        "test_normal, test_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn, test_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"âš ï¸ æµ‹è¯•é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†\")\n",
        "    test_data = pd.DataFrame()\n",
        "    datasets_meta['Testing_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "    datasets_meta['Testing_set'] = {\n",
        "        'final_columns': list(test_data.columns),\n",
        "        'dataset_path': 'Testing_set', \n",
        "        'shape': test_data.shape,\n",
        "        'normal_meta': test_normal_meta,\n",
        "        'pcdn_meta': test_pcdn_meta\n",
        "    }\n",
        "    print(f\"âœ… æµ‹è¯•æ•°æ®: {len(test_normal)}ä¸ªæ­£å¸¸ + {len(test_pcdn)}ä¸ªPCDN = {len(test_data)}æ€»æ ·æœ¬\")\n",
        "\n",
        "# ğŸ“Š æ‰§è¡Œè·¨æ•°æ®é›†åˆ—ç»“æ„åˆ†æ\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ” å¼€å§‹è·¨æ•°æ®é›†åˆ—ç»“æ„åˆ†æ...\")\n",
        "analyze_cross_dataset_columns(datasets_meta)\n",
        "\n",
        "# ğŸ“‹ æ•°æ®åŠ è½½æ€»ç»“\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“‹ æ•°æ®åŠ è½½æ€»ç»“\")\n",
        "print(\"=\"*80)\n",
        "for dataset_name, meta in datasets_meta.items():\n",
        "    if 'status' in meta and meta['status'] == 'empty':\n",
        "        print(f\"ğŸ“‚ {dataset_name}: ç©ºæ•°æ®é›†\")\n",
        "    else:\n",
        "        print(f\"ğŸ“‚ {dataset_name}: {meta['shape'][0]}è¡Œ x {meta['shape'][1]}åˆ—\")\n",
        "\n",
        "print(\"ğŸ‰ æ•°æ®åŠ è½½å’Œåˆ†æå®Œæˆï¼\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” å¢å¼ºç‰ˆæ•°æ®åŠ è½½ï¼šå®Œæ•´çš„åˆ—ä¸€è‡´æ€§è¯Šæ–­\n",
        "\n",
        "### ğŸ¯ åŠŸèƒ½å¢å¼º\n",
        "ç°åœ¨çš„æ•°æ®åŠ è½½ç³»ç»Ÿå®ç°äº†**å››çº§åˆ—ä¸€è‡´æ€§æ£€æŸ¥**ï¼š\n",
        "\n",
        "#### ğŸ“Š ç¬¬ä¸€çº§ï¼šå•ä¸ªCSVæ–‡ä»¶å†…æ£€æŸ¥\n",
        "```\n",
        "ğŸ” æ£€æŸ¥CSVæ–‡ä»¶åˆ—ç»“æ„ (APP_0):\n",
        "  æ–‡ä»¶1: file1.csv -> 47åˆ—\n",
        "  æ–‡ä»¶2: file2.csv -> 48åˆ—  â† å‘ç°å¼‚å¸¸ï¼\n",
        "  æ–‡ä»¶3: file3.csv -> 47åˆ—\n",
        "```\n",
        "\n",
        "#### ğŸ“Š ç¬¬äºŒçº§ï¼šæ•°æ®é›†å†…ç»Ÿä¸€\n",
        "```\n",
        "âŒ å‘ç°åˆ—æ•°ä¸ä¸€è‡´é—®é¢˜ï¼\n",
        "  ğŸ“Š 47åˆ—: æ–‡ä»¶1, æ–‡ä»¶3\n",
        "  ğŸ“Š 48åˆ—: æ–‡ä»¶2\n",
        "\n",
        "ğŸ”§ å°†ç»Ÿä¸€ä½¿ç”¨ 47 ä¸ªå…±åŒåˆ—\n",
        "```\n",
        "\n",
        "#### ğŸ“Š ç¬¬ä¸‰çº§ï¼šè·¨æ•°æ®é›†æ¯”è¾ƒ\n",
        "```\n",
        "ğŸ” è·¨æ•°æ®é›†åˆ—ç»“æ„ä¸€è‡´æ€§åˆ†æ\n",
        "ğŸ“Š Training_set: 47ä¸ªåŸå§‹åˆ—\n",
        "ğŸ“Š Validation_set: 47ä¸ªåŸå§‹åˆ—\n",
        "ğŸ“Š Testing_set: 48ä¸ªåŸå§‹åˆ—  â† æ‰¾åˆ°é—®é¢˜æ ¹æºï¼\n",
        "\n",
        "âŒ å‘ç°åˆ—æ•°ä¸ä¸€è‡´: [47, 48]\n",
        "```\n",
        "\n",
        "#### ğŸ“Š ç¬¬å››çº§ï¼šè¯¦ç»†å·®å¼‚åˆ†æ\n",
        "- **å„æ•°æ®é›†ç‹¬æœ‰åˆ—åˆ†æ**ï¼šç²¾ç¡®æ˜¾ç¤ºå“ªäº›åˆ—æ˜¯æ¯ä¸ªæ•°æ®é›†ç‹¬æœ‰çš„\n",
        "- **ä¸¤ä¸¤å¯¹æ¯”**ï¼šè¯¦ç»†æ¯”è¾ƒä»»æ„ä¸¤ä¸ªæ•°æ®é›†çš„åˆ—å·®å¼‚\n",
        "- **ç»Ÿä¸€ç­–ç•¥å»ºè®®**ï¼šè‡ªåŠ¨æ¨èæœ€ä½³çš„åˆ—å¯¹é½æ–¹æ¡ˆ\n",
        "\n",
        "### ğŸ” è¯Šæ–­è¾“å‡ºç¤ºä¾‹\n",
        "```\n",
        "ğŸ“ å„æ•°æ®é›†ç‹¬æœ‰åˆ—åˆ†æ:\n",
        "  ğŸ” Training_set:\n",
        "    ç‹¬æœ‰åˆ—æ•°: 0\n",
        "  ğŸ” Testing_set:\n",
        "    ç‹¬æœ‰åˆ—æ•°: 1\n",
        "    ç‹¬æœ‰åˆ—å: ['mysterious_extra_column']\n",
        "\n",
        "ğŸ”„ ä¸¤ä¸¤æ•°æ®é›†å¯¹æ¯”:\n",
        "  ğŸ“Š Training_set vs Testing_set:\n",
        "    ä»…Testing_setæœ‰: ['mysterious_extra_column']\n",
        "\n",
        "ğŸ’¡ ç»Ÿä¸€ç­–ç•¥å»ºè®®:\n",
        "ğŸ”§ å»ºè®®ç»Ÿä¸€åˆ°å…±åŒåˆ—é›†åˆ (47åˆ—)\n",
        "   âš ï¸ å°†ä¸¢å¤±çš„åˆ—: ['mysterious_extra_column']\n",
        "```\n",
        "\n",
        "### âœ… é—®é¢˜è§£å†³ä¿éšœ\n",
        "- âœ… **ç²¾ç¡®å®šä½**ï¼šå‡†ç¡®æ‰¾åˆ°å“ªä¸ªCSVæ–‡ä»¶ã€å“ªä¸ªæ•°æ®é›†æœ‰é¢å¤–åˆ—\n",
        "- âœ… **è‡ªåŠ¨ä¿®å¤**ï¼šè‡ªåŠ¨å¯¹é½åˆ°å…±åŒåˆ—é›†åˆ\n",
        "- âœ… **é›¶é—æ¼**ï¼šç¡®ä¿ä¸ä¼šå†å‡ºç°ç»´åº¦ä¸ä¸€è‡´é”™è¯¯\n",
        "- âœ… **å®Œæ•´è®°å½•**ï¼šè¯¦ç»†è®°å½•æ¯ä¸ªå¤„ç†æ­¥éª¤\n",
        "\n",
        "ç°åœ¨æ‚¨å¯ä»¥å‡†ç¡®çŸ¥é“\"ä¸ºä»€ä¹ˆæµ‹è¯•é›†æ¯”è®­ç»ƒé›†å¤š1ä¸ªç‰¹å¾\"çš„å…·ä½“åŸå› äº†ï¼ğŸ¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®æ¢ç´¢å’ŒåŸºæœ¬ä¿¡æ¯\n",
        "print(\"=== è®­ç»ƒæ•°æ®åŸºæœ¬ä¿¡æ¯ ===\")\n",
        "print(f\"æ•°æ®å½¢çŠ¶: {train_data.shape}\")\n",
        "print(f\"\\nåˆ—å ({len(train_data.columns)}ä¸ªç‰¹å¾):\")\n",
        "print(train_data.columns.tolist())\n",
        "\n",
        "print(\"\\n=== æ ‡ç­¾åˆ†å¸ƒ ===\")\n",
        "label_counts = train_data['label'].value_counts()\n",
        "print(label_counts)\n",
        "print(f\"æ­£å¸¸æµé‡æ¯”ä¾‹: {label_counts[0]/len(train_data)*100:.2f}%\")\n",
        "print(f\"PCDNæµé‡æ¯”ä¾‹: {label_counts[1]/len(train_data)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== æ•°æ®ç±»å‹ä¿¡æ¯ ===\")\n",
        "print(train_data.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®é¢„å¤„ç†\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # åˆ é™¤æŒ‡å®šçš„ä¸éœ€è¦çš„åˆ—\n",
        "    columns_to_drop = [\n",
        "        'source_file',  # æºæ–‡ä»¶ä¿¡æ¯ï¼ˆæ·»åŠ çš„è¾…åŠ©åˆ—ï¼‰\n",
        "        'frame.number', # å¸§ç¼–å·\n",
        "        'frame.time_relative', # ç›¸å¯¹æ—¶é—´\n",
        "        'ip.version',   # IPç‰ˆæœ¬\n",
        "        'ip.ttl',       # IP TTL\n",
        "        'ip.src', 'ip.dst',  # IPåœ°å€\n",
        "        'ipv6.plen',    # IPv6 payloadé•¿åº¦\n",
        "        'ipv6.nxt',     # IPv6 ä¸‹ä¸€ä¸ªå¤´\n",
        "        'ipv6.src', 'ipv6.dst',  # IPv6åœ°å€\n",
        "        '_ws.col.Protocol', # Wiresharkåè®®åˆ—\n",
        "        'ssl.handshake.extensions_server_name',  # SSLæ‰©å±•ä¿¡æ¯\n",
        "        'eth.src',      # MACåœ°å€\n",
        "        'pcap_duration', # PCAPæŒç»­æ—¶é—´\n",
        "        'app',          # åº”ç”¨ç¨‹åº\n",
        "        'os',           # æ“ä½œç³»ç»Ÿ\n",
        "        'date',         # æ—¥æœŸ\n",
        "        'flow_id',      # æµID\n",
        "        'dpi_file_name', # DPIæ–‡ä»¶å\n",
        "        'dpi_five_tuple', # äº”å…ƒç»„\n",
        "        'dpi_rule_result', # DPIè§„åˆ™ç»“æœ\n",
        "        'dpi_label',    # DPIæ ‡ç­¾\n",
        "        'ulProtoID',    # ä¸Šå±‚åè®®ID\n",
        "        'dpi_rule_pkt', # DPIè§„åˆ™åŒ…\n",
        "        'dpi_packets',  # DPIåŒ…æ•°\n",
        "        'dpi_bytes',    # DPIå­—èŠ‚æ•°\n",
        "        'label_source', # æ ‡ç­¾æº\n",
        "        'id',           # IDå­—æ®µ\n",
        "        'category'      # categoryå­—æ®µ\n",
        "    ]\n",
        "    \n",
        "    # åˆ é™¤å­˜åœ¨çš„åˆ—\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
        "    \n",
        "    # ä¿ç•™æ‰€æœ‰å…¶ä»–ç‰¹å¾ï¼ˆåŒ…æ‹¬æ•°ç»„ç‰¹å¾ï¼‰ï¼Œè¿™äº›å¯èƒ½å¯¹åˆ†ç±»æœ‰ç”¨\n",
        "    # æ•°ç»„ç‰¹å¾å¦‚ ip_direction, pkt_len, iat ç­‰å°†åœ¨åç»­æ­¥éª¤ä¸­è¿›è¡Œç¼–ç å¤„ç†\n",
        "    \n",
        "    # å¤„ç†ç¼ºå¤±å€¼\n",
        "    df_processed = df_processed.fillna(0)\n",
        "    \n",
        "    # å¤„ç†æ— ç©·å¤§å€¼\n",
        "    df_processed = df_processed.replace([np.inf, -np.inf], 0)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# é¢„å¤„ç†è®­ç»ƒæ•°æ®\n",
        "train_processed = preprocess_data(train_data)\n",
        "\n",
        "# æ™ºèƒ½å¤„ç†ç©ºæ•°æ®é›†çš„æƒ…å†µ\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "val_exists = len(val_data) > 0\n",
        "test_exists = len(test_data) > 0\n",
        "\n",
        "print(f\"éªŒè¯é›†å­˜åœ¨: {'æ˜¯' if val_exists else 'å¦'}\")\n",
        "print(f\"æµ‹è¯•é›†å­˜åœ¨: {'æ˜¯' if test_exists else 'å¦'}\")\n",
        "\n",
        "if not val_exists and not test_exists:\n",
        "    # ä¸¤ä¸ªæ•°æ®é›†éƒ½ä¸ºç©ºï¼Œè¿›è¡Œ60/20/20åˆ†å‰²\n",
        "    print(\"éªŒè¯é›†å’Œæµ‹è¯•é›†éƒ½ä¸ºç©ºï¼Œä»è®­ç»ƒæ•°æ®åˆ†å‰²ä¸º 60% è®­ç»ƒ / 20% éªŒè¯ / 20% æµ‹è¯•\")\n",
        "    train_temp, temp_split = train_test_split(\n",
        "        train_processed, test_size=0.4, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    val_processed, test_processed = train_test_split(\n",
        "        temp_split, test_size=0.5, random_state=42, \n",
        "        stratify=temp_split['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    \n",
        "elif not val_exists:\n",
        "    # åªæœ‰éªŒè¯é›†ä¸ºç©ºï¼Œåˆ†å‰²80/20\n",
        "    print(\"éªŒè¯é›†ä¸ºç©ºï¼Œä»è®­ç»ƒæ•°æ®åˆ†å‰²ä¸º 80% è®­ç»ƒ / 20% éªŒè¯\")\n",
        "    train_temp, val_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    test_processed = preprocess_data(test_data)\n",
        "    \n",
        "elif not test_exists:\n",
        "    # åªæœ‰æµ‹è¯•é›†ä¸ºç©ºï¼Œåˆ†å‰²80/20\n",
        "    print(\"æµ‹è¯•é›†ä¸ºç©ºï¼Œä»è®­ç»ƒæ•°æ®åˆ†å‰²ä¸º 80% è®­ç»ƒ / 20% æµ‹è¯•\")\n",
        "    train_temp, test_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    \n",
        "else:\n",
        "    # ä¸¤ä¸ªæ•°æ®é›†éƒ½å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨\n",
        "    print(\"ä½¿ç”¨åŸå§‹çš„éªŒè¯é›†å’Œæµ‹è¯•é›†\")\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    test_processed = preprocess_data(test_data)\n",
        "\n",
        "print(f\"é¢„å¤„ç†åè®­ç»ƒæ•°æ®å½¢çŠ¶: {train_processed.shape}\")\n",
        "print(f\"é¢„å¤„ç†åéªŒè¯æ•°æ®å½¢çŠ¶: {val_processed.shape}\")\n",
        "print(f\"é¢„å¤„ç†åæµ‹è¯•æ•°æ®å½¢çŠ¶: {test_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†\n",
        "# æå–ç‰¹å¾å’Œæ ‡ç­¾\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"é¢„å¤„ç†åç‰¹å¾æ•°é‡: {len(feature_columns)}\")\n",
        "print(f\"ç‰¹å¾åˆ—è¡¨å‰10ä¸ª: {feature_columns[:10]}\")\n",
        "\n",
        "# å¤„ç†åºåˆ—ç‰¹å¾ - è¿™äº›å­—æ®µåŒ…å«ç½‘ç»œåŒ…çš„æ—¶åºä¿¡æ¯\n",
        "def process_sequence_features(df, enable_sequence_processing=True):\n",
        "    \"\"\"\n",
        "    å¤„ç†åºåˆ—ç±»å‹çš„ç‰¹å¾ (pkt_len, ip_direction, iat)\n",
        "    \n",
        "    å‚æ•°:\n",
        "        df: è¾“å…¥æ•°æ®æ¡†\n",
        "        enable_sequence_processing: æ˜¯å¦å¯ç”¨åºåˆ—ç‰¹å¾å¤„ç†\n",
        "            - True: æå–ç»Ÿè®¡ç‰¹å¾(å‡å€¼ã€æ–¹å·®ã€åˆ†ä½æ•°ç­‰)\n",
        "            - False: ç›´æ¥åˆ é™¤åºåˆ—ç‰¹å¾\n",
        "    \n",
        "    å¯¹äºXGBoostè¿™ç§åŸºäºæ ‘çš„ç®—æ³•ï¼Œéœ€è¦å°†åºåˆ—æ•°æ®è½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„ç»Ÿè®¡ç‰¹å¾ï¼š\n",
        "    1. pkt_len: åŒ…é•¿åº¦åºåˆ— - åæ˜ æµé‡çš„æ•°æ®ä¼ è¾“æ¨¡å¼\n",
        "    2. ip_direction: IPæ–¹å‘åºåˆ— - åæ˜ é€šä¿¡çš„æ–¹å‘æ¨¡å¼  \n",
        "    3. iat: åŒ…é—´åˆ°è¾¾æ—¶é—´é—´éš”åºåˆ— - åæ˜ æµé‡çš„æ—¶é—´ç‰¹å¾\n",
        "    \n",
        "    XGBoostæ— æ³•ç›´æ¥å¤„ç†å˜é•¿åºåˆ—ï¼Œéœ€è¦æå–å›ºå®šç»´åº¦çš„ç‰¹å¾\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    # å®šä¹‰åºåˆ—ç‰¹å¾åŠå…¶å«ä¹‰\n",
        "    sequence_columns = {\n",
        "        'ip_direction': 'ç½‘ç»œåŒ…æ–¹å‘åºåˆ— (0=å‡ºç«™, 1=å…¥ç«™)',\n",
        "        'pkt_len': 'ç½‘ç»œåŒ…é•¿åº¦åºåˆ—',\n",
        "        'iat': 'åŒ…é—´åˆ°è¾¾æ—¶é—´é—´éš”åºåˆ—'\n",
        "    }\n",
        "    \n",
        "    if not enable_sequence_processing:\n",
        "        # å¦‚æœç¦ç”¨åºåˆ—ç‰¹å¾å¤„ç†ï¼Œç›´æ¥åˆ é™¤è¿™äº›åˆ—\n",
        "        print(\"ğŸ—‘ï¸ åˆ é™¤åºåˆ—ç‰¹å¾æ¨¡å¼:\")\n",
        "        for col in sequence_columns.keys():\n",
        "            if col in df_copy.columns:\n",
        "                df_copy = df_copy.drop(columns=[col])\n",
        "                print(f\"  âœ— å·²åˆ é™¤: {col}\")\n",
        "        return df_copy\n",
        "    \n",
        "    for col, description in sequence_columns.items():\n",
        "        if col in df_copy.columns:\n",
        "            print(f\"å¤„ç†åºåˆ—ç‰¹å¾: {col} - {description}\")\n",
        "            \n",
        "            try:\n",
        "                # å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å€¼åˆ—è¡¨ï¼ˆé¿å…ä½¿ç”¨evalï¼‰\n",
        "                def safe_parse_array(x):\n",
        "                    \"\"\"å®‰å…¨è§£ææ•°ç»„å­—ç¬¦ä¸²\"\"\"\n",
        "                    if pd.isna(x) or x == '' or x == '[]':\n",
        "                        return []\n",
        "                    if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "                        try:\n",
        "                            # ä½¿ç”¨ast.literal_evalæ›¿ä»£evalï¼Œæ›´å®‰å…¨\n",
        "                            return ast.literal_eval(x)\n",
        "                        except (ValueError, SyntaxError):\n",
        "                            return []\n",
        "                    return []\n",
        "                \n",
        "                sequences = df_copy[col].apply(safe_parse_array)\n",
        "                \n",
        "                # === åŸºç¡€ç»Ÿè®¡ç‰¹å¾ ===\n",
        "                df_copy[f'{col}_mean'] = sequences.apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_std'] = sequences.apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_min'] = sequences.apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_max'] = sequences.apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_median'] = sequences.apply(lambda x: np.median(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_range'] = sequences.apply(lambda x: (np.max(x) - np.min(x)) if len(x) > 0 else 0)\n",
        "                \n",
        "                # === åˆ†ä½æ•°ç‰¹å¾ ===\n",
        "                df_copy[f'{col}_q25'] = sequences.apply(lambda x: np.percentile(x, 25) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_q75'] = sequences.apply(lambda x: np.percentile(x, 75) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_iqr'] = df_copy[f'{col}_q75'] - df_copy[f'{col}_q25']\n",
        "                \n",
        "                # === åºåˆ—é•¿åº¦ç‰¹å¾ ===\n",
        "                df_copy[f'{col}_len'] = sequences.apply(lambda x: len(x))\n",
        "                \n",
        "                # === åºåˆ—æ¨¡å¼ç‰¹å¾ ===\n",
        "                # å˜å¼‚ç³»æ•° (æ ‡å‡†å·®/å‡å€¼) - è¡¡é‡åºåˆ—çš„ç›¸å¯¹å˜åŒ–ç¨‹åº¦\n",
        "                df_copy[f'{col}_cv'] = sequences.apply(lambda x: np.std(x)/np.mean(x) if len(x) > 0 and np.mean(x) != 0 else 0)\n",
        "                \n",
        "                # ååº¦å’Œå³°åº¦ - è¡¡é‡åºåˆ—åˆ†å¸ƒå½¢çŠ¶\n",
        "                df_copy[f'{col}_skew'] = sequences.apply(lambda x: stats.skew(x) if len(x) > 1 else 0)\n",
        "                df_copy[f'{col}_kurtosis'] = sequences.apply(lambda x: stats.kurtosis(x) if len(x) > 1 else 0)\n",
        "                \n",
        "                # === åºåˆ—ç‰¹æœ‰çš„ç‰¹å¾ ===\n",
        "                if col == 'ip_direction':\n",
        "                    # å¯¹äºæ–¹å‘åºåˆ—ï¼šç»Ÿè®¡å‡ºç«™/å…¥ç«™æ¯”ä¾‹\n",
        "                    df_copy[f'{col}_out_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 0])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_in_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 1])/len(x) if len(x) > 0 else 0)\n",
        "                    # æ–¹å‘å˜åŒ–æ¬¡æ•° - åæ˜ é€šä¿¡æ¨¡å¼\n",
        "                    df_copy[f'{col}_changes'] = sequences.apply(lambda x: sum([1 for i in range(1, len(x)) if x[i] != x[i-1]]) if len(x) > 1 else 0)\n",
        "                \n",
        "                elif col == 'pkt_len':\n",
        "                    # å¯¹äºåŒ…é•¿åº¦åºåˆ—ï¼šå°åŒ…/å¤§åŒ…æ¯”ä¾‹\n",
        "                    df_copy[f'{col}_small_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i <= 64])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_large_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i >= 1400])/len(x) if len(x) > 0 else 0)\n",
        "                \n",
        "                elif col == 'iat':\n",
        "                    # å¯¹äºæ—¶é—´é—´éš”åºåˆ—ï¼šçªå‘æ€§æ£€æµ‹\n",
        "                    df_copy[f'{col}_burst_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i < 0.01])/len(x) if len(x) > 0 else 0)  # å°äº10msçš„æ¯”ä¾‹\n",
        "                    df_copy[f'{col}_long_gap_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i > 1.0])/len(x) if len(x) > 0 else 0)  # å¤§äº1sçš„æ¯”ä¾‹\n",
        "                \n",
        "                # === è¶‹åŠ¿ç‰¹å¾ ===\n",
        "                # åºåˆ—é€’å¢/é€’å‡è¶‹åŠ¿\n",
        "                def trend_analysis(seq):\n",
        "                    if len(seq) < 2:\n",
        "                        return 0, 0\n",
        "                    increasing = sum([1 for i in range(1, len(seq)) if seq[i] > seq[i-1]])\n",
        "                    decreasing = sum([1 for i in range(1, len(seq)) if seq[i] < seq[i-1]])\n",
        "                    return increasing/len(seq), decreasing/len(seq)\n",
        "                \n",
        "                trends = sequences.apply(trend_analysis)\n",
        "                df_copy[f'{col}_increasing_ratio'] = trends.apply(lambda x: x[0])\n",
        "                df_copy[f'{col}_decreasing_ratio'] = trends.apply(lambda x: x[1])\n",
        "                \n",
        "                # åˆ é™¤åŸå§‹åºåˆ—åˆ—\n",
        "                df_copy = df_copy.drop(columns=[col])\n",
        "                print(f\"  -> å·²ä» {col} æå– {len([c for c in df_copy.columns if c.startswith(col)])} ä¸ªç‰¹å¾\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  -> å¤„ç† {col} æ—¶å‡ºé”™ï¼Œå°†ç›´æ¥ç¼–ç : {e}\")\n",
        "                # å¦‚æœå¤„ç†å¤±è´¥ï¼Œå°±ç®€å•ç¼–ç \n",
        "                df_copy[col] = LabelEncoder().fit_transform(df_copy[col].astype(str))\n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "# å¤„ç†æ‰€æœ‰æ•°æ®é›†çš„åºåˆ—ç‰¹å¾\n",
        "print(\"å¼€å§‹å¤„ç†åºåˆ—ç‰¹å¾...\")\n",
        "print(\"=\" * 60)\n",
        "train_processed = process_sequence_features(train_processed, enable_sequence_processing=ENABLE_SEQUENCE_FEATURES)\n",
        "val_processed = process_sequence_features(val_processed, enable_sequence_processing=ENABLE_SEQUENCE_FEATURES)\n",
        "test_processed = process_sequence_features(test_processed, enable_sequence_processing=ENABLE_SEQUENCE_FEATURES)\n",
        "\n",
        "# æ›´æ–°ç‰¹å¾åˆ—è¡¨\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"\\nå¤„ç†åºåˆ—ç‰¹å¾åçš„ç‰¹å¾æ•°é‡: {len(feature_columns)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# æ£€æŸ¥å‰©ä½™çš„éæ•°å€¼åˆ—\n",
        "non_numeric_cols = []\n",
        "for col in feature_columns:\n",
        "    if not pd.api.types.is_numeric_dtype(train_processed[col]):\n",
        "        non_numeric_cols.append(col)\n",
        "\n",
        "print(f\"\\néœ€è¦ç¼–ç çš„éæ•°å€¼åˆ—æ•°é‡: {len(non_numeric_cols)}\")\n",
        "if non_numeric_cols:\n",
        "    print(f\"éæ•°å€¼åˆ—: {non_numeric_cols[:5]}...\")  # æ˜¾ç¤ºå‰5ä¸ª\n",
        "\n",
        "# å¯¹éæ•°å€¼åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç \n",
        "if non_numeric_cols:\n",
        "    print(\"å¼€å§‹å¯¹éæ•°å€¼åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç ...\")\n",
        "    for col in non_numeric_cols:\n",
        "        try:\n",
        "            le = LabelEncoder()\n",
        "            # åˆå¹¶æ‰€æœ‰æ•°æ®é›†çš„è¯¥åˆ—å€¼è¿›è¡Œç¼–ç \n",
        "            all_values = pd.concat([\n",
        "                train_processed[col].fillna('missing').astype(str),\n",
        "                val_processed[col].fillna('missing').astype(str),\n",
        "                test_processed[col].fillna('missing').astype(str)\n",
        "            ])\n",
        "            le.fit(all_values)\n",
        "            \n",
        "            train_processed[col] = le.transform(train_processed[col].fillna('missing').astype(str))\n",
        "            val_processed[col] = le.transform(val_processed[col].fillna('missing').astype(str))\n",
        "            test_processed[col] = le.transform(test_processed[col].fillna('missing').astype(str))\n",
        "            print(f\"  âœ“ å·²ç¼–ç : {col}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— ç¼–ç å¤±è´¥ {col}: {e}\")\n",
        "            # ç¼–ç å¤±è´¥çš„åˆ—ç›´æ¥åˆ é™¤\n",
        "            if col in train_processed.columns:\n",
        "                train_processed = train_processed.drop(columns=[col])\n",
        "                val_processed = val_processed.drop(columns=[col])\n",
        "                test_processed = test_processed.drop(columns=[col])\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰æ•°æ®é›†å…·æœ‰ç›¸åŒçš„ç‰¹å¾åˆ—\n",
        "print(\"\\nğŸ”§ æ£€æŸ¥æ•°æ®é›†ç‰¹å¾ä¸€è‡´æ€§...\")\n",
        "train_features = set(train_processed.columns) - {'label'}\n",
        "val_features = set(val_processed.columns) - {'label'}\n",
        "test_features = set(test_processed.columns) - {'label'}\n",
        "\n",
        "print(f\"è®­ç»ƒé›†ç‰¹å¾æ•°: {len(train_features)}\")\n",
        "print(f\"éªŒè¯é›†ç‰¹å¾æ•°: {len(val_features)}\")\n",
        "print(f\"æµ‹è¯•é›†ç‰¹å¾æ•°: {len(test_features)}\")\n",
        "\n",
        "# å–ä¸‰ä¸ªæ•°æ®é›†çš„ç‰¹å¾äº¤é›†ï¼Œç¡®ä¿ä¸€è‡´æ€§\n",
        "common_features = train_features.intersection(val_features).intersection(test_features)\n",
        "print(f\"å…±åŒç‰¹å¾æ•°: {len(common_features)}\")\n",
        "\n",
        "if len(common_features) < len(train_features):\n",
        "    print(\"âš ï¸ è­¦å‘Š: æ•°æ®é›†é—´ç‰¹å¾ä¸ä¸€è‡´ï¼Œä½¿ç”¨å…±åŒç‰¹å¾\")\n",
        "    # åªä¿ç•™å…±åŒç‰¹å¾\n",
        "    feature_cols_to_keep = list(common_features) + ['label']\n",
        "    train_processed = train_processed[feature_cols_to_keep]\n",
        "    val_processed = val_processed[feature_cols_to_keep]\n",
        "    test_processed = test_processed[feature_cols_to_keep]\n",
        "\n",
        "print(\"\\næ•°æ®é¢„å¤„ç†å®Œæˆï¼\")\n",
        "print(f\"æœ€ç»ˆç‰¹å¾æ•°é‡: {len(common_features)}\")\n",
        "print(\"âœ… æ‰€æœ‰æ•°æ®é›†ç‰¹å¾å·²å¯¹é½\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” åºåˆ—ç‰¹å¾å¤„ç†ç­–ç•¥è¯´æ˜\n",
        "\n",
        "### ä¸ºä»€ä¹ˆXGBoostéœ€è¦ç‰¹æ®Šå¤„ç†åºåˆ—ç‰¹å¾ï¼Ÿ\n",
        "\n",
        "**XGBoostçš„é™åˆ¶ï¼š**\n",
        "- XGBoostæ˜¯åŸºäºæ ‘çš„ç®—æ³•ï¼Œåªèƒ½å¤„ç†å›ºå®šç»´åº¦çš„è¡¨æ ¼æ•°æ®\n",
        "- æ— æ³•ç›´æ¥å¤„ç†å˜é•¿åºåˆ—ï¼ˆå¦‚[1,0,1,1,0]è¿™æ ·çš„æ•°ç»„ï¼‰\n",
        "- éœ€è¦å°†åºåˆ—è½¬æ¢ä¸ºå›ºå®šæ•°é‡çš„æ•°å€¼ç‰¹å¾\n",
        "\n",
        "### ğŸ“Š æˆ‘ä»¬æå–çš„åºåˆ—ç‰¹å¾ç±»å‹\n",
        "\n",
        "**1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾**\n",
        "- å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§/æœ€å°å€¼ã€ä¸­ä½æ•°ã€å››åˆ†ä½æ•°\n",
        "- è¿™äº›ç‰¹å¾æ•è·åºåˆ—çš„æ•´ä½“åˆ†å¸ƒç‰¹æ€§\n",
        "\n",
        "**2. å½¢çŠ¶ç‰¹å¾**\n",
        "- ååº¦(skewness)ï¼šåºåˆ—åˆ†å¸ƒçš„å¯¹ç§°æ€§\n",
        "- å³°åº¦(kurtosis)ï¼šåºåˆ—åˆ†å¸ƒçš„å°–é”ç¨‹åº¦\n",
        "- å˜å¼‚ç³»æ•°ï¼šç›¸å¯¹å˜åŒ–ç¨‹åº¦\n",
        "\n",
        "**3. åºåˆ—æ¨¡å¼ç‰¹å¾**\n",
        "- **æ–¹å‘åºåˆ—(ip_direction)**ï¼šå‡ºç«™/å…¥ç«™æ¯”ä¾‹ã€æ–¹å‘å˜åŒ–æ¬¡æ•°\n",
        "- **åŒ…é•¿åº¦åºåˆ—(pkt_len)**ï¼šå°åŒ…/å¤§åŒ…æ¯”ä¾‹\n",
        "- **æ—¶é—´é—´éš”åºåˆ—(iat)**ï¼šçªå‘ä¼ è¾“/é•¿é—´éš”æ¯”ä¾‹\n",
        "\n",
        "**4. è¶‹åŠ¿ç‰¹å¾**\n",
        "- é€’å¢/é€’å‡è¶‹åŠ¿ï¼šåæ˜ åºåˆ—çš„æ—¶é—´æ¼”åŒ–æ¨¡å¼\n",
        "\n",
        "### ğŸ¯ è¿™äº›ç‰¹å¾å¯¹PCDNæ£€æµ‹çš„æ„ä¹‰\n",
        "\n",
        "**æ­£å¸¸æµé‡ vs PCDNæµé‡çš„åŒºåˆ«ï¼š**\n",
        "- **åŒ…å¤§å°æ¨¡å¼**ï¼šPCDNå¯èƒ½æœ‰ç‰¹å®šçš„åˆ†å—ä¼ è¾“æ¨¡å¼\n",
        "- **æ–¹å‘æ¨¡å¼**ï¼šPCDNçš„ä¸Šä¼ /ä¸‹è½½æ¯”ä¾‹å¯èƒ½ä¸åŒ\n",
        "- **æ—¶é—´æ¨¡å¼**ï¼šPCDNçš„ä¼ è¾“èŠ‚å¥å¯èƒ½æ›´è§„å¾‹æˆ–æ›´çªå‘\n",
        "- **åºåˆ—é•¿åº¦**ï¼šPCDNä¼šè¯å¯èƒ½æœ‰ç‰¹å®šçš„åŒ…æ•°é‡æ¨¡å¼\n",
        "\n",
        "### âš™ï¸ åºåˆ—ç‰¹å¾æ§åˆ¶å‚æ•°\n",
        "\n",
        "é€šè¿‡ä¿®æ”¹ `ENABLE_SEQUENCE_FEATURES` å‚æ•°å¯ä»¥æ§åˆ¶åºåˆ—ç‰¹å¾çš„å¤„ç†æ–¹å¼ï¼š\n",
        "\n",
        "- **`ENABLE_SEQUENCE_FEATURES = True`** (é»˜è®¤)\n",
        "  - å¯¹ `ip_direction`, `pkt_len`, `iat` è¿›è¡Œå¤æ‚çš„ç»Ÿè®¡ç‰¹å¾æå–\n",
        "  - ç”Ÿæˆ45+ä¸ªæ–°ç‰¹å¾ï¼Œå……åˆ†åˆ©ç”¨æ—¶åºä¿¡æ¯\n",
        "  - é€‚åˆå¯¹æ¨¡å‹æ€§èƒ½è¦æ±‚è¾ƒé«˜çš„åœºæ™¯\n",
        "\n",
        "- **`ENABLE_SEQUENCE_FEATURES = False`**\n",
        "  - ç›´æ¥åˆ é™¤è¿™3ä¸ªåºåˆ—ç‰¹å¾\n",
        "  - æ¨¡å‹ä»…ä½¿ç”¨å…¶ä»–ç½‘ç»œç‰¹å¾è¿›è¡Œè®­ç»ƒ\n",
        "  - é€‚åˆè®¡ç®—èµ„æºæœ‰é™æˆ–å¸Œæœ›ç®€åŒ–æ¨¡å‹çš„åœºæ™¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åºåˆ—ç‰¹å¾å¤„ç†æ•ˆæœå±•ç¤º\n",
        "print(\"ğŸ” åºåˆ—ç‰¹å¾å¤„ç†æ•ˆæœåˆ†æ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    # ç»Ÿè®¡æ¯ä¸ªåºåˆ—ç‰¹å¾ç”Ÿæˆäº†å¤šå°‘ä¸ªæ–°ç‰¹å¾\n",
        "    sequence_feature_counts = {}\n",
        "    for original_col in ['ip_direction', 'pkt_len', 'iat']:\n",
        "        derived_features = [col for col in train_processed.columns if col.startswith(original_col)]\n",
        "        sequence_feature_counts[original_col] = len(derived_features)\n",
        "        print(f\"{original_col:15} -> ç”Ÿæˆäº† {len(derived_features):2d} ä¸ªç‰¹å¾\")\n",
        "        if derived_features:\n",
        "            print(f\"                   åŒ…æ‹¬: {', '.join(derived_features[:5])}{'...' if len(derived_features) > 5 else ''}\")\n",
        "\n",
        "    total_sequence_features = sum(sequence_feature_counts.values())\n",
        "    print(f\"\\nğŸ“ˆ æ€»å…±ä»3ä¸ªåºåˆ—ç‰¹å¾ç”Ÿæˆäº† {total_sequence_features} ä¸ªæ•°å€¼ç‰¹å¾\")\n",
        "\n",
        "    # å±•ç¤ºä¸€äº›å…³é”®ç‰¹å¾çš„å«ä¹‰\n",
        "    print(f\"\\nğŸ“‹ å…³é”®ç‰¹å¾å«ä¹‰ç¤ºä¾‹:\")\n",
        "    feature_meanings = {\n",
        "        'pkt_len_mean': 'å¹³å‡åŒ…å¤§å° - åæ˜ ä¼ è¾“æ•°æ®çš„ç²’åº¦',\n",
        "        'pkt_len_cv': 'åŒ…å¤§å°å˜å¼‚ç³»æ•° - åæ˜ ä¼ è¾“çš„è§„å¾‹æ€§',\n",
        "        'ip_direction_changes': 'æ–¹å‘å˜åŒ–æ¬¡æ•° - åæ˜ äº¤äº’æ¨¡å¼',\n",
        "        'iat_burst_ratio': 'çªå‘ä¼ è¾“æ¯”ä¾‹ - åæ˜ æ—¶é—´æ¨¡å¼',\n",
        "        'pkt_len_small_pkt_ratio': 'å°åŒ…æ¯”ä¾‹ - åæ˜ åè®®ç‰¹å¾'\n",
        "    }\n",
        "\n",
        "    for feat, meaning in feature_meanings.items():\n",
        "        if feat in train_processed.columns:\n",
        "            print(f\"  {feat:25}: {meaning}\")\n",
        "\n",
        "    print(\"\\nâœ¨ è¿™äº›ç‰¹å¾å°†å¸®åŠ©XGBoostå­¦ä¹ æ­£å¸¸æµé‡å’ŒPCDNæµé‡çš„è¡Œä¸ºå·®å¼‚æ¨¡å¼\")\n",
        "else:\n",
        "    print(\"ğŸ—‘ï¸ åºåˆ—ç‰¹å¾å·²è¢«åˆ é™¤\")\n",
        "    print(\"- ip_direction, pkt_len, iat ä¸‰ä¸ªç‰¹å¾å·²ä»æ•°æ®é›†ä¸­ç§»é™¤\")\n",
        "    print(\"- æ¨¡å‹å°†ä»…ä½¿ç”¨å…¶ä»–ç½‘ç»œæµé‡ç‰¹å¾è¿›è¡Œè®­ç»ƒ\")\n",
        "    print(\"- è¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹å¯¹æµé‡æ—¶åºæ¨¡å¼çš„å­¦ä¹ èƒ½åŠ›\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®åˆ†å¸ƒå¯è§†åŒ–\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# æ ‡ç­¾åˆ†å¸ƒ\n",
        "label_counts = train_processed['label'].value_counts()\n",
        "axes[0, 0].pie(label_counts.values, labels=['Normal Traffic', 'PCDN Traffic'], autopct='%1.1f%%')\n",
        "axes[0, 0].set_title('Training Data Label Distribution')\n",
        "\n",
        "# é€‰æ‹©å‡ ä¸ªé‡è¦çš„æ•°å€¼ç‰¹å¾è¿›è¡Œå¯è§†åŒ–\n",
        "numeric_features = ['ip.len', 'tcp.srcport', 'tcp.dstport', 'sum_pkt_len', 'total_pkts']\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    # å¦‚æœå¯ç”¨åºåˆ—ç‰¹å¾ï¼Œæ·»åŠ åºåˆ—ç‰¹å¾è¿›è¡Œå¯è§†åŒ–\n",
        "    sequence_features = ['pkt_len_mean', 'ip_direction_changes', 'iat_mean']\n",
        "    all_viz_features = numeric_features + sequence_features\n",
        "    print(\"åŒ…å«åºåˆ—ç‰¹å¾çš„å¯è§†åŒ–\")\n",
        "else:\n",
        "    # å¦‚æœç¦ç”¨åºåˆ—ç‰¹å¾ï¼Œåªä½¿ç”¨åŸºç¡€ç‰¹å¾\n",
        "    all_viz_features = numeric_features\n",
        "    print(\"ä»…ä½¿ç”¨åŸºç¡€ç‰¹å¾çš„å¯è§†åŒ–\")\n",
        "\n",
        "# æ£€æŸ¥å¯ç”¨æ€§\n",
        "available_features = [f for f in all_viz_features if f in train_processed.columns]\n",
        "print(f\"å¯ç”¨äºå¯è§†åŒ–çš„ç‰¹å¾: {available_features}\")\n",
        "\n",
        "if len(available_features) >= 3:\n",
        "    # ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”\n",
        "    for i, feature in enumerate(available_features[:3]):\n",
        "        if i == 0:\n",
        "            ax = axes[0, 1]\n",
        "        elif i == 1:\n",
        "            ax = axes[1, 0]\n",
        "        else:\n",
        "            ax = axes[1, 1]\n",
        "        \n",
        "        normal_data = train_processed[train_processed['label'] == 0][feature]\n",
        "        pcdn_data = train_processed[train_processed['label'] == 1][feature]\n",
        "        \n",
        "        ax.hist(normal_data, alpha=0.7, label='Normal', bins=30, density=True)\n",
        "        ax.hist(pcdn_data, alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "        ax.set_title(f'Distribution of {feature}')\n",
        "        ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"æ•°æ®åˆ†å¸ƒå¯è§†åŒ–å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ åºåˆ—ç‰¹å¾å¤„ç†æ€»ç»“\n",
        "\n",
        "### ğŸ”„ å®Œæ•´å¤„ç†æµç¨‹\n",
        "\n",
        "**åŸå§‹åºåˆ—ç‰¹å¾ â†’ æå–ç»Ÿè®¡ç‰¹å¾ â†’ XGBoostè®­ç»ƒ**\n",
        "\n",
        "1. **åŸå§‹æ•°æ®æ ¼å¼**ï¼š\n",
        "   - `pkt_len`: `\"[40, 40, 1432, 712, ...]\"` (åŒ…é•¿åº¦åºåˆ—)\n",
        "   - `ip_direction`: `\"[0, 0, 1, 1, 0, ...]\"` (æ–¹å‘åºåˆ—)  \n",
        "   - `iat`: `\"[0.0, 0.016, 0.083, ...]\"` (æ—¶é—´é—´éš”åºåˆ—)\n",
        "\n",
        "2. **ç‰¹å¾æå–ç­–ç•¥**ï¼š\n",
        "   - **ç»Ÿè®¡ç‰¹å¾**ï¼šå‡å€¼ã€æ–¹å·®ã€åˆ†ä½æ•°ç­‰ (é€‚ç”¨äºæ‰€æœ‰åºåˆ—)\n",
        "   - **é¢†åŸŸç‰¹å¾**ï¼šæ ¹æ®åºåˆ—å«ä¹‰è®¾è®¡çš„ä¸“é—¨ç‰¹å¾\n",
        "   - **æ¨¡å¼ç‰¹å¾**ï¼šå˜åŒ–è¶‹åŠ¿ã€çªå‘æ€§ç­‰æ—¶åºç‰¹å¾\n",
        "\n",
        "3. **XGBoostä¼˜åŠ¿**ï¼š\n",
        "   - å¯ä»¥è‡ªåŠ¨å‘ç°ç‰¹å¾ä¹‹é—´çš„å¤æ‚ç»„åˆ\n",
        "   - é€šè¿‡æ ‘ç»“æ„æ•è·éçº¿æ€§æ¨¡å¼\n",
        "   - ç‰¹å¾é‡è¦æ€§åˆ†æå¸®åŠ©ç†è§£å“ªäº›åºåˆ—æ¨¡å¼æœ€é‡è¦\n",
        "\n",
        "### ğŸ“ˆ é¢„æœŸæ•ˆæœ\n",
        "\n",
        "é€šè¿‡è¿™ç§å¤„ç†æ–¹å¼ï¼Œæˆ‘ä»¬å°†**3ä¸ªå˜é•¿åºåˆ—**è½¬æ¢ä¸º**æ•°åä¸ªå›ºå®šé•¿åº¦çš„æ•°å€¼ç‰¹å¾**ï¼Œè¿™äº›ç‰¹å¾èƒ½å¤Ÿå……åˆ†è¡¨è¾¾ç½‘ç»œæµé‡çš„æ—¶åºè¡Œä¸ºæ¨¡å¼ï¼Œå¸®åŠ©XGBoostå‡†ç¡®åŒºåˆ†æ­£å¸¸æµé‡å’ŒPCDNæµé‡ã€‚\n",
        "\n",
        "## ğŸ› ï¸ ä»£ç è´¨é‡æ”¹è¿›\n",
        "\n",
        "### ğŸ”’ å®‰å…¨æ€§ä¿®å¤\n",
        "- **æ›¿æ¢ `eval()` å‡½æ•°**ï¼šä½¿ç”¨ `ast.literal_eval()` å®‰å…¨è§£ææ•°ç»„å­—ç¬¦ä¸²ï¼Œé¿å…ä»£ç æ³¨å…¥é£é™©\n",
        "- **å¢å¼ºé”™è¯¯å¤„ç†**ï¼šæ·»åŠ å®Œå–„çš„å¼‚å¸¸æ•è·å’Œæ•°æ®éªŒè¯\n",
        "\n",
        "### ğŸ“Š æ•°æ®ä¸€è‡´æ€§ä¿è¯\n",
        "- **ç‰¹å¾ç»´åº¦å¯¹é½**ï¼šç¡®ä¿è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†å…·æœ‰ç›¸åŒçš„ç‰¹å¾åˆ—\n",
        "- **ç©ºæ•°æ®é›†å¤„ç†**ï¼šè‡ªåŠ¨ä»è®­ç»ƒé›†åˆ†å‰²éªŒè¯/æµ‹è¯•é›†ï¼Œé˜²æ­¢æ•°æ®ç¼ºå¤±\n",
        "- **æ•°æ®è´¨é‡æ£€æŸ¥**ï¼šæ£€æµ‹NaNå€¼ã€æ— ç©·å€¼å’Œæ ‡ç­¾åˆ†å¸ƒ\n",
        "\n",
        "### âš™ï¸ æ¨¡å‹å‚æ•°ä¼˜åŒ–\n",
        "- **XGBoostç‰ˆæœ¬å…¼å®¹æ€§**ï¼šæ™ºèƒ½é€‚é…ä¸åŒç‰ˆæœ¬çš„XGBoostå‚æ•°\n",
        "- **å¤šé‡å¤‡ç”¨æ–¹æ¡ˆ**ï¼šç¡®ä¿åœ¨å„ç§ç¯å¢ƒä¸‹éƒ½èƒ½æ­£å¸¸è¿è¡Œ\n",
        "- **æ”¹è¿›é”™è¯¯å¤„ç†**ï¼šæ›´robustçš„åºåˆ—ç‰¹å¾å¤„ç†æµç¨‹\n",
        "\n",
        "### ğŸ”§ XGBoostç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤\n",
        "\n",
        "**é—®é¢˜èƒŒæ™¯**: ä¸åŒç‰ˆæœ¬çš„XGBoostå¯¹ `early_stopping_rounds` å‚æ•°çš„å¤„ç†æ–¹å¼ä¸åŒ\n",
        "- **æ—§ç‰ˆæœ¬** (< 1.6): åœ¨ `fit()` æ–¹æ³•ä¸­ä½¿ç”¨ `early_stopping_rounds`\n",
        "- **æ–°ç‰ˆæœ¬** (>= 1.6): å¯èƒ½éœ€è¦åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®æˆ–ä½¿ç”¨å…¶ä»–æ–¹å¼\n",
        "\n",
        "**è§£å†³æ–¹æ¡ˆ**: å®ç°äº†ä¸‰å±‚å…¼å®¹ç­–ç•¥\n",
        "1. **ä¼˜å…ˆå°è¯•**: æ–°ç‰ˆæœ¬æ–¹å¼ï¼ˆæ— early_stopping_roundsï¼‰\n",
        "2. **å¤‡ç”¨æ–¹æ¡ˆ1**: åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶è®¾ç½®early_stopping_rounds \n",
        "3. **å¤‡ç”¨æ–¹æ¡ˆ2**: å®Œå…¨ç¦ç”¨early stoppingï¼Œå¢åŠ è®­ç»ƒè½®æ•°è¡¥å¿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ› ï¸ XGBoostç‰ˆæœ¬å…¼å®¹æ€§è¯´æ˜\n",
        "\n",
        "å¦‚æœæ‚¨é‡åˆ°äº† `XGBClassifier.fit() got an unexpected argument 'early_stopping_rounds'` é”™è¯¯ï¼Œä¸ç”¨æ‹…å¿ƒï¼\n",
        "\n",
        "**åŸå› **: XGBooståœ¨ä¸åŒç‰ˆæœ¬ä¸­å¯¹early stoppingçš„å¤„ç†æ–¹å¼æœ‰æ‰€å˜åŒ–\n",
        "\n",
        "**è§£å†³æ–¹æ¡ˆ**: ä»£ç å·²ç»å®ç°äº†æ™ºèƒ½ç‰ˆæœ¬é€‚é…\n",
        "- âœ… **è‡ªåŠ¨æ£€æµ‹**ï¼šä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹XGBoostç‰ˆæœ¬\n",
        "- âœ… **æ™ºèƒ½é™çº§**ï¼šå¦‚æœæ–°æ–¹å¼å¤±è´¥ï¼Œä¼šè‡ªåŠ¨å°è¯•å¤‡ç”¨æ–¹æ¡ˆ  \n",
        "- âœ… **ç¡®ä¿è¿è¡Œ**ï¼šæœ€ç»ˆç¡®ä¿æ¨¡å‹èƒ½å¤ŸæˆåŠŸè®­ç»ƒ\n",
        "\n",
        "**å…¼å®¹çš„XGBoostç‰ˆæœ¬**: \n",
        "- XGBoost 1.0+ âœ…\n",
        "- XGBoost 1.6+ âœ… \n",
        "- XGBoost 2.0+ âœ…\n",
        "\n",
        "æ‚¨æ— éœ€æ‰‹åŠ¨ä¿®æ”¹ä»»ä½•ä»£ç ï¼Œç›´æ¥è¿è¡Œå³å¯ï¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å‡†å¤‡è®­ç»ƒæ•°æ® - ä½¿ç”¨å¤„ç†åçš„å®é™…ç‰¹å¾åˆ—\n",
        "final_feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"å®é™…ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {len(final_feature_columns)}\")\n",
        "\n",
        "X_train = train_processed[final_feature_columns]\n",
        "y_train = train_processed['label']\n",
        "\n",
        "X_val = val_processed[final_feature_columns] \n",
        "y_val = val_processed['label']\n",
        "\n",
        "X_test = test_processed[final_feature_columns]\n",
        "y_test = test_processed['label']\n",
        "\n",
        "print(f\"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train.shape}\")\n",
        "print(f\"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val.shape}\")\n",
        "print(f\"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test.shape}\")\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éæ•°å€¼æ•°æ®\n",
        "print(f\"\\nè®­ç»ƒæ•°æ®ä¸­çš„æ•°æ®ç±»å‹:\")\n",
        "print(X_train.dtypes.value_counts())\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æ•°å€¼å‹\n",
        "X_train = X_train.select_dtypes(include=[np.number])\n",
        "X_val = X_val.select_dtypes(include=[np.number])\n",
        "X_test = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\næœ€ç»ˆç‰¹å¾æ•°é‡: {X_train.shape[1]}\")\n",
        "\n",
        "# æ•°æ®è´¨é‡æ£€æŸ¥\n",
        "print(\"\\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:\")\n",
        "print(f\"è®­ç»ƒé›†æ˜¯å¦åŒ…å«NaN: {X_train.isnull().any().any()}\")\n",
        "print(f\"éªŒè¯é›†æ˜¯å¦åŒ…å«NaN: {X_val.isnull().any().any()}\")\n",
        "print(f\"æµ‹è¯•é›†æ˜¯å¦åŒ…å«NaN: {X_test.isnull().any().any()}\")\n",
        "print(f\"è®­ç»ƒé›†æ˜¯å¦åŒ…å«æ— ç©·å€¼: {np.isinf(X_train).any().any()}\")\n",
        "print(f\"æ ‡ç­¾åˆ†å¸ƒ - è®­ç»ƒé›†: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"æ ‡ç­¾åˆ†å¸ƒ - éªŒè¯é›†: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"æ ‡ç­¾åˆ†å¸ƒ - æµ‹è¯•é›†: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ä¸€è‡´\n",
        "assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´ï¼\"\n",
        "print(\"âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoostæ¨¡å‹è®­ç»ƒ\n",
        "print(\"å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...\")\n",
        "\n",
        "# æ£€æŸ¥XGBoostç‰ˆæœ¬å¹¶é€‚é…å‚æ•°\n",
        "import xgboost\n",
        "print(f\"XGBoostç‰ˆæœ¬: {xgboost.__version__}\")\n",
        "\n",
        "# åˆ›å»ºXGBooståˆ†ç±»å™¨\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# å…¼å®¹ä¸åŒç‰ˆæœ¬çš„XGBoostè®­ç»ƒæ–¹å¼\n",
        "print(\"å¼€å§‹æ¨¡å‹è®­ç»ƒ...\")\n",
        "try:\n",
        "    # æ–°ç‰ˆæœ¬XGBoostçš„æ–¹å¼ (>= 1.6.0)\n",
        "    xgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False  # ç®€åŒ–è¾“å‡º\n",
        "    )\n",
        "    print(\"âœ… ä½¿ç”¨æ–°ç‰ˆXGBoostè®­ç»ƒæ–¹å¼\")\n",
        "except TypeError as e:\n",
        "    if \"early_stopping_rounds\" in str(e):\n",
        "        # å¦‚æœæ˜¯early_stopping_roundså‚æ•°é—®é¢˜ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹å¼\n",
        "        print(\"âš ï¸ æ£€æµ‹åˆ°XGBoostç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œä½¿ç”¨å¤‡ç”¨è®­ç»ƒæ–¹å¼...\")\n",
        "        \n",
        "        # æ–¹å¼1: åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®early_stopping_rounds (æŸäº›ç‰ˆæœ¬)\n",
        "        try:\n",
        "            xgb_model = xgb.XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                eval_metric='logloss',\n",
        "                early_stopping_rounds=10\n",
        "            )\n",
        "            xgb_model.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "            print(\"âœ… ä½¿ç”¨early_stopping_roundsåœ¨åˆå§‹åŒ–ä¸­çš„æ–¹å¼\")\n",
        "        except:\n",
        "            # æ–¹å¼2: ä¸ä½¿ç”¨early stoppingï¼Œå¢åŠ n_estimators\n",
        "            print(\"ğŸ”„ ä½¿ç”¨æ— early stoppingçš„æ–¹å¼ï¼Œå¢åŠ è®­ç»ƒè½®æ•°...\")\n",
        "            xgb_model = xgb.XGBClassifier(\n",
        "                n_estimators=150,  # å¢åŠ è½®æ•°è¡¥å¿\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                eval_metric='logloss'\n",
        "            )\n",
        "            xgb_model.fit(X_train, y_train, verbose=False)\n",
        "            print(\"âœ… ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ–¹å¼ï¼ˆæ— early stoppingï¼‰\")\n",
        "    else:\n",
        "        raise e  # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º\n",
        "\n",
        "print(\"\\nXGBoostæ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡å‹è¯„ä¼°\n",
        "def evaluate_model(model, X, y, data_name):\n",
        "    \"\"\"\n",
        "    è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
        "    \"\"\"\n",
        "    # é¢„æµ‹\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "    \n",
        "    print(f\"\\n=== {data_name} è¯„ä¼°ç»“æœ ===\")\n",
        "    \n",
        "    # åˆ†ç±»æŠ¥å‘Š\n",
        "    print(\"\\nåˆ†ç±»æŠ¥å‘Š:\")\n",
        "    print(classification_report(y, y_pred, target_names=['Normal', 'PCDN']))\n",
        "    \n",
        "    # AUCåˆ†æ•°\n",
        "    auc_score = roc_auc_score(y, y_pred_proba)\n",
        "    print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
        "    \n",
        "    return y_pred, y_pred_proba, auc_score\n",
        "\n",
        "# è¯„ä¼°è®­ç»ƒé›†\n",
        "train_pred, train_proba, train_auc = evaluate_model(xgb_model, X_train, y_train, \"è®­ç»ƒé›†\")\n",
        "\n",
        "# è¯„ä¼°éªŒè¯é›†\n",
        "val_pred, val_proba, val_auc = evaluate_model(xgb_model, X_val, y_val, \"éªŒè¯é›†\")\n",
        "\n",
        "# è¯„ä¼°æµ‹è¯•é›†\n",
        "test_pred, test_proba, test_auc = evaluate_model(xgb_model, X_test, y_test, \"æµ‹è¯•é›†\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== Top 20 æœ€é‡è¦ç‰¹å¾ ===\")\n",
        "print(importance_df.head(20))\n",
        "\n",
        "# ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = importance_df.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 20 Feature Importance in XGBoost Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶ROCæ›²çº¿å’Œæ··æ·†çŸ©é˜µ\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# ROCæ›²çº¿\n",
        "datasets = [\n",
        "    (y_train, train_proba, \"Training\", train_auc),\n",
        "    (y_val, val_proba, \"Validation\", val_auc),\n",
        "    (y_test, test_proba, \"Testing\", test_auc)\n",
        "]\n",
        "\n",
        "ax_roc = axes[0, 0]\n",
        "for y_true, y_prob, label, auc in datasets:\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    ax_roc.plot(fpr, tpr, label=f'{label} (AUC = {auc:.3f})')\n",
        "\n",
        "ax_roc.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "ax_roc.set_xlabel('False Positive Rate')\n",
        "ax_roc.set_ylabel('True Positive Rate')\n",
        "ax_roc.set_title('ROC Curves')\n",
        "ax_roc.legend()\n",
        "ax_roc.grid(True)\n",
        "\n",
        "# æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Normal', 'PCDN'], \n",
        "            yticklabels=['Normal', 'PCDN'],\n",
        "            ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Confusion Matrix - Test Set')\n",
        "axes[0, 1].set_ylabel('True Label')\n",
        "axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "# é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n",
        "axes[1, 0].hist(test_proba[y_test == 0], alpha=0.7, label='Normal', bins=30, density=True)\n",
        "axes[1, 0].hist(test_proba[y_test == 1], alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "axes[1, 0].set_xlabel('Prediction Probability')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Prediction Probability Distribution')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# å­¦ä¹ æ›²çº¿ï¼ˆè®­ç»ƒå†å²ï¼‰\n",
        "results = xgb_model.evals_result()\n",
        "if 'validation_0' in results:\n",
        "    epochs = len(results['validation_0']['logloss'])\n",
        "    x_axis = range(0, epochs)\n",
        "    axes[1, 1].plot(x_axis, results['validation_0']['logloss'], label='Validation')\n",
        "    axes[1, 1].set_xlabel('Epochs')\n",
        "    axes[1, 1].set_ylabel('Log Loss')\n",
        "    axes[1, 1].set_title('Model Learning Curve')\n",
        "    axes[1, 1].legend()\n",
        "else:\n",
        "    axes[1, 1].text(0.5, 0.5, 'Learning curve not available', \n",
        "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "    axes[1, 1].set_title('Learning Curve')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾ç›¸å…³æ€§åˆ†æ\n",
        "if len(importance_df) >= 10:\n",
        "    # é€‰æ‹©æœ€é‡è¦çš„10ä¸ªç‰¹å¾è¿›è¡Œç›¸å…³æ€§åˆ†æ\n",
        "    top_10_features = importance_df.head(10)['feature'].tolist()\n",
        "    corr_data = train_processed[top_10_features + ['label']]\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = corr_data.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f')\n",
        "    plt.title('Correlation Matrix of Top 10 Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n=== ä¸æ ‡ç­¾ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾ ===\")\n",
        "    label_corr = correlation_matrix['label'].abs().sort_values(ascending=False)\n",
        "    print(label_corr[label_corr.index != 'label'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜æ¨¡å‹å’Œç»“æœ (å¢å¼ºç‰ˆ)\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "output_dir = './output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ä¿å­˜æ¨¡å‹\n",
        "model_path = os.path.join(output_dir, 'xgboost_pcdn_classifier.pkl')\n",
        "try:\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜ç‰¹å¾é‡è¦æ€§\n",
        "importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
        "try:\n",
        "    importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ°: {importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ç‰¹å¾é‡è¦æ€§ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜æ€§èƒ½æŠ¥å‘Š\n",
        "performance_path = os.path.join(output_dir, 'model_performance.csv')\n",
        "try:\n",
        "    performance_summary.to_csv(performance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… æ¨¡å‹æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {performance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ€§èƒ½æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š(åŒ…å«åºåˆ—ç‰¹å¾é…ç½®ä¿¡æ¯)\n",
        "analysis_report = {\n",
        "    'Project': 'PCDN vs Normal Traffic Classification',\n",
        "    'Algorithm': 'XGBoost',\n",
        "    'Sequence_Features_Enabled': ENABLE_SEQUENCE_FEATURES,\n",
        "    'Total_Features': len(feature_names),\n",
        "    'Train_Samples': len(y_train),\n",
        "    'Val_Samples': len(y_val),\n",
        "    'Test_Samples': len(y_test),\n",
        "    'Train_AUC': train_auc,\n",
        "    'Val_AUC': val_auc,\n",
        "    'Test_AUC': test_auc,\n",
        "    'Top_5_Features': importance_df.head(5)['feature'].tolist()\n",
        "}\n",
        "\n",
        "report_path = os.path.join(output_dir, 'analysis_report.txt')\n",
        "try:\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PCDNæµé‡åˆ†ç±»é¡¹ç›®åˆ†ææŠ¥å‘Š\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"åºåˆ—ç‰¹å¾å¤„ç†æ¨¡å¼: {'å¯ç”¨' if ENABLE_SEQUENCE_FEATURES else 'ç¦ç”¨'}\\n\")\n",
        "        if ENABLE_SEQUENCE_FEATURES:\n",
        "            f.write(\"- ä½¿ç”¨äº†å¤æ‚çš„åºåˆ—ç‰¹å¾å·¥ç¨‹\\n\")\n",
        "            f.write(\"- ä»3ä¸ªåºåˆ—ç‰¹å¾ç”Ÿæˆäº†45+ä¸ªç»Ÿè®¡ç‰¹å¾\\n\")\n",
        "        else:\n",
        "            f.write(\"- åˆ é™¤äº†åºåˆ—ç‰¹å¾ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å‹\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        for key, value in analysis_report.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "    print(f\"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åˆ†ææŠ¥å‘Šä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "print(f\"\\nğŸ“ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°ç›®å½•: {output_dir}\")\n",
        "print(f\"ğŸ”§ å½“å‰é…ç½®: åºåˆ—ç‰¹å¾ {'å¯ç”¨' if ENABLE_SEQUENCE_FEATURES else 'ç¦ç”¨'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡å‹æ€§èƒ½æ€»ç»“\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             æ¨¡å‹æ€§èƒ½æ€»ç»“\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "performance_summary = pd.DataFrame({\n",
        "    'æ•°æ®é›†': ['è®­ç»ƒé›†', 'éªŒè¯é›†', 'æµ‹è¯•é›†'],\n",
        "    'AUC Score': [train_auc, val_auc, test_auc],\n",
        "    'æ ·æœ¬æ•°é‡': [len(y_train), len(y_val), len(y_test)]\n",
        "})\n",
        "\n",
        "print(performance_summary.to_string(index=False))\n",
        "\n",
        "print(f\"\\nç‰¹å¾æ€»æ•°: {len(feature_names)}\")\n",
        "print(f\"æœ€é‡è¦çš„5ä¸ªç‰¹å¾:\")\n",
        "for i, (idx, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
        "    print(f\"  {i}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\næ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜æ¨¡å‹å’Œç»“æœ\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "output_dir = './output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ä¿å­˜æ¨¡å‹\n",
        "model_path = os.path.join(output_dir, 'xgboost_pcdn_classifier.pkl')\n",
        "try:\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜ç‰¹å¾é‡è¦æ€§\n",
        "importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
        "try:\n",
        "    importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ°: {importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ç‰¹å¾é‡è¦æ€§ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜æ€§èƒ½æŠ¥å‘Š\n",
        "performance_path = os.path.join(output_dir, 'model_performance.csv')\n",
        "try:\n",
        "    performance_summary.to_csv(performance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… æ¨¡å‹æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {performance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ€§èƒ½æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š\n",
        "analysis_report = {\n",
        "    'Project': 'PCDN vs Normal Traffic Classification',\n",
        "    'Algorithm': 'XGBoost',\n",
        "    'Total_Features': len(feature_names),\n",
        "    'Train_Samples': len(y_train),\n",
        "    'Val_Samples': len(y_val),\n",
        "    'Test_Samples': len(y_test),\n",
        "    'Train_AUC': train_auc,\n",
        "    'Val_AUC': val_auc,\n",
        "    'Test_AUC': test_auc,\n",
        "    'Top_5_Features': importance_df.head(5)['feature'].tolist()\n",
        "}\n",
        "\n",
        "report_path = os.path.join(output_dir, 'analysis_report.txt')\n",
        "try:\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PCDNæµé‡åˆ†ç±»é¡¹ç›®åˆ†ææŠ¥å‘Š\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        for key, value in analysis_report.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "    print(f\"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åˆ†ææŠ¥å‘Šä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "print(f\"\\nğŸ“ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°ç›®å½•: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ é¡¹ç›®å®Œæˆæ€»ç»“\n",
        "\n",
        "### âœ… å·²å®Œæˆçš„å·¥ä½œ\n",
        "\n",
        "1. **æ•°æ®åŠ è½½ä¸é¢„å¤„ç†** \n",
        "   - âœ… æ™ºèƒ½è·¯å¾„æ£€æµ‹å’Œæ•°æ®åŠ è½½\n",
        "   - âœ… è‡ªåŠ¨å¤„ç†ç¼ºå¤±çš„éªŒè¯/æµ‹è¯•é›†\n",
        "   - âœ… å®‰å…¨çš„åºåˆ—ç‰¹å¾è§£æ\n",
        "\n",
        "2. **ç‰¹å¾å·¥ç¨‹**\n",
        "   - âœ… åºåˆ—ç‰¹å¾è½¬ç»Ÿè®¡ç‰¹å¾ (45+ æ–°ç‰¹å¾)\n",
        "   - âœ… éæ•°å€¼ç‰¹å¾è‡ªåŠ¨ç¼–ç \n",
        "   - âœ… æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—\n",
        "\n",
        "3. **æ¨¡å‹è®­ç»ƒ**\n",
        "   - âœ… XGBooståˆ†ç±»å™¨è®­ç»ƒ\n",
        "   - âœ… æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "   - âœ… æ¨¡å‹æ€§èƒ½è¯„ä¼°\n",
        "\n",
        "4. **ç»“æœåˆ†æ**\n",
        "   - âœ… ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "   - âœ… ROCæ›²çº¿å’Œæ··æ·†çŸ©é˜µ\n",
        "   - âœ… ç›¸å…³æ€§åˆ†æ\n",
        "   - âœ… å¯è§†åŒ–å±•ç¤º\n",
        "\n",
        "5. **è¾“å‡ºç®¡ç†**\n",
        "   - âœ… æ¨¡å‹æ–‡ä»¶ä¿å­˜\n",
        "   - âœ… ç»“æœæŠ¥å‘Šå¯¼å‡º\n",
        "   - âœ… é¡¹ç›®æ–‡æ¡£å®Œå–„\n",
        "\n",
        "### ğŸš€ ä½¿ç”¨æ–¹æ³•\n",
        "\n",
        "1. **ç¯å¢ƒå‡†å¤‡**: ç¡®ä¿å®‰è£…æ‰€éœ€PythonåŒ… (pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, scipy)\n",
        "\n",
        "2. **æ•°æ®å‡†å¤‡**: å°†æ•°æ®é›†æ–‡ä»¶å¤¹æ”¾åœ¨notebookåŒç›®å½•ä¸‹\n",
        "\n",
        "3. **è¿è¡Œé¡¹ç›®**: ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰ä»£ç å•å…ƒ\n",
        "\n",
        "4. **æŸ¥çœ‹ç»“æœ**: æ£€æŸ¥ `./output/` ç›®å½•ä¸­çš„è¾“å‡ºæ–‡ä»¶\n",
        "\n",
        "### ğŸ¯ é¡¹ç›®ä»·å€¼\n",
        "\n",
        "- **å®ç”¨æ€§**: å¯ç›´æ¥ç”¨äºPCDNæµé‡æ£€æµ‹\n",
        "- **æ‰©å±•æ€§**: å¯è½»æ¾é€‚é…å…¶ä»–ç½‘ç»œæµé‡åˆ†ç±»ä»»åŠ¡  \n",
        "- **å¯è§£é‡Šæ€§**: è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "- **å¯ç»´æŠ¤æ€§**: æ¸…æ™°çš„ä»£ç ç»“æ„å’Œæ–‡æ¡£\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bysj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
