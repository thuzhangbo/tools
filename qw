# PCDN vs Normal Traffic Classification using XGBoost
# ä½¿ç”¨ ip.proto å’Œ tcp.srcport ç‰¹å¾è¿›è¡ŒäºŒåˆ†ç±»

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
from sklearn.preprocessing import StandardScaler
import os
import glob
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®å›¾è¡¨æ ·å¼
try:
    plt.style.use('seaborn-v0_8')
except:
    plt.style.use('default')
sns.set_palette("husl")

print("ğŸš€ å¼€å§‹PCDNæµé‡åˆ†ç±»ä»»åŠ¡")
print("ğŸ“‹ ä½¿ç”¨ç‰¹å¾: ip.proto, tcp.srcport")
print("=" * 60)

# å®šä¹‰æ•°æ®è·¯å¾„
base_path = "pcdn_32_pkts_2class_feature_enhance_v17.4_dataset"
train_path = os.path.join(base_path, "Training_set")
val_path = os.path.join(base_path, "Validation_set") 
test_path = os.path.join(base_path, "Testing_set")

# é€‰æ‹©çš„ç‰¹å¾
selected_features = ['ip.proto', 'tcp.srcport']

def load_data_from_directory(directory_path, label):
    """åŠ è½½æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = glob.glob(os.path.join(directory_path, "*.csv"))
    dataframes = []
    
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            df['label'] = label  # æ·»åŠ æ ‡ç­¾
            df['source_file'] = os.path.basename(file)  # è®°å½•æ¥æºæ–‡ä»¶
            dataframes.append(df)
            print(f"âœ… åŠ è½½æ–‡ä»¶: {file} (æ ·æœ¬æ•°: {len(df)})")
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file}, é”™è¯¯: {e}")
    
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"ğŸ“Š {directory_path} æ€»æ ·æœ¬æ•°: {len(combined_df)}")
        return combined_df
    else:
        print(f"âš ï¸  {directory_path} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return pd.DataFrame()

# 1. åŠ è½½è®­ç»ƒé›†æ•°æ®
print("\n1ï¸âƒ£ åŠ è½½è®­ç»ƒé›†æ•°æ®...")
train_normal = load_data_from_directory(os.path.join(train_path, "APP_0"), 0)  # æ­£å¸¸æµé‡
train_pcdn = load_data_from_directory(os.path.join(train_path, "APP_1"), 1)    # PCDNæµé‡

# 2. åŠ è½½éªŒè¯é›†æ•°æ®
print("\n2ï¸âƒ£ åŠ è½½éªŒè¯é›†æ•°æ®...")
val_normal = load_data_from_directory(os.path.join(val_path, "APP_0"), 0)
val_pcdn = load_data_from_directory(os.path.join(val_path, "APP_1"), 1)

# 3. åŠ è½½æµ‹è¯•é›†æ•°æ®
print("\n3ï¸âƒ£ åŠ è½½æµ‹è¯•é›†æ•°æ®...")
test_normal = load_data_from_directory(os.path.join(test_path, "APP_0"), 0)
test_pcdn = load_data_from_directory(os.path.join(test_path, "APP_1"), 1)

# 4. åˆå¹¶æ•°æ®é›†
print("\n4ï¸âƒ£ åˆå¹¶æ•°æ®é›†...")

# æ£€æŸ¥æ˜¯å¦æœ‰ç©ºçš„æ•°æ®é›†
def safe_concat(dataframes, set_name):
    """å®‰å…¨åˆå¹¶æ•°æ®é›†ï¼Œå¤„ç†ç©ºæ•°æ®é›†çš„æƒ…å†µ"""
    non_empty_dfs = [df for df in dataframes if not df.empty]
    if not non_empty_dfs:
        print(f"âš ï¸  {set_name} æ²¡æœ‰æœ‰æ•ˆæ•°æ®!")
        return pd.DataFrame()
    return pd.concat(non_empty_dfs, ignore_index=True)

train_data = safe_concat([train_normal, train_pcdn], "è®­ç»ƒé›†")
val_data = safe_concat([val_normal, val_pcdn], "éªŒè¯é›†")
test_data = safe_concat([test_normal, test_pcdn], "æµ‹è¯•é›†")

# æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
if train_data.empty or val_data.empty or test_data.empty:
    print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ–‡ä»¶")
    exit()

print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬ (æ­£å¸¸: {len(train_normal)}, PCDN: {len(train_pcdn)})")
print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬ (æ­£å¸¸: {len(val_normal)}, PCDN: {len(val_pcdn)})")
print(f"æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬ (æ­£å¸¸: {len(test_normal)}, PCDN: {len(test_pcdn)})")

# 5. ç‰¹å¾æå–å’Œé¢„å¤„ç†
print(f"\n5ï¸âƒ£ ç‰¹å¾æå–å’Œé¢„å¤„ç†...")
print(f"é€‰æ‹©çš„ç‰¹å¾: {selected_features}")

def preprocess_features(data, features):
    """é¢„å¤„ç†ç‰¹å¾æ•°æ®"""
    processed_data = data.copy()
    
    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
    missing_features = [f for f in features if f not in processed_data.columns]
    if missing_features:
        print(f"âš ï¸  ç¼ºå¤±ç‰¹å¾: {missing_features}")
        return None
    
    # æå–é€‰æ‹©çš„ç‰¹å¾
    feature_data = processed_data[features].copy()
    
    # å¤„ç†ç¼ºå¤±å€¼
    print(f"ğŸ“Š ç‰¹å¾ç¼ºå¤±å€¼ç»Ÿè®¡:")
    for feature in features:
        missing_count = feature_data[feature].isna().sum()
        print(f"  {feature}: {missing_count} ä¸ªç¼ºå¤±å€¼")
        
        # å¯¹äºæ•°å€¼å‹ç‰¹å¾ï¼Œç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
        if missing_count > 0:
            median_val = feature_data[feature].median()
            if pd.isna(median_val):  # å¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaNï¼ˆå…¨éƒ¨éƒ½æ˜¯ç¼ºå¤±å€¼ï¼‰
                # ä½¿ç”¨0å¡«å……æˆ–è€…ç‰¹å¾çš„å…¸å‹å€¼
                if feature == 'ip.proto':
                    fill_val = 6  # TCPåè®®
                elif feature == 'tcp.srcport':
                    fill_val = 0  # é»˜è®¤ç«¯å£
                else:
                    fill_val = 0  # é€šç”¨é»˜è®¤å€¼
                print(f"    ç‰¹å¾å…¨éƒ¨ç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤å€¼ {fill_val} å¡«å……")
            else:
                fill_val = median_val
                print(f"    å·²ç”¨ä¸­ä½æ•° {fill_val} å¡«å……")
            
            feature_data[feature].fillna(fill_val, inplace=True)
    
    return feature_data

# é¢„å¤„ç†å„æ•°æ®é›†çš„ç‰¹å¾
X_train = preprocess_features(train_data, selected_features)
X_val = preprocess_features(val_data, selected_features)
X_test = preprocess_features(test_data, selected_features)

if X_train is None or X_val is None or X_test is None:
    print("âŒ ç‰¹å¾é¢„å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç‰¹å¾åç§°")
    exit()

# æå–æ ‡ç­¾
y_train = train_data['label'].values
y_val = val_data['label'].values  
y_test = test_data['label'].values

# 6. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆé‡è¦ï¼šç¡®ä¿è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–å‚æ•°ï¼‰
print(f"\n6ï¸âƒ£ ç‰¹å¾æ ‡å‡†åŒ–...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # åªåœ¨è®­ç»ƒé›†ä¸Šfit
X_val_scaled = scaler.transform(X_val)          # éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°
X_test_scaled = scaler.transform(X_test)        # æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°

print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ")
print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train_scaled.shape}")
print(f"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val_scaled.shape}")
print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test_scaled.shape}")

# 7. æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡
print(f"\n7ï¸âƒ£ æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_train)}")
print(f"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_val)}")
print(f"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_test)}")

# æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡ (è®­ç»ƒé›†):")
feature_stats = pd.DataFrame(X_train, columns=selected_features).describe()
print(feature_stats)

print(f"\nğŸ¯ æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¼€å§‹æ¨¡å‹è®­ç»ƒ...")

# 8. XGBoostæ¨¡å‹è®­ç»ƒï¼ˆç‰ˆæœ¬å…¼å®¹ï¼‰
print(f"\n8ï¸âƒ£ å¼€å§‹XGBoostæ¨¡å‹è®­ç»ƒ...")
print("=" * 80)

# æ£€æŸ¥XGBoostç‰ˆæœ¬
print(f"XGBoostç‰ˆæœ¬: {xgb.__version__}")

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = "output"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

# ğŸ”§ ç‰ˆæœ¬å…¼å®¹çš„è®­ç»ƒç­–ç•¥ï¼ˆä¸‰çº§å›é€€æœºåˆ¶ï¼‰
training_success = False
training_method = ""
xgb_model = None

print("\nğŸ”§ å¼€å§‹æ™ºèƒ½è®­ç»ƒï¼ˆç‰ˆæœ¬å…¼å®¹ï¼‰...")

try:
    # æ–¹æ³•1: æ–°ç‰ˆæœ¬XGBoostæ–¹å¼ï¼ˆæ¨èï¼‰
    print("ğŸ”„ å°è¯•æ–¹æ³•1: æ–°ç‰ˆæœ¬è®­ç»ƒæ–¹å¼...")
    xgb_model = xgb.XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        max_depth=6,
        learning_rate=0.1,
        n_estimators=100,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=0  # å‡å°‘è¾“å‡º
    )
    xgb_model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_val_scaled, y_val)],
        verbose=False
    )
    training_success = True
    training_method = "æ–°ç‰ˆæœ¬æ–¹å¼"
    print("âœ… æ–¹æ³•1æˆåŠŸ")
    
except Exception as e:
    print(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {e}")
    
    try:
        # æ–¹æ³•2: åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶è®¾ç½®early_stopping_rounds
        print("ğŸ”„ å°è¯•æ–¹æ³•2: åˆå§‹åŒ–æ—¶è®¾ç½®early_stopping...")
        xgb_model = xgb.XGBClassifier(
            objective='binary:logistic',
            eval_metric='logloss',
            max_depth=6,
            learning_rate=0.1,
            n_estimators=100,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            early_stopping_rounds=10,
            verbosity=0
        )
        xgb_model.fit(
            X_train_scaled, y_train,
            eval_set=[(X_val_scaled, y_val)],
            verbose=False
        )
        training_success = True
        training_method = "åˆå§‹åŒ–early_stoppingæ–¹å¼"
        print("âœ… æ–¹æ³•2æˆåŠŸ")
        
    except Exception as e2:
        print(f"âš ï¸ æ–¹æ³•2å¤±è´¥: {e2}")
        
        try:
            # æ–¹æ³•3: ç¦ç”¨early stoppingï¼Œå¢åŠ è®­ç»ƒè½®æ•°
            print("ğŸ”„ å°è¯•æ–¹æ³•3: ç¦ç”¨early stopping...")
            xgb_model = xgb.XGBClassifier(
                objective='binary:logistic',
                eval_metric='logloss',
                max_depth=6,
                learning_rate=0.1,
                n_estimators=150,  # å¢åŠ è½®æ•°è¡¥å¿
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbosity=0
            )
            xgb_model.fit(X_train_scaled, y_train)
            training_success = True
            training_method = "æ— early stoppingæ–¹å¼"
            print("âœ… æ–¹æ³•3æˆåŠŸ")
            
        except Exception as e3:
            print(f"âŒ æ–¹æ³•3ä¹Ÿå¤±è´¥: {e3}")
            training_success = False

if training_success:
    print(f"\nğŸ‰ æ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
    print(f"ğŸ“‹ ä½¿ç”¨æ–¹æ³•: {training_method}")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {X_train_scaled.shape[0]}æ ·æœ¬, {X_train_scaled.shape[1]}ç‰¹å¾")
    print(f"ğŸ“Š éªŒè¯æ•°æ®: {X_val_scaled.shape[0]}æ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {X_test_scaled.shape[0]}æ ·æœ¬")
else:
    print("âŒ æ‰€æœ‰è®­ç»ƒæ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥XGBoostç‰ˆæœ¬å’Œæ•°æ®")
    exit()

print("=" * 80)

# 9. æ¨¡å‹é¢„æµ‹
print(f"\n9ï¸âƒ£ æ¨¡å‹é¢„æµ‹...")

# åœ¨å„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹
y_train_pred = xgb_model.predict(X_train_scaled)
y_train_prob = xgb_model.predict_proba(X_train_scaled)[:, 1]

y_val_pred = xgb_model.predict(X_val_scaled)
y_val_prob = xgb_model.predict_proba(X_val_scaled)[:, 1]

y_test_pred = xgb_model.predict(X_test_scaled)
y_test_prob = xgb_model.predict_proba(X_test_scaled)[:, 1]

# 10. æ¨¡å‹è¯„ä¼°
print(f"\nğŸ”Ÿ æ¨¡å‹è¯„ä¼°ç»“æœ...")

# å‡†ç¡®ç‡
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# AUC (å®‰å…¨è®¡ç®—ï¼Œå¤„ç†å•ç±»åˆ«æƒ…å†µ)
def safe_auc_score(y_true, y_prob, set_name):
    """å®‰å…¨è®¡ç®—AUCï¼Œå¤„ç†å•ç±»åˆ«æƒ…å†µ"""
    if len(np.unique(y_true)) < 2:
        print(f"âš ï¸  {set_name} åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è®¡ç®—AUC")
        return 0.5  # è¿”å›é»˜è®¤å€¼
    return roc_auc_score(y_true, y_prob)

train_auc = safe_auc_score(y_train, y_train_prob, "è®­ç»ƒé›†")
val_auc = safe_auc_score(y_val, y_val_prob, "éªŒè¯é›†")
test_auc = safe_auc_score(y_test, y_test_prob, "æµ‹è¯•é›†")

print(f"ğŸ“Š å‡†ç¡®ç‡ (Accuracy):")
print(f"  è®­ç»ƒé›†: {train_acc:.4f}")
print(f"  éªŒè¯é›†: {val_acc:.4f}")
print(f"  æµ‹è¯•é›†: {test_acc:.4f}")

print(f"\nğŸ“Š AUCå€¼:")
print(f"  è®­ç»ƒé›†: {train_auc:.4f}")
print(f"  éªŒè¯é›†: {val_auc:.4f}")
print(f"  æµ‹è¯•é›†: {test_auc:.4f}")

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print(f"\nğŸ“‹ æµ‹è¯•é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_test_pred, target_names=['æ­£å¸¸æµé‡', 'PCDNæµé‡']))

# 11. ç‰¹å¾é‡è¦æ€§åˆ†æ
print(f"\n1ï¸âƒ£1ï¸âƒ£ ç‰¹å¾é‡è¦æ€§åˆ†æ...")

# è·å–ç‰¹å¾é‡è¦æ€§
feature_importance = xgb_model.feature_importances_
feature_names = selected_features

# åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åº:")
for idx, row in importance_df.iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

# 12. å¯è§†åŒ–ç»“æœ
print(f"\n1ï¸âƒ£2ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# åˆ›å»ºå›¾è¡¨
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('PCDNæµé‡åˆ†ç±»ç»“æœåˆ†æ', fontsize=16, fontweight='bold')

# 1. ç‰¹å¾é‡è¦æ€§å›¾
ax1 = axes[0, 0]
# åŠ¨æ€ç”Ÿæˆé¢œè‰²ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„é¢œè‰²
colors = plt.cm.Set3(np.linspace(0, 1, len(importance_df)))
bars = ax1.bar(importance_df['feature'], importance_df['importance'], 
               color=colors)
ax1.set_title('ç‰¹å¾é‡è¦æ€§åˆ†æ', fontweight='bold')
ax1.set_xlabel('ç‰¹å¾åç§°')
ax1.set_ylabel('é‡è¦æ€§åˆ†æ•°')
ax1.tick_params(axis='x', rotation=45)

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, importance in zip(bars, importance_df['importance']):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{importance:.3f}', ha='center', va='bottom', fontweight='bold')

# 2. ROCæ›²çº¿
ax2 = axes[0, 1]

# å®‰å…¨ç»˜åˆ¶ROCæ›²çº¿
def safe_plot_roc(y_true, y_prob, label, ax):
    """å®‰å…¨ç»˜åˆ¶ROCæ›²çº¿"""
    if len(np.unique(y_true)) < 2:
        return  # è·³è¿‡å•ç±»åˆ«æƒ…å†µ
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    ax.plot(fpr, tpr, label=label, linewidth=2)

safe_plot_roc(y_train, y_train_prob, f'è®­ç»ƒé›† (AUC = {train_auc:.3f})', ax2)
safe_plot_roc(y_val, y_val_prob, f'éªŒè¯é›† (AUC = {val_auc:.3f})', ax2)
safe_plot_roc(y_test, y_test_prob, f'æµ‹è¯•é›† (AUC = {test_auc:.3f})', ax2)

ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='éšæœºåˆ†ç±»')
ax2.set_title('ROCæ›²çº¿æ¯”è¾ƒ', fontweight='bold')
ax2.set_xlabel('å‡æ­£ç‡ (FPR)')
ax2.set_ylabel('çœŸæ­£ç‡ (TPR)')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. æ··æ·†çŸ©é˜µ
ax3 = axes[1, 0]
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3,
            xticklabels=['æ­£å¸¸æµé‡', 'PCDNæµé‡'],
            yticklabels=['æ­£å¸¸æµé‡', 'PCDNæµé‡'])
ax3.set_title('æµ‹è¯•é›†æ··æ·†çŸ©é˜µ', fontweight='bold')
ax3.set_xlabel('é¢„æµ‹æ ‡ç­¾')
ax3.set_ylabel('çœŸå®æ ‡ç­¾')

# 4. å‡†ç¡®ç‡å¯¹æ¯”
ax4 = axes[1, 1]
datasets = ['è®­ç»ƒé›†', 'éªŒè¯é›†', 'æµ‹è¯•é›†']
accuracies = [train_acc, val_acc, test_acc]
colors = ['#FF9999', '#66B2FF', '#99FF99']

bars = ax4.bar(datasets, accuracies, color=colors, alpha=0.8)
ax4.set_title('å„æ•°æ®é›†å‡†ç¡®ç‡å¯¹æ¯”', fontweight='bold')
ax4.set_ylabel('å‡†ç¡®ç‡')
ax4.set_ylim(0, 1.1)

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()

# å®‰å…¨ä¿å­˜å›¾è¡¨
try:
    plt.savefig('pcdn_classification_results.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜ä¸º: pcdn_classification_results.png")
except Exception as e:
    print(f"âš ï¸  å›¾è¡¨ä¿å­˜å¤±è´¥: {e}")
    print("ğŸ“Š å›¾è¡¨ä»åœ¨å†…å­˜ä¸­æ˜¾ç¤º")

plt.show()

# 13. æ€»ç»“æŠ¥å‘Š
print(f"\n1ï¸âƒ£3ï¸âƒ£ åˆ†ç±»ä»»åŠ¡æ€»ç»“æŠ¥å‘Š")
print("=" * 60)
print(f"ğŸ¯ ä»»åŠ¡: PCDNæµé‡ä¸æ­£å¸¸æµé‡äºŒåˆ†ç±»")
print(f"ğŸ”§ ä½¿ç”¨ç‰¹å¾: {', '.join(selected_features)}")
print(f"ğŸ“Š æ•°æ®è§„æ¨¡:")
print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

print(f"\nğŸ† æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:")
print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"  æµ‹è¯•é›†AUCå€¼: {test_auc:.4f}")

print(f"\nğŸ“ˆ ç‰¹å¾é‡è¦æ€§:")
for idx, row in importance_df.iterrows():
    percentage = (row['importance'] / importance_df['importance'].sum()) * 100
    print(f"  {row['feature']}: {row['importance']:.4f} ({percentage:.1f}%)")

print(f"\nâœ… åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° pcdn_classification_results.png")
print("=" * 60)
