{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'bysj (Python 3.10.18)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n bysj ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDN vs Normal Traffic Classification using XGBoost\n",
        "\n",
        "这个项目使用XGBoost对正常流量和PCDN流量进行二分类。\n",
        "\n",
        "## 数据集结构\n",
        "- Training_set/APP_0: 正常流量\n",
        "- Training_set/APP_1: PCDN流量\n",
        "- Validation_set: 验证集\n",
        "- Testing_set: 测试集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from scipy import stats  # 用于序列特征的偏度和峰度计算\n",
        "import ast  # 用于安全解析数组字符串\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置中文字体支持\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"库导入完成！\")\n",
        "\n",
        "# ===== 配置参数 =====\n",
        "# 控制是否对序列特征进行特殊处理\n",
        "ENABLE_SEQUENCE_FEATURES = True  # 🔧 在这里修改: True=特征工程, False=删除序列特征\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"🔧 序列特征处理模式: {'启用 ✅' if ENABLE_SEQUENCE_FEATURES else '禁用 ❌'}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(\"📊 启用模式 - 将执行复杂的序列特征工程:\")\n",
        "    print(\"   • ip_direction → 15个统计特征 (方向模式分析)\")\n",
        "    print(\"   • pkt_len → 15个统计特征 (包大小模式分析)\")  \n",
        "    print(\"   • iat → 15个统计特征 (时间间隔模式分析)\")\n",
        "    print(\"   • 总计生成 45+ 个新特征\")\n",
        "    print(\"   💡 适合: 追求最佳性能，充分利用时序信息\")\n",
        "else:\n",
        "    print(\"🗑️ 禁用模式 - 将删除序列特征:\")\n",
        "    print(\"   • 直接删除 ip_direction, pkt_len, iat\")\n",
        "    print(\"   • 仅使用其他网络流量特征训练\")\n",
        "    print(\"   • 模型更简单，训练速度更快\")\n",
        "    print(\"   💡 适合: 资源有限，或希望简化模型的场景\")\n",
        "\n",
        "print(\"\\n💭 要切换模式，请修改上方 ENABLE_SEQUENCE_FEATURES 的值\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据加载函数\n",
        "def load_data_from_directory(base_path, label):\n",
        "    \"\"\"\n",
        "    从指定目录加载所有CSV文件并添加标签 - 增强版：确保列一致性\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(base_path, '*.csv'))\n",
        "    print(f\"在 {base_path} 中找到 {len(csv_files)} 个CSV文件\")\n",
        "    \n",
        "    dataframes = []\n",
        "    all_columns_info = []  # 收集所有文件的列信息\n",
        "    \n",
        "    # 🔍 第一遍扫描：检查所有文件的列结构\n",
        "    print(f\"\\n🔍 检查CSV文件列结构 ({os.path.basename(base_path)}):\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            # 只读取第一行来获取列名，避免加载整个文件\n",
        "            df_sample = pd.read_csv(file, nrows=1)\n",
        "            file_columns = list(df_sample.columns)\n",
        "            all_columns_info.append({\n",
        "                'file': file,\n",
        "                'columns': file_columns,\n",
        "                'count': len(file_columns)\n",
        "            })\n",
        "            \n",
        "            print(f\"  文件{i+1}: {os.path.basename(file)} -> {len(file_columns)}列\")\n",
        "            \n",
        "            # 显示前几个和后几个列名\n",
        "            if len(file_columns) <= 6:\n",
        "                print(f\"    列名: {file_columns}\")\n",
        "            else:\n",
        "                print(f\"    列名: {file_columns[:3]} ... {file_columns[-3:]}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 检查文件 {file} 时出错: {e}\")\n",
        "    \n",
        "    # 🔍 分析列数差异\n",
        "    column_counts = [info['count'] for info in all_columns_info]\n",
        "    unique_counts = sorted(set(column_counts))\n",
        "    \n",
        "    if len(unique_counts) > 1:\n",
        "        print(f\"\\n❌ 发现列数不一致问题！\")\n",
        "        for count in unique_counts:\n",
        "            files_with_count = [f\"文件{i+1}({os.path.basename(info['file'])})\" \n",
        "                              for i, info in enumerate(all_columns_info) \n",
        "                              if info['count'] == count]\n",
        "            print(f\"  📊 {count}列: {', '.join(files_with_count)}\")\n",
        "        \n",
        "        # 🔧 计算所有文件的共同列\n",
        "        if all_columns_info:\n",
        "            common_columns = set(all_columns_info[0]['columns'])\n",
        "            for info in all_columns_info[1:]:\n",
        "                common_columns = common_columns.intersection(set(info['columns']))\n",
        "            \n",
        "            common_columns_list = sorted(list(common_columns))\n",
        "            print(f\"\\n🔧 所有文件的共同列数: {len(common_columns_list)}\")\n",
        "            \n",
        "            # 显示各文件将被排除的列\n",
        "            print(\"📝 各文件独有的列:\")\n",
        "            for i, info in enumerate(all_columns_info):\n",
        "                excluded = set(info['columns']) - common_columns\n",
        "                if excluded:\n",
        "                    print(f\"  文件{i+1}: {sorted(list(excluded))}\")\n",
        "                else:\n",
        "                    print(f\"  文件{i+1}: 无独有列\")\n",
        "                    \n",
        "            print(f\"✅ 将统一使用 {len(common_columns_list)} 个共同列\")\n",
        "            target_columns = common_columns_list\n",
        "        else:\n",
        "            target_columns = None\n",
        "            print(\"❌ 无法确定共同列\")\n",
        "    else:\n",
        "        print(f\"✅ 所有文件列数一致: {column_counts[0]}列\")\n",
        "        target_columns = None\n",
        "    \n",
        "    # 📊 第二遍：使用统一列结构加载数据\n",
        "    print(f\"\\n📊 正式加载数据:\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            original_shape = df.shape\n",
        "            \n",
        "            # 如果需要，只保留共同列\n",
        "            if target_columns is not None:\n",
        "                df = df[target_columns]\n",
        "                print(f\"  🔧 文件{i+1}: {original_shape} -> {df.shape} (列对齐)\")\n",
        "            \n",
        "            df['label'] = label  # 添加标签列\n",
        "            df['source_file'] = os.path.basename(file)  # 添加源文件信息\n",
        "            dataframes.append(df)\n",
        "            print(f\"  ✅ {os.path.basename(file)}: {len(df)}行 x {len(df.columns)}列\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 加载文件 {file} 时出错: {e}\")\n",
        "    \n",
        "    # 🎯 最终合并和验证\n",
        "    if dataframes:\n",
        "        # 最后检查：确保所有dataframe列数一致\n",
        "        final_shapes = [df.shape for df in dataframes]\n",
        "        final_col_counts = [shape[1] for shape in final_shapes]\n",
        "        \n",
        "        if len(set(final_col_counts)) == 1:\n",
        "            print(f\"✅ 合并前最终检查通过: 所有文件均为 {final_col_counts[0]} 列\")\n",
        "        else:\n",
        "            print(f\"⚠️ 合并前发现列数差异: {final_col_counts}\")\n",
        "        \n",
        "        result = pd.concat(dataframes, ignore_index=True)\n",
        "        \n",
        "        print(f\"🎉 数据加载完成:\")\n",
        "        print(f\"  📁 文件数: {len(csv_files)}\")\n",
        "        print(f\"  📊 最终形状: {result.shape}\")\n",
        "        print(f\"  🏷️ 标签值: {label}\")\n",
        "        \n",
        "        # 📋 返回结果和列信息\n",
        "        columns_meta = {\n",
        "            'final_columns': list(result.columns),\n",
        "            'original_columns_info': all_columns_info,\n",
        "            'target_columns': target_columns,\n",
        "            'dataset_path': base_path\n",
        "        }\n",
        "        \n",
        "        return result, columns_meta\n",
        "    else:\n",
        "        print(\"❌ 没有成功加载任何文件\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "\n",
        "def analyze_cross_dataset_columns(datasets_info):\n",
        "    \"\"\"\n",
        "    分析不同数据集之间的列差异\n",
        "    datasets_info: 字典，格式为 {'数据集名称': columns_meta}\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🔍 跨数据集列结构一致性分析\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # 收集所有数据集的列信息\n",
        "    dataset_columns = {}\n",
        "    for dataset_name, meta in datasets_info.items():\n",
        "        if 'final_columns' in meta and meta['final_columns']:\n",
        "            # 排除我们添加的辅助列\n",
        "            original_cols = [col for col in meta['final_columns'] \n",
        "                           if col not in ['label', 'source_file']]\n",
        "            dataset_columns[dataset_name] = set(original_cols)\n",
        "            print(f\"📊 {dataset_name}: {len(original_cols)}个原始列\")\n",
        "    \n",
        "    if len(dataset_columns) < 2:\n",
        "        print(\"⚠️ 数据集数量不足，无法进行跨数据集比较\")\n",
        "        return\n",
        "    \n",
        "    # 计算所有数据集的列统计\n",
        "    all_datasets = list(dataset_columns.keys())\n",
        "    column_counts = {name: len(cols) for name, cols in dataset_columns.items()}\n",
        "    \n",
        "    print(f\"\\n📈 列数统计:\")\n",
        "    for dataset, count in column_counts.items():\n",
        "        print(f\"  {dataset}: {count}列\")\n",
        "    \n",
        "    # 检查列数是否一致\n",
        "    unique_counts = set(column_counts.values())\n",
        "    if len(unique_counts) == 1:\n",
        "        print(\"✅ 所有数据集列数一致\")\n",
        "    else:\n",
        "        print(f\"❌ 发现列数不一致: {sorted(unique_counts)}\")\n",
        "    \n",
        "    # 计算所有数据集的共同列和独有列\n",
        "    all_columns = set()\n",
        "    for cols in dataset_columns.values():\n",
        "        all_columns = all_columns.union(cols)\n",
        "    \n",
        "    common_columns = set.intersection(*dataset_columns.values()) if dataset_columns else set()\n",
        "    \n",
        "    print(f\"\\n🔧 列分析结果:\")\n",
        "    print(f\"  所有唯一列数: {len(all_columns)}\")\n",
        "    print(f\"  共同列数: {len(common_columns)}\")\n",
        "    print(f\"  差异列数: {len(all_columns) - len(common_columns)}\")\n",
        "    \n",
        "    # 详细分析每个数据集的独有列\n",
        "    print(f\"\\n📝 各数据集独有列分析:\")\n",
        "    for dataset, cols in dataset_columns.items():\n",
        "        unique_to_dataset = cols - common_columns\n",
        "        shared_with_others = cols - unique_to_dataset\n",
        "        \n",
        "        print(f\"\\n  🔍 {dataset}:\")\n",
        "        print(f\"    总列数: {len(cols)}\")\n",
        "        print(f\"    与其他数据集共同的列: {len(shared_with_others)}\")\n",
        "        print(f\"    独有列数: {len(unique_to_dataset)}\")\n",
        "        \n",
        "        if unique_to_dataset:\n",
        "            print(f\"    独有列名: {sorted(list(unique_to_dataset))}\")\n",
        "        \n",
        "        # 找出该数据集缺失但其他数据集有的列\n",
        "        missing_columns = common_columns - cols\n",
        "        if missing_columns:\n",
        "            print(f\"    缺失的共同列: {sorted(list(missing_columns))}\")\n",
        "    \n",
        "    # 生成两两对比\n",
        "    print(f\"\\n🔄 两两数据集对比:\")\n",
        "    datasets_list = list(dataset_columns.keys())\n",
        "    for i in range(len(datasets_list)):\n",
        "        for j in range(i+1, len(datasets_list)):\n",
        "            dataset1, dataset2 = datasets_list[i], datasets_list[j]\n",
        "            cols1, cols2 = dataset_columns[dataset1], dataset_columns[dataset2]\n",
        "            \n",
        "            common = cols1.intersection(cols2)\n",
        "            only_in_1 = cols1 - cols2\n",
        "            only_in_2 = cols2 - cols1\n",
        "            \n",
        "            print(f\"\\n  📊 {dataset1} vs {dataset2}:\")\n",
        "            print(f\"    共同列: {len(common)}\")\n",
        "            print(f\"    仅{dataset1}有: {len(only_in_1)}\")\n",
        "            print(f\"    仅{dataset2}有: {len(only_in_2)}\")\n",
        "            \n",
        "            if only_in_1:\n",
        "                print(f\"    仅{dataset1}有的列: {sorted(list(only_in_1))}\")\n",
        "            if only_in_2:\n",
        "                print(f\"    仅{dataset2}有的列: {sorted(list(only_in_2))}\")\n",
        "    \n",
        "    # 推荐的统一策略\n",
        "    print(f\"\\n💡 统一策略建议:\")\n",
        "    if len(common_columns) == len(all_columns):\n",
        "        print(\"✅ 所有数据集列完全一致，无需调整\")\n",
        "    else:\n",
        "        print(f\"🔧 建议统一到共同列集合 ({len(common_columns)}列)\")\n",
        "        print(f\"   这将确保所有数据集具有相同的特征结构\")\n",
        "        \n",
        "        lost_columns = all_columns - common_columns\n",
        "        if lost_columns:\n",
        "            print(f\"   ⚠️ 将丢失的列: {sorted(list(lost_columns))}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "\n",
        "# 使用相对路径定义数据目录\n",
        "# 数据集应该与此notebook在同一目录下\n",
        "data_dir = './pcdn_32_pkts_2class_feature_enhance_v17.4_dataset'\n",
        "\n",
        "# 检查数据目录是否存在\n",
        "if not os.path.exists(data_dir):\n",
        "    print(f\"❌ 数据目录不存在: {data_dir}\")\n",
        "    print(\"请确保数据集文件夹与notebook在同一目录下\")\n",
        "    print(\"当前工作目录:\", os.getcwd())\n",
        "    print(\"当前目录内容:\", [f for f in os.listdir('.') if not f.startswith('.')])\n",
        "    \n",
        "    # 尝试查找数据目录\n",
        "    possible_dirs = [d for d in os.listdir('.') if 'pcdn' in d.lower() and os.path.isdir(d)]\n",
        "    if possible_dirs:\n",
        "        print(f\"发现可能的数据目录: {possible_dirs}\")\n",
        "        data_dir = possible_dirs[0]\n",
        "        print(f\"使用数据目录: {data_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"找不到数据目录，请检查数据集位置\")\n",
        "else:\n",
        "    print(f\"✅ 找到数据目录: {data_dir}\")\n",
        "\n",
        "# 加载训练数据\n",
        "print(\"\\n开始加载训练数据...\")\n",
        "train_normal = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)  # 正常流量标签为0\n",
        "train_pcdn = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)    # PCDN流量标签为1\n",
        "\n",
        "# 合并训练数据\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "print(f\"\\n训练数据加载完成！\")\n",
        "print(f\"正常流量样本数: {len(train_normal)}\")\n",
        "print(f\"PCDN流量样本数: {len(train_pcdn)}\")\n",
        "print(f\"总训练样本数: {len(train_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 数据加载阶段的根本问题发现\n",
        "\n",
        "### 🚨 问题分析\n",
        "您的分析完全正确！**\"训练集47个特征，测试集48个特征\"问题的根源很可能在数据加载阶段**。\n",
        "\n",
        "### 🔍 潜在原因\n",
        "当使用 `pd.concat()` 合并多个CSV文件时，如果不同CSV文件的列数不一致：\n",
        "\n",
        "```python\n",
        "# 原来的问题代码\n",
        "dataframes = []\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)  # 不同file可能有不同列数！\n",
        "    dataframes.append(df)\n",
        "return pd.concat(dataframes, ignore_index=True)  # 会取所有列的并集！\n",
        "```\n",
        "\n",
        "**具体场景：**\n",
        "- 📁 训练集CSV文件：都有47个原始列\n",
        "- 📁 验证集CSV文件：都有47个原始列  \n",
        "- 📁 测试集CSV文件：某个文件有48个列（多了1个额外列）\n",
        "\n",
        "**结果：**\n",
        "- 🔧 训练集合并后：47个特征 + NaN填充\n",
        "- 🔧 验证集合并后：47个特征 + NaN填充\n",
        "- 🔧 测试集合并后：48个特征（47个共同 + 1个独有）\n",
        "\n",
        "### ✅ 解决方案\n",
        "**新的数据加载函数**实现了：\n",
        "\n",
        "#### 🔍 两阶段检查\n",
        "1. **第一阶段**：快速扫描所有CSV文件的列结构（只读第1行）\n",
        "2. **第二阶段**：使用统一的列集合加载所有数据\n",
        "\n",
        "#### 📊 详细诊断\n",
        "- 显示每个文件的确切列数\n",
        "- 标识哪些文件有额外的列\n",
        "- 计算所有文件的**共同列集**\n",
        "\n",
        "#### 🔧 自动修复\n",
        "- 自动对齐到共同列集\n",
        "- 排除各文件的独有列\n",
        "- 确保所有数据集具有相同的列结构\n",
        "\n",
        "#### 💡 运行效果\n",
        "```\n",
        "🔍 检查CSV文件列结构 (Testing_set):\n",
        "  文件1: test1.csv -> 47列\n",
        "  文件2: test2.csv -> 48列  ← 发现问题！\n",
        "  文件3: test3.csv -> 47列\n",
        "\n",
        "❌ 发现列数不一致问题！\n",
        "  📊 47列: 文件1, 文件3\n",
        "  📊 48列: 文件2\n",
        "\n",
        "🔧 所有文件的共同列数: 47\n",
        "📝 各文件独有的列:\n",
        "  文件1: 无独有列\n",
        "  文件2: ['额外的列名']  ← 找到罪魁祸首！\n",
        "  文件3: 无独有列\n",
        "\n",
        "✅ 将统一使用 47 个共同列\n",
        "```\n",
        "\n",
        "这样就从根源上解决了特征维度不一致的问题！🎯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载验证和测试数据\n",
        "print(\"开始加载验证数据...\")\n",
        "val_normal = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查验证数据是否为空\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"⚠️ 警告: 验证集为空，将使用训练集的一部分作为验证集\")\n",
        "    val_data = pd.DataFrame()\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "\n",
        "print(\"\\n开始加载测试数据...\")\n",
        "test_normal = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查测试数据是否为空\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"⚠️ 警告: 测试集为空，将使用训练集的一部分作为测试集\")\n",
        "    test_data = pd.DataFrame()\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "\n",
        "print(f\"\\n验证集样本数: {len(val_data)} (正常: {len(val_normal)}, PCDN: {len(val_pcdn)})\")\n",
        "print(f\"测试集样本数: {len(test_data)} (正常: {len(test_normal)}, PCDN: {len(test_pcdn)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 增强版数据加载：支持跨数据集列分析\n",
        "print(\"🚀 开始增强版数据加载...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 收集所有数据集的列信息\n",
        "datasets_meta = {}\n",
        "\n",
        "# 加载训练数据\n",
        "print(\"\\n📂 加载训练数据...\")\n",
        "train_normal, train_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)\n",
        "train_pcdn, train_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)\n",
        "\n",
        "# 合并训练数据并记录元信息\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "datasets_meta['Training_set'] = {\n",
        "    'final_columns': list(train_data.columns),\n",
        "    'dataset_path': 'Training_set',\n",
        "    'shape': train_data.shape,\n",
        "    'normal_meta': train_normal_meta,\n",
        "    'pcdn_meta': train_pcdn_meta\n",
        "}\n",
        "\n",
        "print(f\"✅ 训练数据: {len(train_normal)}个正常 + {len(train_pcdn)}个PCDN = {len(train_data)}总样本\")\n",
        "\n",
        "# 加载验证数据\n",
        "print(\"\\n📂 加载验证数据...\")\n",
        "val_normal, val_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn, val_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查验证数据是否为空\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"⚠️ 验证集为空，将使用训练集的一部分作为验证集\")\n",
        "    val_data = pd.DataFrame()\n",
        "    datasets_meta['Validation_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "    datasets_meta['Validation_set'] = {\n",
        "        'final_columns': list(val_data.columns),\n",
        "        'dataset_path': 'Validation_set',\n",
        "        'shape': val_data.shape,\n",
        "        'normal_meta': val_normal_meta,\n",
        "        'pcdn_meta': val_pcdn_meta\n",
        "    }\n",
        "    print(f\"✅ 验证数据: {len(val_normal)}个正常 + {len(val_pcdn)}个PCDN = {len(val_data)}总样本\")\n",
        "\n",
        "# 加载测试数据\n",
        "print(\"\\n📂 加载测试数据...\")\n",
        "test_normal, test_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn, test_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查测试数据是否为空\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"⚠️ 测试集为空，将使用训练集的一部分作为测试集\")\n",
        "    test_data = pd.DataFrame()\n",
        "    datasets_meta['Testing_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "    datasets_meta['Testing_set'] = {\n",
        "        'final_columns': list(test_data.columns),\n",
        "        'dataset_path': 'Testing_set', \n",
        "        'shape': test_data.shape,\n",
        "        'normal_meta': test_normal_meta,\n",
        "        'pcdn_meta': test_pcdn_meta\n",
        "    }\n",
        "    print(f\"✅ 测试数据: {len(test_normal)}个正常 + {len(test_pcdn)}个PCDN = {len(test_data)}总样本\")\n",
        "\n",
        "# 📊 执行跨数据集列结构分析\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🔍 开始跨数据集列结构分析...\")\n",
        "analyze_cross_dataset_columns(datasets_meta)\n",
        "\n",
        "# 📋 数据加载总结\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📋 数据加载总结\")\n",
        "print(\"=\"*80)\n",
        "for dataset_name, meta in datasets_meta.items():\n",
        "    if 'status' in meta and meta['status'] == 'empty':\n",
        "        print(f\"📂 {dataset_name}: 空数据集\")\n",
        "    else:\n",
        "        print(f\"📂 {dataset_name}: {meta['shape'][0]}行 x {meta['shape'][1]}列\")\n",
        "\n",
        "print(\"🎉 数据加载和分析完成！\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 增强版数据加载：完整的列一致性诊断\n",
        "\n",
        "### 🎯 功能增强\n",
        "现在的数据加载系统实现了**四级列一致性检查**：\n",
        "\n",
        "#### 📊 第一级：单个CSV文件内检查\n",
        "```\n",
        "🔍 检查CSV文件列结构 (APP_0):\n",
        "  文件1: file1.csv -> 47列\n",
        "  文件2: file2.csv -> 48列  ← 发现异常！\n",
        "  文件3: file3.csv -> 47列\n",
        "```\n",
        "\n",
        "#### 📊 第二级：数据集内统一\n",
        "```\n",
        "❌ 发现列数不一致问题！\n",
        "  📊 47列: 文件1, 文件3\n",
        "  📊 48列: 文件2\n",
        "\n",
        "🔧 将统一使用 47 个共同列\n",
        "```\n",
        "\n",
        "#### 📊 第三级：跨数据集比较\n",
        "```\n",
        "🔍 跨数据集列结构一致性分析\n",
        "📊 Training_set: 47个原始列\n",
        "📊 Validation_set: 47个原始列\n",
        "📊 Testing_set: 48个原始列  ← 找到问题根源！\n",
        "\n",
        "❌ 发现列数不一致: [47, 48]\n",
        "```\n",
        "\n",
        "#### 📊 第四级：详细差异分析\n",
        "- **各数据集独有列分析**：精确显示哪些列是每个数据集独有的\n",
        "- **两两对比**：详细比较任意两个数据集的列差异\n",
        "- **统一策略建议**：自动推荐最佳的列对齐方案\n",
        "\n",
        "### 🔍 诊断输出示例\n",
        "```\n",
        "📝 各数据集独有列分析:\n",
        "  🔍 Training_set:\n",
        "    独有列数: 0\n",
        "  🔍 Testing_set:\n",
        "    独有列数: 1\n",
        "    独有列名: ['mysterious_extra_column']\n",
        "\n",
        "🔄 两两数据集对比:\n",
        "  📊 Training_set vs Testing_set:\n",
        "    仅Testing_set有: ['mysterious_extra_column']\n",
        "\n",
        "💡 统一策略建议:\n",
        "🔧 建议统一到共同列集合 (47列)\n",
        "   ⚠️ 将丢失的列: ['mysterious_extra_column']\n",
        "```\n",
        "\n",
        "### ✅ 问题解决保障\n",
        "- ✅ **精确定位**：准确找到哪个CSV文件、哪个数据集有额外列\n",
        "- ✅ **自动修复**：自动对齐到共同列集合\n",
        "- ✅ **零遗漏**：确保不会再出现维度不一致错误\n",
        "- ✅ **完整记录**：详细记录每个处理步骤\n",
        "\n",
        "现在您可以准确知道\"为什么测试集比训练集多1个特征\"的具体原因了！🎯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据探索和基本信息\n",
        "print(\"=== 训练数据基本信息 ===\")\n",
        "print(f\"数据形状: {train_data.shape}\")\n",
        "print(f\"\\n列名 ({len(train_data.columns)}个特征):\")\n",
        "print(train_data.columns.tolist())\n",
        "\n",
        "print(\"\\n=== 标签分布 ===\")\n",
        "label_counts = train_data['label'].value_counts()\n",
        "print(label_counts)\n",
        "print(f\"正常流量比例: {label_counts[0]/len(train_data)*100:.2f}%\")\n",
        "print(f\"PCDN流量比例: {label_counts[1]/len(train_data)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== 数据类型信息 ===\")\n",
        "print(train_data.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据预处理\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    数据预处理函数\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # 删除指定的不需要的列\n",
        "    columns_to_drop = [\n",
        "        'source_file',  # 源文件信息（添加的辅助列）\n",
        "        'frame.number', # 帧编号\n",
        "        'frame.time_relative', # 相对时间\n",
        "        'ip.version',   # IP版本\n",
        "        'ip.ttl',       # IP TTL\n",
        "        'ip.src', 'ip.dst',  # IP地址\n",
        "        'ipv6.plen',    # IPv6 payload长度\n",
        "        'ipv6.nxt',     # IPv6 下一个头\n",
        "        'ipv6.src', 'ipv6.dst',  # IPv6地址\n",
        "        '_ws.col.Protocol', # Wireshark协议列\n",
        "        'ssl.handshake.extensions_server_name',  # SSL扩展信息\n",
        "        'eth.src',      # MAC地址\n",
        "        'pcap_duration', # PCAP持续时间\n",
        "        'app',          # 应用程序\n",
        "        'os',           # 操作系统\n",
        "        'date',         # 日期\n",
        "        'flow_id',      # 流ID\n",
        "        'dpi_file_name', # DPI文件名\n",
        "        'dpi_five_tuple', # 五元组\n",
        "        'dpi_rule_result', # DPI规则结果\n",
        "        'dpi_label',    # DPI标签\n",
        "        'ulProtoID',    # 上层协议ID\n",
        "        'dpi_rule_pkt', # DPI规则包\n",
        "        'dpi_packets',  # DPI包数\n",
        "        'dpi_bytes',    # DPI字节数\n",
        "        'label_source', # 标签源\n",
        "        'id',           # ID字段\n",
        "        'category'      # category字段\n",
        "    ]\n",
        "    \n",
        "    # 删除存在的列\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
        "    \n",
        "    # 保留所有其他特征（包括数组特征），这些可能对分类有用\n",
        "    # 数组特征如 ip_direction, pkt_len, iat 等将在后续步骤中进行编码处理\n",
        "    \n",
        "    # 处理缺失值\n",
        "    df_processed = df_processed.fillna(0)\n",
        "    \n",
        "    # 处理无穷大值\n",
        "    df_processed = df_processed.replace([np.inf, -np.inf], 0)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# 预处理训练数据\n",
        "train_processed = preprocess_data(train_data)\n",
        "\n",
        "# 智能处理空数据集的情况\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "val_exists = len(val_data) > 0\n",
        "test_exists = len(test_data) > 0\n",
        "\n",
        "print(f\"验证集存在: {'是' if val_exists else '否'}\")\n",
        "print(f\"测试集存在: {'是' if test_exists else '否'}\")\n",
        "\n",
        "if not val_exists and not test_exists:\n",
        "    # 两个数据集都为空，进行60/20/20分割\n",
        "    print(\"验证集和测试集都为空，从训练数据分割为 60% 训练 / 20% 验证 / 20% 测试\")\n",
        "    train_temp, temp_split = train_test_split(\n",
        "        train_processed, test_size=0.4, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    val_processed, test_processed = train_test_split(\n",
        "        temp_split, test_size=0.5, random_state=42, \n",
        "        stratify=temp_split['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    \n",
        "elif not val_exists:\n",
        "    # 只有验证集为空，分割80/20\n",
        "    print(\"验证集为空，从训练数据分割为 80% 训练 / 20% 验证\")\n",
        "    train_temp, val_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    test_processed = preprocess_data(test_data)\n",
        "    \n",
        "elif not test_exists:\n",
        "    # 只有测试集为空，分割80/20\n",
        "    print(\"测试集为空，从训练数据分割为 80% 训练 / 20% 测试\")\n",
        "    train_temp, test_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    \n",
        "else:\n",
        "    # 两个数据集都存在，直接使用\n",
        "    print(\"使用原始的验证集和测试集\")\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    test_processed = preprocess_data(test_data)\n",
        "\n",
        "print(f\"预处理后训练数据形状: {train_processed.shape}\")\n",
        "print(f\"预处理后验证数据形状: {val_processed.shape}\")\n",
        "print(f\"预处理后测试数据形状: {test_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征工程和数据预处理\n",
        "# 提取特征和标签\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"预处理后特征数量: {len(feature_columns)}\")\n",
        "print(f\"特征列表前10个: {feature_columns[:10]}\")\n",
        "\n",
        "# 处理序列特征 - 这些字段包含网络包的时序信息\n",
        "def process_sequence_features(df, enable_sequence_processing=True):\n",
        "    \"\"\"\n",
        "    处理序列类型的特征 (pkt_len, ip_direction, iat)\n",
        "    \n",
        "    参数:\n",
        "        df: 输入数据框\n",
        "        enable_sequence_processing: 是否启用序列特征处理\n",
        "            - True: 提取统计特征(均值、方差、分位数等)\n",
        "            - False: 直接删除序列特征\n",
        "    \n",
        "    对于XGBoost这种基于树的算法，需要将序列数据转换为有意义的统计特征：\n",
        "    1. pkt_len: 包长度序列 - 反映流量的数据传输模式\n",
        "    2. ip_direction: IP方向序列 - 反映通信的方向模式  \n",
        "    3. iat: 包间到达时间间隔序列 - 反映流量的时间特征\n",
        "    \n",
        "    XGBoost无法直接处理变长序列，需要提取固定维度的特征\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    # 定义序列特征及其含义\n",
        "    sequence_columns = {\n",
        "        'ip_direction': '网络包方向序列 (0=出站, 1=入站)',\n",
        "        'pkt_len': '网络包长度序列',\n",
        "        'iat': '包间到达时间间隔序列'\n",
        "    }\n",
        "    \n",
        "    if not enable_sequence_processing:\n",
        "        # 如果禁用序列特征处理，直接删除这些列\n",
        "        print(\"🗑️ 删除序列特征模式:\")\n",
        "        for col in sequence_columns.keys():\n",
        "            if col in df_copy.columns:\n",
        "                df_copy = df_copy.drop(columns=[col])\n",
        "                print(f\"  ✗ 已删除: {col}\")\n",
        "        return df_copy\n",
        "    \n",
        "    for col, description in sequence_columns.items():\n",
        "        if col in df_copy.columns:\n",
        "            print(f\"处理序列特征: {col} - {description}\")\n",
        "            \n",
        "            try:\n",
        "                # 安全地将字符串转换为数值列表（避免使用eval）\n",
        "                def safe_parse_array(x):\n",
        "                    \"\"\"安全解析数组字符串\"\"\"\n",
        "                    if pd.isna(x) or x == '' or x == '[]':\n",
        "                        return []\n",
        "                    if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "                        try:\n",
        "                            # 使用ast.literal_eval替代eval，更安全\n",
        "                            return ast.literal_eval(x)\n",
        "                        except (ValueError, SyntaxError):\n",
        "                            return []\n",
        "                    return []\n",
        "                \n",
        "                sequences = df_copy[col].apply(safe_parse_array)\n",
        "                \n",
        "                # === 基础统计特征 ===\n",
        "                df_copy[f'{col}_mean'] = sequences.apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_std'] = sequences.apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_min'] = sequences.apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_max'] = sequences.apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_median'] = sequences.apply(lambda x: np.median(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_range'] = sequences.apply(lambda x: (np.max(x) - np.min(x)) if len(x) > 0 else 0)\n",
        "                \n",
        "                # === 分位数特征 ===\n",
        "                df_copy[f'{col}_q25'] = sequences.apply(lambda x: np.percentile(x, 25) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_q75'] = sequences.apply(lambda x: np.percentile(x, 75) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_iqr'] = df_copy[f'{col}_q75'] - df_copy[f'{col}_q25']\n",
        "                \n",
        "                # === 序列长度特征 ===\n",
        "                df_copy[f'{col}_len'] = sequences.apply(lambda x: len(x))\n",
        "                \n",
        "                # === 序列模式特征 ===\n",
        "                # 变异系数 (标准差/均值) - 衡量序列的相对变化程度\n",
        "                df_copy[f'{col}_cv'] = sequences.apply(lambda x: np.std(x)/np.mean(x) if len(x) > 0 and np.mean(x) != 0 else 0)\n",
        "                \n",
        "                # 偏度和峰度 - 衡量序列分布形状\n",
        "                df_copy[f'{col}_skew'] = sequences.apply(lambda x: stats.skew(x) if len(x) > 1 else 0)\n",
        "                df_copy[f'{col}_kurtosis'] = sequences.apply(lambda x: stats.kurtosis(x) if len(x) > 1 else 0)\n",
        "                \n",
        "                # === 序列特有的特征 ===\n",
        "                if col == 'ip_direction':\n",
        "                    # 对于方向序列：统计出站/入站比例\n",
        "                    df_copy[f'{col}_out_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 0])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_in_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 1])/len(x) if len(x) > 0 else 0)\n",
        "                    # 方向变化次数 - 反映通信模式\n",
        "                    df_copy[f'{col}_changes'] = sequences.apply(lambda x: sum([1 for i in range(1, len(x)) if x[i] != x[i-1]]) if len(x) > 1 else 0)\n",
        "                \n",
        "                elif col == 'pkt_len':\n",
        "                    # 对于包长度序列：小包/大包比例\n",
        "                    df_copy[f'{col}_small_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i <= 64])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_large_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i >= 1400])/len(x) if len(x) > 0 else 0)\n",
        "                \n",
        "                elif col == 'iat':\n",
        "                    # 对于时间间隔序列：突发性检测\n",
        "                    df_copy[f'{col}_burst_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i < 0.01])/len(x) if len(x) > 0 else 0)  # 小于10ms的比例\n",
        "                    df_copy[f'{col}_long_gap_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i > 1.0])/len(x) if len(x) > 0 else 0)  # 大于1s的比例\n",
        "                \n",
        "                # === 趋势特征 ===\n",
        "                # 序列递增/递减趋势\n",
        "                def trend_analysis(seq):\n",
        "                    if len(seq) < 2:\n",
        "                        return 0, 0\n",
        "                    increasing = sum([1 for i in range(1, len(seq)) if seq[i] > seq[i-1]])\n",
        "                    decreasing = sum([1 for i in range(1, len(seq)) if seq[i] < seq[i-1]])\n",
        "                    return increasing/len(seq), decreasing/len(seq)\n",
        "                \n",
        "                trends = sequences.apply(trend_analysis)\n",
        "                df_copy[f'{col}_increasing_ratio'] = trends.apply(lambda x: x[0])\n",
        "                df_copy[f'{col}_decreasing_ratio'] = trends.apply(lambda x: x[1])\n",
        "                \n",
        "                # 删除原始序列列\n",
        "                df_copy = df_copy.drop(columns=[col])\n",
        "                print(f\"  -> 已从 {col} 提取 {len([c for c in df_copy.columns if c.startswith(col)])} 个特征\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  -> 处理 {col} 时出错，将直接编码: {e}\")\n",
        "                # 如果处理失败，就简单编码\n",
        "                df_copy[col] = LabelEncoder().fit_transform(df_copy[col].astype(str))\n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "# 处理所有数据集的序列特征\n",
        "print(\"开始处理序列特征...\")\n",
        "print(\"=\" * 60)\n",
        "train_processed = process_sequence_features(train_processed, enable_sequence_processing=ENABLE_SEQUENCE_FEATURES)\n",
        "val_processed = process_sequence_features(val_processed, enable_sequence_processing=ENABLE_SEQUENCE_FEATURES)\n",
        "test_processed = process_sequence_features(test_processed, enable_sequence_processing=ENABLE_SEQUENCE_FEATURES)\n",
        "\n",
        "# 更新特征列表\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"\\n处理序列特征后的特征数量: {len(feature_columns)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 检查剩余的非数值列\n",
        "non_numeric_cols = []\n",
        "for col in feature_columns:\n",
        "    if not pd.api.types.is_numeric_dtype(train_processed[col]):\n",
        "        non_numeric_cols.append(col)\n",
        "\n",
        "print(f\"\\n需要编码的非数值列数量: {len(non_numeric_cols)}\")\n",
        "if non_numeric_cols:\n",
        "    print(f\"非数值列: {non_numeric_cols[:5]}...\")  # 显示前5个\n",
        "\n",
        "# 对非数值列进行标签编码\n",
        "if non_numeric_cols:\n",
        "    print(\"开始对非数值列进行标签编码...\")\n",
        "    for col in non_numeric_cols:\n",
        "        try:\n",
        "            le = LabelEncoder()\n",
        "            # 合并所有数据集的该列值进行编码\n",
        "            all_values = pd.concat([\n",
        "                train_processed[col].fillna('missing').astype(str),\n",
        "                val_processed[col].fillna('missing').astype(str),\n",
        "                test_processed[col].fillna('missing').astype(str)\n",
        "            ])\n",
        "            le.fit(all_values)\n",
        "            \n",
        "            train_processed[col] = le.transform(train_processed[col].fillna('missing').astype(str))\n",
        "            val_processed[col] = le.transform(val_processed[col].fillna('missing').astype(str))\n",
        "            test_processed[col] = le.transform(test_processed[col].fillna('missing').astype(str))\n",
        "            print(f\"  ✓ 已编码: {col}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ 编码失败 {col}: {e}\")\n",
        "            # 编码失败的列直接删除\n",
        "            if col in train_processed.columns:\n",
        "                train_processed = train_processed.drop(columns=[col])\n",
        "                val_processed = val_processed.drop(columns=[col])\n",
        "                test_processed = test_processed.drop(columns=[col])\n",
        "\n",
        "# 确保所有数据集具有相同的特征列\n",
        "print(\"\\n🔧 检查数据集特征一致性...\")\n",
        "train_features = set(train_processed.columns) - {'label'}\n",
        "val_features = set(val_processed.columns) - {'label'}\n",
        "test_features = set(test_processed.columns) - {'label'}\n",
        "\n",
        "print(f\"训练集特征数: {len(train_features)}\")\n",
        "print(f\"验证集特征数: {len(val_features)}\")\n",
        "print(f\"测试集特征数: {len(test_features)}\")\n",
        "\n",
        "# 取三个数据集的特征交集，确保一致性\n",
        "common_features = train_features.intersection(val_features).intersection(test_features)\n",
        "print(f\"共同特征数: {len(common_features)}\")\n",
        "\n",
        "if len(common_features) < len(train_features):\n",
        "    print(\"⚠️ 警告: 数据集间特征不一致，使用共同特征\")\n",
        "    # 只保留共同特征\n",
        "    feature_cols_to_keep = list(common_features) + ['label']\n",
        "    train_processed = train_processed[feature_cols_to_keep]\n",
        "    val_processed = val_processed[feature_cols_to_keep]\n",
        "    test_processed = test_processed[feature_cols_to_keep]\n",
        "\n",
        "print(\"\\n数据预处理完成！\")\n",
        "print(f\"最终特征数量: {len(common_features)}\")\n",
        "print(\"✅ 所有数据集特征已对齐\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 序列特征处理策略说明\n",
        "\n",
        "### 为什么XGBoost需要特殊处理序列特征？\n",
        "\n",
        "**XGBoost的限制：**\n",
        "- XGBoost是基于树的算法，只能处理固定维度的表格数据\n",
        "- 无法直接处理变长序列（如[1,0,1,1,0]这样的数组）\n",
        "- 需要将序列转换为固定数量的数值特征\n",
        "\n",
        "### 📊 我们提取的序列特征类型\n",
        "\n",
        "**1. 基础统计特征**\n",
        "- 均值、标准差、最大/最小值、中位数、四分位数\n",
        "- 这些特征捕获序列的整体分布特性\n",
        "\n",
        "**2. 形状特征**\n",
        "- 偏度(skewness)：序列分布的对称性\n",
        "- 峰度(kurtosis)：序列分布的尖锐程度\n",
        "- 变异系数：相对变化程度\n",
        "\n",
        "**3. 序列模式特征**\n",
        "- **方向序列(ip_direction)**：出站/入站比例、方向变化次数\n",
        "- **包长度序列(pkt_len)**：小包/大包比例\n",
        "- **时间间隔序列(iat)**：突发传输/长间隔比例\n",
        "\n",
        "**4. 趋势特征**\n",
        "- 递增/递减趋势：反映序列的时间演化模式\n",
        "\n",
        "### 🎯 这些特征对PCDN检测的意义\n",
        "\n",
        "**正常流量 vs PCDN流量的区别：**\n",
        "- **包大小模式**：PCDN可能有特定的分块传输模式\n",
        "- **方向模式**：PCDN的上传/下载比例可能不同\n",
        "- **时间模式**：PCDN的传输节奏可能更规律或更突发\n",
        "- **序列长度**：PCDN会话可能有特定的包数量模式\n",
        "\n",
        "### ⚙️ 序列特征控制参数\n",
        "\n",
        "通过修改 `ENABLE_SEQUENCE_FEATURES` 参数可以控制序列特征的处理方式：\n",
        "\n",
        "- **`ENABLE_SEQUENCE_FEATURES = True`** (默认)\n",
        "  - 对 `ip_direction`, `pkt_len`, `iat` 进行复杂的统计特征提取\n",
        "  - 生成45+个新特征，充分利用时序信息\n",
        "  - 适合对模型性能要求较高的场景\n",
        "\n",
        "- **`ENABLE_SEQUENCE_FEATURES = False`**\n",
        "  - 直接删除这3个序列特征\n",
        "  - 模型仅使用其他网络特征进行训练\n",
        "  - 适合计算资源有限或希望简化模型的场景\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 序列特征处理效果展示\n",
        "print(\"🔍 序列特征处理效果分析\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    # 统计每个序列特征生成了多少个新特征\n",
        "    sequence_feature_counts = {}\n",
        "    for original_col in ['ip_direction', 'pkt_len', 'iat']:\n",
        "        derived_features = [col for col in train_processed.columns if col.startswith(original_col)]\n",
        "        sequence_feature_counts[original_col] = len(derived_features)\n",
        "        print(f\"{original_col:15} -> 生成了 {len(derived_features):2d} 个特征\")\n",
        "        if derived_features:\n",
        "            print(f\"                   包括: {', '.join(derived_features[:5])}{'...' if len(derived_features) > 5 else ''}\")\n",
        "\n",
        "    total_sequence_features = sum(sequence_feature_counts.values())\n",
        "    print(f\"\\n📈 总共从3个序列特征生成了 {total_sequence_features} 个数值特征\")\n",
        "\n",
        "    # 展示一些关键特征的含义\n",
        "    print(f\"\\n📋 关键特征含义示例:\")\n",
        "    feature_meanings = {\n",
        "        'pkt_len_mean': '平均包大小 - 反映传输数据的粒度',\n",
        "        'pkt_len_cv': '包大小变异系数 - 反映传输的规律性',\n",
        "        'ip_direction_changes': '方向变化次数 - 反映交互模式',\n",
        "        'iat_burst_ratio': '突发传输比例 - 反映时间模式',\n",
        "        'pkt_len_small_pkt_ratio': '小包比例 - 反映协议特征'\n",
        "    }\n",
        "\n",
        "    for feat, meaning in feature_meanings.items():\n",
        "        if feat in train_processed.columns:\n",
        "            print(f\"  {feat:25}: {meaning}\")\n",
        "\n",
        "    print(\"\\n✨ 这些特征将帮助XGBoost学习正常流量和PCDN流量的行为差异模式\")\n",
        "else:\n",
        "    print(\"🗑️ 序列特征已被删除\")\n",
        "    print(\"- ip_direction, pkt_len, iat 三个特征已从数据集中移除\")\n",
        "    print(\"- 模型将仅使用其他网络流量特征进行训练\")\n",
        "    print(\"- 这可能会影响模型对流量时序模式的学习能力\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据分布可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 标签分布\n",
        "label_counts = train_processed['label'].value_counts()\n",
        "axes[0, 0].pie(label_counts.values, labels=['Normal Traffic', 'PCDN Traffic'], autopct='%1.1f%%')\n",
        "axes[0, 0].set_title('Training Data Label Distribution')\n",
        "\n",
        "# 选择几个重要的数值特征进行可视化\n",
        "numeric_features = ['ip.len', 'tcp.srcport', 'tcp.dstport', 'sum_pkt_len', 'total_pkts']\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    # 如果启用序列特征，添加序列特征进行可视化\n",
        "    sequence_features = ['pkt_len_mean', 'ip_direction_changes', 'iat_mean']\n",
        "    all_viz_features = numeric_features + sequence_features\n",
        "    print(\"包含序列特征的可视化\")\n",
        "else:\n",
        "    # 如果禁用序列特征，只使用基础特征\n",
        "    all_viz_features = numeric_features\n",
        "    print(\"仅使用基础特征的可视化\")\n",
        "\n",
        "# 检查可用性\n",
        "available_features = [f for f in all_viz_features if f in train_processed.columns]\n",
        "print(f\"可用于可视化的特征: {available_features}\")\n",
        "\n",
        "if len(available_features) >= 3:\n",
        "    # 特征分布对比\n",
        "    for i, feature in enumerate(available_features[:3]):\n",
        "        if i == 0:\n",
        "            ax = axes[0, 1]\n",
        "        elif i == 1:\n",
        "            ax = axes[1, 0]\n",
        "        else:\n",
        "            ax = axes[1, 1]\n",
        "        \n",
        "        normal_data = train_processed[train_processed['label'] == 0][feature]\n",
        "        pcdn_data = train_processed[train_processed['label'] == 1][feature]\n",
        "        \n",
        "        ax.hist(normal_data, alpha=0.7, label='Normal', bins=30, density=True)\n",
        "        ax.hist(pcdn_data, alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "        ax.set_title(f'Distribution of {feature}')\n",
        "        ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"数据分布可视化完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 序列特征处理总结\n",
        "\n",
        "### 🔄 完整处理流程\n",
        "\n",
        "**原始序列特征 → 提取统计特征 → XGBoost训练**\n",
        "\n",
        "1. **原始数据格式**：\n",
        "   - `pkt_len`: `\"[40, 40, 1432, 712, ...]\"` (包长度序列)\n",
        "   - `ip_direction`: `\"[0, 0, 1, 1, 0, ...]\"` (方向序列)  \n",
        "   - `iat`: `\"[0.0, 0.016, 0.083, ...]\"` (时间间隔序列)\n",
        "\n",
        "2. **特征提取策略**：\n",
        "   - **统计特征**：均值、方差、分位数等 (适用于所有序列)\n",
        "   - **领域特征**：根据序列含义设计的专门特征\n",
        "   - **模式特征**：变化趋势、突发性等时序特征\n",
        "\n",
        "3. **XGBoost优势**：\n",
        "   - 可以自动发现特征之间的复杂组合\n",
        "   - 通过树结构捕获非线性模式\n",
        "   - 特征重要性分析帮助理解哪些序列模式最重要\n",
        "\n",
        "### 📈 预期效果\n",
        "\n",
        "通过这种处理方式，我们将**3个变长序列**转换为**数十个固定长度的数值特征**，这些特征能够充分表达网络流量的时序行为模式，帮助XGBoost准确区分正常流量和PCDN流量。\n",
        "\n",
        "## 🛠️ 代码质量改进\n",
        "\n",
        "### 🔒 安全性修复\n",
        "- **替换 `eval()` 函数**：使用 `ast.literal_eval()` 安全解析数组字符串，避免代码注入风险\n",
        "- **增强错误处理**：添加完善的异常捕获和数据验证\n",
        "\n",
        "### 📊 数据一致性保证\n",
        "- **特征维度对齐**：确保训练、验证、测试集具有相同的特征列\n",
        "- **空数据集处理**：自动从训练集分割验证/测试集，防止数据缺失\n",
        "- **数据质量检查**：检测NaN值、无穷值和标签分布\n",
        "\n",
        "### ⚙️ 模型参数优化\n",
        "- **XGBoost版本兼容性**：智能适配不同版本的XGBoost参数\n",
        "- **多重备用方案**：确保在各种环境下都能正常运行\n",
        "- **改进错误处理**：更robust的序列特征处理流程\n",
        "\n",
        "### 🔧 XGBoost版本兼容性修复\n",
        "\n",
        "**问题背景**: 不同版本的XGBoost对 `early_stopping_rounds` 参数的处理方式不同\n",
        "- **旧版本** (< 1.6): 在 `fit()` 方法中使用 `early_stopping_rounds`\n",
        "- **新版本** (>= 1.6): 可能需要在初始化时设置或使用其他方式\n",
        "\n",
        "**解决方案**: 实现了三层兼容策略\n",
        "1. **优先尝试**: 新版本方式（无early_stopping_rounds）\n",
        "2. **备用方案1**: 在模型初始化时设置early_stopping_rounds \n",
        "3. **备用方案2**: 完全禁用early stopping，增加训练轮数补偿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🛠️ XGBoost版本兼容性说明\n",
        "\n",
        "如果您遇到了 `XGBClassifier.fit() got an unexpected argument 'early_stopping_rounds'` 错误，不用担心！\n",
        "\n",
        "**原因**: XGBoost在不同版本中对early stopping的处理方式有所变化\n",
        "\n",
        "**解决方案**: 代码已经实现了智能版本适配\n",
        "- ✅ **自动检测**：代码会自动检测XGBoost版本\n",
        "- ✅ **智能降级**：如果新方式失败，会自动尝试备用方案  \n",
        "- ✅ **确保运行**：最终确保模型能够成功训练\n",
        "\n",
        "**兼容的XGBoost版本**: \n",
        "- XGBoost 1.0+ ✅\n",
        "- XGBoost 1.6+ ✅ \n",
        "- XGBoost 2.0+ ✅\n",
        "\n",
        "您无需手动修改任何代码，直接运行即可！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备训练数据 - 使用处理后的实际特征列\n",
        "final_feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"实际使用的特征数量: {len(final_feature_columns)}\")\n",
        "\n",
        "X_train = train_processed[final_feature_columns]\n",
        "y_train = train_processed['label']\n",
        "\n",
        "X_val = val_processed[final_feature_columns] \n",
        "y_val = val_processed['label']\n",
        "\n",
        "X_test = test_processed[final_feature_columns]\n",
        "y_test = test_processed['label']\n",
        "\n",
        "print(f\"训练集特征形状: {X_train.shape}\")\n",
        "print(f\"验证集特征形状: {X_val.shape}\")\n",
        "print(f\"测试集特征形状: {X_test.shape}\")\n",
        "\n",
        "# 检查是否还有非数值数据\n",
        "print(f\"\\n训练数据中的数据类型:\")\n",
        "print(X_train.dtypes.value_counts())\n",
        "\n",
        "# 确保所有数据都是数值型\n",
        "X_train = X_train.select_dtypes(include=[np.number])\n",
        "X_val = X_val.select_dtypes(include=[np.number])\n",
        "X_test = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\n最终特征数量: {X_train.shape[1]}\")\n",
        "\n",
        "# 数据质量检查\n",
        "print(\"\\n🔍 数据质量检查:\")\n",
        "print(f\"训练集是否包含NaN: {X_train.isnull().any().any()}\")\n",
        "print(f\"验证集是否包含NaN: {X_val.isnull().any().any()}\")\n",
        "print(f\"测试集是否包含NaN: {X_test.isnull().any().any()}\")\n",
        "print(f\"训练集是否包含无穷值: {np.isinf(X_train).any().any()}\")\n",
        "print(f\"标签分布 - 训练集: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"标签分布 - 验证集: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"标签分布 - 测试集: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# 检查特征维度是否一致\n",
        "assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"特征维度不一致！\"\n",
        "print(\"✅ 数据质量检查通过\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost模型训练\n",
        "print(\"开始训练XGBoost模型...\")\n",
        "\n",
        "# 检查XGBoost版本并适配参数\n",
        "import xgboost\n",
        "print(f\"XGBoost版本: {xgboost.__version__}\")\n",
        "\n",
        "# 创建XGBoost分类器\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# 兼容不同版本的XGBoost训练方式\n",
        "print(\"开始模型训练...\")\n",
        "try:\n",
        "    # 新版本XGBoost的方式 (>= 1.6.0)\n",
        "    xgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False  # 简化输出\n",
        "    )\n",
        "    print(\"✅ 使用新版XGBoost训练方式\")\n",
        "except TypeError as e:\n",
        "    if \"early_stopping_rounds\" in str(e):\n",
        "        # 如果是early_stopping_rounds参数问题，使用备用方式\n",
        "        print(\"⚠️ 检测到XGBoost版本兼容性问题，使用备用训练方式...\")\n",
        "        \n",
        "        # 方式1: 在初始化时设置early_stopping_rounds (某些版本)\n",
        "        try:\n",
        "            xgb_model = xgb.XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                eval_metric='logloss',\n",
        "                early_stopping_rounds=10\n",
        "            )\n",
        "            xgb_model.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "            print(\"✅ 使用early_stopping_rounds在初始化中的方式\")\n",
        "        except:\n",
        "            # 方式2: 不使用early stopping，增加n_estimators\n",
        "            print(\"🔄 使用无early stopping的方式，增加训练轮数...\")\n",
        "            xgb_model = xgb.XGBClassifier(\n",
        "                n_estimators=150,  # 增加轮数补偿\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                eval_metric='logloss'\n",
        "            )\n",
        "            xgb_model.fit(X_train, y_train, verbose=False)\n",
        "            print(\"✅ 使用标准训练方式（无early stopping）\")\n",
        "    else:\n",
        "        raise e  # 如果是其他错误，重新抛出\n",
        "\n",
        "print(\"\\nXGBoost模型训练完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模型评估\n",
        "def evaluate_model(model, X, y, data_name):\n",
        "    \"\"\"\n",
        "    评估模型性能\n",
        "    \"\"\"\n",
        "    # 预测\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "    \n",
        "    print(f\"\\n=== {data_name} 评估结果 ===\")\n",
        "    \n",
        "    # 分类报告\n",
        "    print(\"\\n分类报告:\")\n",
        "    print(classification_report(y, y_pred, target_names=['Normal', 'PCDN']))\n",
        "    \n",
        "    # AUC分数\n",
        "    auc_score = roc_auc_score(y, y_pred_proba)\n",
        "    print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
        "    \n",
        "    return y_pred, y_pred_proba, auc_score\n",
        "\n",
        "# 评估训练集\n",
        "train_pred, train_proba, train_auc = evaluate_model(xgb_model, X_train, y_train, \"训练集\")\n",
        "\n",
        "# 评估验证集\n",
        "val_pred, val_proba, val_auc = evaluate_model(xgb_model, X_val, y_val, \"验证集\")\n",
        "\n",
        "# 评估测试集\n",
        "test_pred, test_proba, test_auc = evaluate_model(xgb_model, X_test, y_test, \"测试集\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征重要性分析\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# 创建特征重要性DataFrame\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== Top 20 最重要特征 ===\")\n",
        "print(importance_df.head(20))\n",
        "\n",
        "# 特征重要性可视化\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = importance_df.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 20 Feature Importance in XGBoost Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 绘制ROC曲线和混淆矩阵\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# ROC曲线\n",
        "datasets = [\n",
        "    (y_train, train_proba, \"Training\", train_auc),\n",
        "    (y_val, val_proba, \"Validation\", val_auc),\n",
        "    (y_test, test_proba, \"Testing\", test_auc)\n",
        "]\n",
        "\n",
        "ax_roc = axes[0, 0]\n",
        "for y_true, y_prob, label, auc in datasets:\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    ax_roc.plot(fpr, tpr, label=f'{label} (AUC = {auc:.3f})')\n",
        "\n",
        "ax_roc.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "ax_roc.set_xlabel('False Positive Rate')\n",
        "ax_roc.set_ylabel('True Positive Rate')\n",
        "ax_roc.set_title('ROC Curves')\n",
        "ax_roc.legend()\n",
        "ax_roc.grid(True)\n",
        "\n",
        "# 混淆矩阵 - 测试集\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Normal', 'PCDN'], \n",
        "            yticklabels=['Normal', 'PCDN'],\n",
        "            ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Confusion Matrix - Test Set')\n",
        "axes[0, 1].set_ylabel('True Label')\n",
        "axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "# 预测概率分布\n",
        "axes[1, 0].hist(test_proba[y_test == 0], alpha=0.7, label='Normal', bins=30, density=True)\n",
        "axes[1, 0].hist(test_proba[y_test == 1], alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "axes[1, 0].set_xlabel('Prediction Probability')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Prediction Probability Distribution')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 学习曲线（训练历史）\n",
        "results = xgb_model.evals_result()\n",
        "if 'validation_0' in results:\n",
        "    epochs = len(results['validation_0']['logloss'])\n",
        "    x_axis = range(0, epochs)\n",
        "    axes[1, 1].plot(x_axis, results['validation_0']['logloss'], label='Validation')\n",
        "    axes[1, 1].set_xlabel('Epochs')\n",
        "    axes[1, 1].set_ylabel('Log Loss')\n",
        "    axes[1, 1].set_title('Model Learning Curve')\n",
        "    axes[1, 1].legend()\n",
        "else:\n",
        "    axes[1, 1].text(0.5, 0.5, 'Learning curve not available', \n",
        "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "    axes[1, 1].set_title('Learning Curve')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征相关性分析\n",
        "if len(importance_df) >= 10:\n",
        "    # 选择最重要的10个特征进行相关性分析\n",
        "    top_10_features = importance_df.head(10)['feature'].tolist()\n",
        "    corr_data = train_processed[top_10_features + ['label']]\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = corr_data.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f')\n",
        "    plt.title('Correlation Matrix of Top 10 Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n=== 与标签相关性最高的特征 ===\")\n",
        "    label_corr = correlation_matrix['label'].abs().sort_values(ascending=False)\n",
        "    print(label_corr[label_corr.index != 'label'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型和结果 (增强版)\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 创建输出目录\n",
        "output_dir = './output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 保存模型\n",
        "model_path = os.path.join(output_dir, 'xgboost_pcdn_classifier.pkl')\n",
        "try:\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"✅ 模型已保存到: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 模型保存失败: {e}\")\n",
        "\n",
        "# 保存特征重要性\n",
        "importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
        "try:\n",
        "    importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 特征重要性已保存到: {importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 特征重要性保存失败: {e}\")\n",
        "\n",
        "# 保存性能报告\n",
        "performance_path = os.path.join(output_dir, 'model_performance.csv')\n",
        "try:\n",
        "    performance_summary.to_csv(performance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 模型性能报告已保存到: {performance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 性能报告保存失败: {e}\")\n",
        "\n",
        "# 保存详细分析报告(包含序列特征配置信息)\n",
        "analysis_report = {\n",
        "    'Project': 'PCDN vs Normal Traffic Classification',\n",
        "    'Algorithm': 'XGBoost',\n",
        "    'Sequence_Features_Enabled': ENABLE_SEQUENCE_FEATURES,\n",
        "    'Total_Features': len(feature_names),\n",
        "    'Train_Samples': len(y_train),\n",
        "    'Val_Samples': len(y_val),\n",
        "    'Test_Samples': len(y_test),\n",
        "    'Train_AUC': train_auc,\n",
        "    'Val_AUC': val_auc,\n",
        "    'Test_AUC': test_auc,\n",
        "    'Top_5_Features': importance_df.head(5)['feature'].tolist()\n",
        "}\n",
        "\n",
        "report_path = os.path.join(output_dir, 'analysis_report.txt')\n",
        "try:\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PCDN流量分类项目分析报告\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"序列特征处理模式: {'启用' if ENABLE_SEQUENCE_FEATURES else '禁用'}\\n\")\n",
        "        if ENABLE_SEQUENCE_FEATURES:\n",
        "            f.write(\"- 使用了复杂的序列特征工程\\n\")\n",
        "            f.write(\"- 从3个序列特征生成了45+个统计特征\\n\")\n",
        "        else:\n",
        "            f.write(\"- 删除了序列特征，使用简化模型\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        for key, value in analysis_report.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "    print(f\"✅ 分析报告已保存到: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 分析报告保存失败: {e}\")\n",
        "\n",
        "print(f\"\\n📁 所有输出文件已保存到目录: {output_dir}\")\n",
        "print(f\"🔧 当前配置: 序列特征 {'启用' if ENABLE_SEQUENCE_FEATURES else '禁用'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模型性能总结\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             模型性能总结\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "performance_summary = pd.DataFrame({\n",
        "    '数据集': ['训练集', '验证集', '测试集'],\n",
        "    'AUC Score': [train_auc, val_auc, test_auc],\n",
        "    '样本数量': [len(y_train), len(y_val), len(y_test)]\n",
        "})\n",
        "\n",
        "print(performance_summary.to_string(index=False))\n",
        "\n",
        "print(f\"\\n特征总数: {len(feature_names)}\")\n",
        "print(f\"最重要的5个特征:\")\n",
        "for i, (idx, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
        "    print(f\"  {i}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\n模型训练和评估完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型和结果\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 创建输出目录\n",
        "output_dir = './output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 保存模型\n",
        "model_path = os.path.join(output_dir, 'xgboost_pcdn_classifier.pkl')\n",
        "try:\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"✅ 模型已保存到: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 模型保存失败: {e}\")\n",
        "\n",
        "# 保存特征重要性\n",
        "importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
        "try:\n",
        "    importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 特征重要性已保存到: {importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 特征重要性保存失败: {e}\")\n",
        "\n",
        "# 保存性能报告\n",
        "performance_path = os.path.join(output_dir, 'model_performance.csv')\n",
        "try:\n",
        "    performance_summary.to_csv(performance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 模型性能报告已保存到: {performance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 性能报告保存失败: {e}\")\n",
        "\n",
        "# 保存详细分析报告\n",
        "analysis_report = {\n",
        "    'Project': 'PCDN vs Normal Traffic Classification',\n",
        "    'Algorithm': 'XGBoost',\n",
        "    'Total_Features': len(feature_names),\n",
        "    'Train_Samples': len(y_train),\n",
        "    'Val_Samples': len(y_val),\n",
        "    'Test_Samples': len(y_test),\n",
        "    'Train_AUC': train_auc,\n",
        "    'Val_AUC': val_auc,\n",
        "    'Test_AUC': test_auc,\n",
        "    'Top_5_Features': importance_df.head(5)['feature'].tolist()\n",
        "}\n",
        "\n",
        "report_path = os.path.join(output_dir, 'analysis_report.txt')\n",
        "try:\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PCDN流量分类项目分析报告\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        for key, value in analysis_report.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "    print(f\"✅ 分析报告已保存到: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 分析报告保存失败: {e}\")\n",
        "\n",
        "print(f\"\\n📁 所有输出文件已保存到目录: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 项目完成总结\n",
        "\n",
        "### ✅ 已完成的工作\n",
        "\n",
        "1. **数据加载与预处理** \n",
        "   - ✅ 智能路径检测和数据加载\n",
        "   - ✅ 自动处理缺失的验证/测试集\n",
        "   - ✅ 安全的序列特征解析\n",
        "\n",
        "2. **特征工程**\n",
        "   - ✅ 序列特征转统计特征 (45+ 新特征)\n",
        "   - ✅ 非数值特征自动编码\n",
        "   - ✅ 数据质量检查和清洗\n",
        "\n",
        "3. **模型训练**\n",
        "   - ✅ XGBoost分类器训练\n",
        "   - ✅ 早停机制防止过拟合\n",
        "   - ✅ 模型性能评估\n",
        "\n",
        "4. **结果分析**\n",
        "   - ✅ 特征重要性分析\n",
        "   - ✅ ROC曲线和混淆矩阵\n",
        "   - ✅ 相关性分析\n",
        "   - ✅ 可视化展示\n",
        "\n",
        "5. **输出管理**\n",
        "   - ✅ 模型文件保存\n",
        "   - ✅ 结果报告导出\n",
        "   - ✅ 项目文档完善\n",
        "\n",
        "### 🚀 使用方法\n",
        "\n",
        "1. **环境准备**: 确保安装所需Python包 (pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, scipy)\n",
        "\n",
        "2. **数据准备**: 将数据集文件夹放在notebook同目录下\n",
        "\n",
        "3. **运行项目**: 依次执行所有代码单元\n",
        "\n",
        "4. **查看结果**: 检查 `./output/` 目录中的输出文件\n",
        "\n",
        "### 🎯 项目价值\n",
        "\n",
        "- **实用性**: 可直接用于PCDN流量检测\n",
        "- **扩展性**: 可轻松适配其他网络流量分类任务  \n",
        "- **可解释性**: 详细的特征重要性分析\n",
        "- **可维护性**: 清晰的代码结构和文档\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bysj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
