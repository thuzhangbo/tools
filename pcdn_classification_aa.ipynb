{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDN vs Normal Traffic Classification using XGBoost\n",
        "\n",
        "这个项目使用XGBoost对正常流量和PCDN流量进行二分类。\n",
        "\n",
        "## 📊 数据集结构\n",
        "- **Training_set/APP_0**: 正常流量 (标签=0)\n",
        "- **Training_set/APP_1**: PCDN流量 (标签=1)  \n",
        "- **Validation_set**: 验证集\n",
        "- **Testing_set**: 测试集\n",
        "\n",
        "## 🔧 主要功能\n",
        "- ✅ **智能数据加载**: 四级列一致性检查，自动发现并修复CSV文件间的列差异\n",
        "- ✅ **序列特征工程**: 将`ip_direction`、`pkt_len`、`iat`转换为丰富的统计特征\n",
        "- ✅ **XGBoost分类**: 版本兼容训练，特征重要性分析\n",
        "- ✅ **完整可视化**: ROC曲线、混淆矩阵、特征重要性图表\n",
        "- ✅ **跨数据集诊断**: 精确识别\"为什么测试集比训练集多1个特征\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📦 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from scipy import stats  # 用于序列特征的偏度和峰度计算\n",
        "import ast  # 用于安全解析数组字符串\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置中文字体支持\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✅ 库导入完成！\")\n",
        "\n",
        "# ===== 🔧 配置参数 =====\n",
        "ENABLE_SEQUENCE_FEATURES = True  # 控制序列特征处理：True=特征工程, False=删除序列特征\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"🔧 序列特征处理模式: {'启用 ✅' if ENABLE_SEQUENCE_FEATURES else '禁用 ❌'}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(\"📊 启用模式 - 执行复杂的序列特征工程:\")\n",
        "    print(\"   • ip_direction → 15个统计特征 (方向模式分析)\")\n",
        "    print(\"   • pkt_len → 15个统计特征 (包大小模式分析)\")  \n",
        "    print(\"   • iat → 15个统计特征 (时间间隔模式分析)\")\n",
        "    print(\"   • 总计生成 45+ 个新特征\")\n",
        "    print(\"   💡 适合: 追求最佳性能，充分利用时序信息\")\n",
        "else:\n",
        "    print(\"🗑️ 禁用模式 - 直接删除序列特征:\")\n",
        "    print(\"   • 删除 ip_direction, pkt_len, iat\")\n",
        "    print(\"   • 仅使用其他网络流量特征训练\")\n",
        "    print(\"   • 模型更简单，训练速度更快\")\n",
        "    print(\"   💡 适合: 资源有限，或希望简化模型的场景\")\n",
        "\n",
        "print(\"\\n💭 要切换模式，请修改上方 ENABLE_SEQUENCE_FEATURES 的值\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 数据类型统一工具：解决数值列 0 与 0.0 不一致问题\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "NUMERIC_FIELDS_CANONICAL = [\n",
        "    'frame.number', 'frame.time_relative', 'ip.version', 'ip.hdr_len',\n",
        "    'ip.dsfield', 'ip.len', 'ip.ttl', 'ip.proto', 'ipv6.plen', 'ipv6.nxt',\n",
        "    'tcp.srcport', 'tcp.dstport', 'tcp.hdr_len', 'tcp.flags.syn', 'tcp.flags.ack',\n",
        "    'tcp.payload', 'udp.srcport', 'udp.dstport', 'udp.length', 'pcap_duration',\n",
        "    'srcport', 'dstport', 'ulProtoID', 'dpi_rule_pkt', 'dpi_packets', 'dpi_bytes',\n",
        "    'flow_uplink_traffic', 'flow_downlink_traffic', 'sum_pkt_len', 'total_pkts',\n",
        "    'srcport_cls', 'dstport_cls', 'pkt_len_avg', 'pkt_len_max', 'pkt_len_min',\n",
        "    'up_pkts', 'up_bytes', 'down_pkts', 'down_bytes', 'up_pkt_ratio', 'down_pkt_ratio',\n",
        "    'up_byte_ratio', 'down_byte_ratio', 'up_down_pkt_ratio', 'up_down_byte_ratio',\n",
        "    'iat_avg', 'iat_max', 'iat_min', 'avg_speed', 'avg_pkt_speed', 'max_burst'\n",
        "]\n",
        "\n",
        "def normalize_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    将常见数值特征统一为 float64，避免 0 与 0.0 类型不一致带来的隐含偏差。\n",
        "    - 仅对能成功转换为数值的列做统一处理\n",
        "    - 其他列保持不变\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "    df_norm = df.copy()\n",
        "\n",
        "    # 优先对预定义字段做统一；若字段不存在则跳过\n",
        "    for col in NUMERIC_FIELDS_CANONICAL:\n",
        "        if col in df_norm.columns:\n",
        "            df_norm[col] = pd.to_numeric(df_norm[col], errors='coerce')\n",
        "            if pd.api.types.is_numeric_dtype(df_norm[col]):\n",
        "                df_norm[col] = df_norm[col].astype('float64')\n",
        "\n",
        "    # 兜底：对其余已是数值类型但不是 float64 的列，也统一为 float64\n",
        "    for col in df_norm.columns:\n",
        "        if col in ['label', 'source_file', 'app_category']:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(df_norm[col]) and df_norm[col].dtype != 'float64':\n",
        "            df_norm[col] = df_norm[col].astype('float64')\n",
        "\n",
        "    return df_norm\n",
        "\n",
        "\n",
        "def check_dtype_consistency(df: pd.DataFrame, fields=None):\n",
        "    \"\"\"简单打印关键字段的数据类型。\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print('⚠️ DataFrame为空，跳过类型检查')\n",
        "        return\n",
        "    fields = fields or ['tcp.payload', 'tcp.srcport', 'tcp.dstport', 'ip.len', 'udp.length']\n",
        "    present = [f for f in fields if f in df.columns]\n",
        "    if not present:\n",
        "        print('ℹ️ 关键字段在当前DataFrame中不存在')\n",
        "        return\n",
        "    print('字段类型检查:')\n",
        "    for f in present:\n",
        "        print(f\"  - {f}: {df[f].dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔁 在数据加载后统一数据类型（请在获取到 train_data/val_data/test_data 后运行）\n",
        "\n",
        "# 假设已得到：train_data, val_data, test_data\n",
        "try:\n",
        "    print(\"统一数值特征为 float64 ...\")\n",
        "    train_data = normalize_data_types(train_data)\n",
        "    val_data = normalize_data_types(val_data) if 'val_data' in globals() else val_data\n",
        "    test_data = normalize_data_types(test_data) if 'test_data' in globals() else test_data\n",
        "\n",
        "    print(\"检查关键字段的数据类型 (训练集):\")\n",
        "    check_dtype_consistency(train_data)\n",
        "    if val_data is not None and len(val_data) > 0:\n",
        "        print(\"检查关键字段的数据类型 (验证集):\")\n",
        "        check_dtype_consistency(val_data)\n",
        "    if test_data is not None and len(test_data) > 0:\n",
        "        print(\"检查关键字段的数据类型 (测试集):\")\n",
        "        check_dtype_consistency(test_data)\n",
        "except NameError:\n",
        "    print(\"尚未定义 train_data/val_data/test_data。请在数据加载完成后再运行本单元。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 智能数据加载函数（支持四级列一致性检查）\n",
        "\n",
        "def load_data_from_directory(base_path, label):\n",
        "    \"\"\"\n",
        "    从指定目录加载所有CSV文件并添加标签 - 增强版：确保列一致性\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(base_path, '*.csv'))\n",
        "    print(f\"在 {base_path} 中找到 {len(csv_files)} 个CSV文件\")\n",
        "    \n",
        "    dataframes = []\n",
        "    all_columns_info = []  # 收集所有文件的列信息\n",
        "    \n",
        "    # 🔍 第一遍扫描：检查所有文件的列结构\n",
        "    print(f\"\\n🔍 检查CSV文件列结构 ({os.path.basename(base_path)}):\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            # 只读取第一行来获取列名，避免加载整个文件\n",
        "            df_sample = pd.read_csv(file, nrows=1)\n",
        "            file_columns = list(df_sample.columns)\n",
        "            all_columns_info.append({\n",
        "                'file': file,\n",
        "                'columns': file_columns,\n",
        "                'count': len(file_columns)\n",
        "            })\n",
        "            \n",
        "            print(f\"  文件{i+1}: {os.path.basename(file)} -> {len(file_columns)}列\")\n",
        "            \n",
        "            # 显示前几个和后几个列名\n",
        "            if len(file_columns) <= 6:\n",
        "                print(f\"    列名: {file_columns}\")\n",
        "            else:\n",
        "                print(f\"    列名: {file_columns[:3]} ... {file_columns[-3:]}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 检查文件 {file} 时出错: {e}\")\n",
        "    \n",
        "    # 🔍 分析列数差异\n",
        "    column_counts = [info['count'] for info in all_columns_info]\n",
        "    unique_counts = sorted(set(column_counts))\n",
        "    \n",
        "    if len(unique_counts) > 1:\n",
        "        print(f\"\\n❌ 发现列数不一致问题！\")\n",
        "        for count in unique_counts:\n",
        "            files_with_count = [f\"文件{i+1}({os.path.basename(info['file'])})\" \n",
        "                              for i, info in enumerate(all_columns_info) \n",
        "                              if info['count'] == count]\n",
        "            print(f\"  📊 {count}列: {', '.join(files_with_count)}\")\n",
        "        \n",
        "        # 🔧 计算所有文件的共同列\n",
        "        if all_columns_info:\n",
        "            common_columns = set(all_columns_info[0]['columns'])\n",
        "            for info in all_columns_info[1:]:\n",
        "                common_columns = common_columns.intersection(set(info['columns']))\n",
        "            \n",
        "            common_columns_list = sorted(list(common_columns))\n",
        "            print(f\"\\n🔧 所有文件的共同列数: {len(common_columns_list)}\")\n",
        "            \n",
        "            # 显示各文件将被排除的列\n",
        "            print(\"📝 各文件独有的列:\")\n",
        "            for i, info in enumerate(all_columns_info):\n",
        "                excluded = set(info['columns']) - common_columns\n",
        "                if excluded:\n",
        "                    print(f\"  文件{i+1}: {sorted(list(excluded))}\")\n",
        "                else:\n",
        "                    print(f\"  文件{i+1}: 无独有列\")\n",
        "                    \n",
        "            print(f\"✅ 将统一使用 {len(common_columns_list)} 个共同列\")\n",
        "            target_columns = common_columns_list\n",
        "        else:\n",
        "            target_columns = None\n",
        "            print(\"❌ 无法确定共同列\")\n",
        "    else:\n",
        "        print(f\"✅ 所有文件列数一致: {column_counts[0]}列\")\n",
        "        target_columns = None\n",
        "    \n",
        "    # 📊 第二遍：使用统一列结构加载数据\n",
        "    print(f\"\\n📊 正式加载数据:\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            original_shape = df.shape\n",
        "            \n",
        "            # 如果需要，只保留共同列\n",
        "            if target_columns is not None:\n",
        "                df = df[target_columns]\n",
        "                print(f\"  🔧 文件{i+1}: {original_shape} -> {df.shape} (列对齐)\")\n",
        "            \n",
        "            df['label'] = label  # 添加标签列\n",
        "            df['source_file'] = os.path.basename(file)  # 添加源文件信息\n",
        "            dataframes.append(df)\n",
        "            print(f\"  ✅ {os.path.basename(file)}: {len(df)}行 x {len(df.columns)}列\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 加载文件 {file} 时出错: {e}\")\n",
        "    \n",
        "    # 🎯 最终合并和验证\n",
        "    if dataframes:\n",
        "        # 最后检查：确保所有dataframe列数一致\n",
        "        final_shapes = [df.shape for df in dataframes]\n",
        "        final_col_counts = [shape[1] for shape in final_shapes]\n",
        "        \n",
        "        if len(set(final_col_counts)) == 1:\n",
        "            print(f\"✅ 合并前最终检查通过: 所有文件均为 {final_col_counts[0]} 列\")\n",
        "        else:\n",
        "            print(f\"⚠️ 合并前发现列数差异: {final_col_counts}\")\n",
        "        \n",
        "        result = pd.concat(dataframes, ignore_index=True)\n",
        "        \n",
        "        print(f\"🎉 数据加载完成:\")\n",
        "        print(f\"  📁 文件数: {len(csv_files)}\")\n",
        "        print(f\"  📊 最终形状: {result.shape}\")\n",
        "        print(f\"  🏷️ 标签值: {label}\")\n",
        "        \n",
        "        # 📋 返回结果和列信息\n",
        "        columns_meta = {\n",
        "            'final_columns': list(result.columns),\n",
        "            'original_columns_info': all_columns_info,\n",
        "            'target_columns': target_columns,\n",
        "            'dataset_path': base_path\n",
        "        }\n",
        "        \n",
        "        return result, columns_meta\n",
        "    else:\n",
        "        print(\"❌ 没有成功加载任何文件\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "\n",
        "def analyze_cross_dataset_columns(datasets_info):\n",
        "    \"\"\"\n",
        "    分析不同数据集之间的列差异，包括CSV文件级别的详细分析\n",
        "    datasets_info: 字典，格式为 {'数据集名称': columns_meta}\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🔍 跨数据集列结构一致性分析\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # 收集所有数据集的列信息\n",
        "    dataset_columns = {}\n",
        "    dataset_csv_info = {}  # 新增：收集每个数据集的CSV文件详细信息\n",
        "    \n",
        "    for dataset_name, meta in datasets_info.items():\n",
        "        if 'final_columns' in meta and meta['final_columns']:\n",
        "            # 排除我们添加的辅助列\n",
        "            original_cols = [col for col in meta['final_columns'] \n",
        "                           if col not in ['label', 'source_file']]\n",
        "            dataset_columns[dataset_name] = set(original_cols)\n",
        "            print(f\"📊 {dataset_name}: {len(original_cols)}个原始列\")\n",
        "            \n",
        "            # 收集CSV文件级别的信息\n",
        "            dataset_csv_info[dataset_name] = {\n",
        "                'normal_meta': meta.get('normal_meta', {}),\n",
        "                'pcdn_meta': meta.get('pcdn_meta', {})\n",
        "            }\n",
        "    \n",
        "    if len(dataset_columns) < 2:\n",
        "        print(\"⚠️ 数据集数量不足，无法进行跨数据集比较\")\n",
        "        return\n",
        "    \n",
        "    # 计算所有数据集的列统计\n",
        "    all_datasets = list(dataset_columns.keys())\n",
        "    column_counts = {name: len(cols) for name, cols in dataset_columns.items()}\n",
        "    \n",
        "    print(f\"\\n📈 列数统计:\")\n",
        "    for dataset, count in column_counts.items():\n",
        "        print(f\"  {dataset}: {count}列\")\n",
        "    \n",
        "    # 检查列数是否一致\n",
        "    unique_counts = set(column_counts.values())\n",
        "    if len(unique_counts) == 1:\n",
        "        print(\"✅ 所有数据集列数一致\")\n",
        "    else:\n",
        "        print(f\"❌ 发现列数不一致: {sorted(unique_counts)}\")\n",
        "    \n",
        "    # 计算所有数据集的共同列和独有列\n",
        "    all_columns = set()\n",
        "    for cols in dataset_columns.values():\n",
        "        all_columns = all_columns.union(cols)\n",
        "    \n",
        "    common_columns = set.intersection(*dataset_columns.values()) if dataset_columns else set()\n",
        "    \n",
        "    print(f\"\\n🔧 列分析结果:\")\n",
        "    print(f\"  所有唯一列数: {len(all_columns)}\")\n",
        "    print(f\"  共同列数: {len(common_columns)}\")\n",
        "    print(f\"  差异列数: {len(all_columns) - len(common_columns)}\")\n",
        "    \n",
        "    # 🆕 CSV文件级别的列差异检查（仅显示不一致的情况）\n",
        "    print(f\"\\n📁 CSV文件列差异检查:\")\n",
        "    found_issues = False\n",
        "    for dataset, cols in dataset_columns.items():\n",
        "        csv_info = dataset_csv_info.get(dataset, {})\n",
        "        \n",
        "        # 检查APP_0 (正常流量) 的CSV文件\n",
        "        normal_meta = csv_info.get('normal_meta', {})\n",
        "        if 'original_columns_info' in normal_meta:\n",
        "            csv_files_info = normal_meta['original_columns_info']\n",
        "            print(f\"\\n  🔍 {dataset} - APP_0:\")\n",
        "            _analyze_csv_files_columns(csv_files_info, \"APP_0\")\n",
        "            found_issues = True\n",
        "        \n",
        "        # 检查APP_1 (PCDN流量) 的CSV文件  \n",
        "        pcdn_meta = csv_info.get('pcdn_meta', {})\n",
        "        if 'original_columns_info' in pcdn_meta:\n",
        "            csv_files_info = pcdn_meta['original_columns_info']\n",
        "            print(f\"  🔍 {dataset} - APP_1:\")\n",
        "            _analyze_csv_files_columns(csv_files_info, \"APP_1\")\n",
        "            found_issues = True\n",
        "    \n",
        "    if not found_issues:\n",
        "        print(\"  ✅ 所有CSV文件列结构一致\")\n",
        "    \n",
        "    # 跨数据集独有列分析（仅显示有差异的数据集）\n",
        "    print(f\"\\n📝 跨数据集独有列分析:\")\n",
        "    has_differences = False\n",
        "    \n",
        "    for dataset, cols in dataset_columns.items():\n",
        "        unique_to_dataset = cols - common_columns\n",
        "        missing_columns = common_columns - cols\n",
        "        \n",
        "        if unique_to_dataset or missing_columns:\n",
        "            has_differences = True\n",
        "            print(f\"\\n  ⚠️ {dataset} ({len(cols)}列):\")\n",
        "            \n",
        "            if unique_to_dataset:\n",
        "                print(f\"    🔸 独有列({len(unique_to_dataset)}): {sorted(list(unique_to_dataset))}\")\n",
        "                \n",
        "                # 显示独有列来源的CSV文件\n",
        "                csv_info = dataset_csv_info.get(dataset, {})\n",
        "                source_files = []\n",
        "                \n",
        "                # 检查APP_0文件\n",
        "                normal_meta = csv_info.get('normal_meta', {})\n",
        "                if 'original_columns_info' in normal_meta:\n",
        "                    for i, file_info in enumerate(normal_meta['original_columns_info']):\n",
        "                        file_columns = set(file_info['columns'])\n",
        "                        file_unique = file_columns.intersection(unique_to_dataset)\n",
        "                        if file_unique:\n",
        "                            filename = os.path.basename(file_info['file'])\n",
        "                            source_files.append(f\"APP_0/{filename}({sorted(list(file_unique))})\")\n",
        "                \n",
        "                # 检查APP_1文件\n",
        "                pcdn_meta = csv_info.get('pcdn_meta', {})\n",
        "                if 'original_columns_info' in pcdn_meta:\n",
        "                    for i, file_info in enumerate(pcdn_meta['original_columns_info']):\n",
        "                        file_columns = set(file_info['columns'])\n",
        "                        file_unique = file_columns.intersection(unique_to_dataset)\n",
        "                        if file_unique:\n",
        "                            filename = os.path.basename(file_info['file'])\n",
        "                            source_files.append(f\"APP_1/{filename}({sorted(list(file_unique))})\")\n",
        "                \n",
        "                if source_files:\n",
        "                    print(f\"    📁 来源: {', '.join(source_files)}\")\n",
        "            \n",
        "            if missing_columns:\n",
        "                print(f\"    🔸 缺失列({len(missing_columns)}): {sorted(list(missing_columns))}\")\n",
        "    \n",
        "    if not has_differences:\n",
        "        print(\"  ✅ 所有数据集列完全一致\")\n",
        "    \n",
        "    # 两两数据集对比（仅显示有差异的对比）\n",
        "    if len(dataset_columns) > 1:\n",
        "        print(f\"\\n🔄 数据集差异对比:\")\n",
        "        datasets_list = list(dataset_columns.keys())\n",
        "        has_pair_differences = False\n",
        "        \n",
        "        for i in range(len(datasets_list)):\n",
        "            for j in range(i+1, len(datasets_list)):\n",
        "                dataset1, dataset2 = datasets_list[i], datasets_list[j]\n",
        "                cols1, cols2 = dataset_columns[dataset1], dataset_columns[dataset2]\n",
        "                \n",
        "                only_in_1 = cols1 - cols2\n",
        "                only_in_2 = cols2 - cols1\n",
        "                \n",
        "                if only_in_1 or only_in_2:\n",
        "                    has_pair_differences = True\n",
        "                    print(f\"\\n  ⚠️ {dataset1} vs {dataset2}:\")\n",
        "                    \n",
        "                    if only_in_1:\n",
        "                        print(f\"    🔸 仅{dataset1}有({len(only_in_1)}): {sorted(list(only_in_1))}\")\n",
        "                        _show_unique_columns_source(dataset1, only_in_1, dataset_csv_info)\n",
        "                        \n",
        "                    if only_in_2:\n",
        "                        print(f\"    🔸 仅{dataset2}有({len(only_in_2)}): {sorted(list(only_in_2))}\")\n",
        "                        _show_unique_columns_source(dataset2, only_in_2, dataset_csv_info)\n",
        "        \n",
        "        if not has_pair_differences:\n",
        "            print(\"  ✅ 所有数据集完全匹配\")\n",
        "    \n",
        "    # 推荐的统一策略\n",
        "    print(f\"\\n💡 统一策略建议:\")\n",
        "    if len(common_columns) == len(all_columns):\n",
        "        print(\"✅ 所有数据集列完全一致，无需调整\")\n",
        "    else:\n",
        "        print(f\"🔧 建议统一到共同列集合 ({len(common_columns)}列)\")\n",
        "        print(f\"   这将确保所有数据集具有相同的特征结构\")\n",
        "        \n",
        "        lost_columns = all_columns - common_columns\n",
        "        if lost_columns:\n",
        "            print(f\"   ⚠️ 将丢失的列: {sorted(list(lost_columns))}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "def _analyze_csv_files_columns(csv_files_info, app_type):\n",
        "    \"\"\"\n",
        "    分析单个APP目录下所有CSV文件的列差异 - 精简版，只显示关键不一致信息\n",
        "    \"\"\"\n",
        "    if not csv_files_info:\n",
        "        return\n",
        "    \n",
        "    # 收集所有文件的列信息\n",
        "    file_column_counts = {}\n",
        "    \n",
        "    for i, file_info in enumerate(csv_files_info):\n",
        "        filename = os.path.basename(file_info['file'])\n",
        "        file_columns = set(file_info['columns'])\n",
        "        col_count = len(file_columns)\n",
        "        \n",
        "        file_column_counts[filename] = {\n",
        "            'columns': file_columns,\n",
        "            'count': col_count\n",
        "        }\n",
        "    \n",
        "    # 分析列数分布\n",
        "    count_distribution = {}\n",
        "    for filename, info in file_column_counts.items():\n",
        "        count = info['count']\n",
        "        if count not in count_distribution:\n",
        "            count_distribution[count] = []\n",
        "        count_distribution[count].append(filename)\n",
        "    \n",
        "    # 只在发现不一致时输出详细信息\n",
        "    if len(count_distribution) > 1:\n",
        "        print(f\"      ❌ {app_type}列数不一致:\")\n",
        "        for count in sorted(count_distribution.keys()):\n",
        "            files = count_distribution[count]\n",
        "            print(f\"        {count}列: {files}\")\n",
        "        \n",
        "        # 计算共同列并只显示有独有列的文件\n",
        "        common_columns_in_app = set(csv_files_info[0]['columns'])\n",
        "        for file_info in csv_files_info[1:]:\n",
        "            common_columns_in_app = common_columns_in_app.intersection(set(file_info['columns']))\n",
        "        \n",
        "        # 只显示有独有列的特殊文件\n",
        "        special_files = []\n",
        "        for filename, info in file_column_counts.items():\n",
        "            file_columns = info['columns']\n",
        "            unique_columns = file_columns - common_columns_in_app\n",
        "            if unique_columns:\n",
        "                special_files.append(f\"{filename}(+{sorted(list(unique_columns))})\")\n",
        "        \n",
        "        if special_files:\n",
        "            print(f\"      🔸 特殊文件: {', '.join(special_files)}\")\n",
        "\n",
        "\n",
        "def _show_unique_columns_source(dataset_name, unique_columns, dataset_csv_info):\n",
        "    \"\"\"\n",
        "    辅助函数：简洁显示独有列来自哪个CSV文件\n",
        "    \"\"\"\n",
        "    csv_info = dataset_csv_info.get(dataset_name, {})\n",
        "    source_files = []\n",
        "    \n",
        "    # 检查APP_0 (正常流量) 的CSV文件\n",
        "    normal_meta = csv_info.get('normal_meta', {})\n",
        "    if 'original_columns_info' in normal_meta:\n",
        "        for file_info in normal_meta['original_columns_info']:\n",
        "            file_columns = set(file_info['columns'])\n",
        "            file_unique = file_columns.intersection(unique_columns)\n",
        "            if file_unique:\n",
        "                filename = os.path.basename(file_info['file'])\n",
        "                source_files.append(f\"APP_0/{filename}\")\n",
        "    \n",
        "    # 检查APP_1 (PCDN流量) 的CSV文件\n",
        "    pcdn_meta = csv_info.get('pcdn_meta', {})\n",
        "    if 'original_columns_info' in pcdn_meta:\n",
        "        for file_info in pcdn_meta['original_columns_info']:\n",
        "            file_columns = set(file_info['columns'])\n",
        "            file_unique = file_columns.intersection(unique_columns)\n",
        "            if file_unique:\n",
        "                filename = os.path.basename(file_info['file'])\n",
        "                source_files.append(f\"APP_1/{filename}\")\n",
        "    \n",
        "    if source_files:\n",
        "        print(f\"      📁 来源: {', '.join(source_files)}\")\n",
        "\n",
        "print(\"✅ 数据加载函数定义完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 智能数据加载：四级列一致性检查\n",
        "print(\"🚀 开始智能数据加载...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 🔧 数据目录配置\n",
        "data_dir = \"./pcdn_32_pkts_2class_feature_enhance_v17.4_dataset\"\n",
        "\n",
        "# 检查数据目录是否存在\n",
        "if not os.path.exists(data_dir):\n",
        "    print(f\"❌ 数据目录不存在: {data_dir}\")\n",
        "    print(\"请确保数据集文件夹与notebook在同一目录下\")\n",
        "    print(\"当前工作目录:\", os.getcwd())\n",
        "    print(\"当前目录内容:\", [f for f in os.listdir('.') if not f.startswith('.')])\n",
        "    \n",
        "    # 尝试查找数据目录\n",
        "    possible_dirs = [d for d in os.listdir('.') if 'pcdn' in d.lower() and os.path.isdir(d)]\n",
        "    if possible_dirs:\n",
        "        print(f\"发现可能的数据目录: {possible_dirs}\")\n",
        "        data_dir = possible_dirs[0]\n",
        "        print(f\"✅ 使用数据目录: {data_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"找不到数据目录，请检查数据集位置\")\n",
        "else:\n",
        "    print(f\"✅ 找到数据目录: {data_dir}\")\n",
        "\n",
        "# 收集所有数据集的列信息\n",
        "datasets_meta = {}\n",
        "\n",
        "# 📂 加载训练数据\n",
        "print(f\"\\n📂 加载训练数据...\")\n",
        "train_normal, train_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)\n",
        "train_pcdn, train_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)\n",
        "\n",
        "# 合并训练数据并记录元信息\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "datasets_meta['Training_set'] = {\n",
        "    'final_columns': list(train_data.columns),\n",
        "    'dataset_path': 'Training_set',\n",
        "    'shape': train_data.shape,\n",
        "    'normal_meta': train_normal_meta,\n",
        "    'pcdn_meta': train_pcdn_meta\n",
        "}\n",
        "\n",
        "print(f\"✅ 训练数据: {len(train_normal)}个正常 + {len(train_pcdn)}个PCDN = {len(train_data)}总样本\")\n",
        "\n",
        "# 📂 加载验证数据\n",
        "print(f\"\\n📂 加载验证数据...\")\n",
        "val_normal, val_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn, val_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查验证数据是否为空\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"⚠️ 验证集为空，将使用训练集的一部分作为验证集\")\n",
        "    val_data = pd.DataFrame()\n",
        "    datasets_meta['Validation_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "    datasets_meta['Validation_set'] = {\n",
        "        'final_columns': list(val_data.columns),\n",
        "        'dataset_path': 'Validation_set',\n",
        "        'shape': val_data.shape,\n",
        "        'normal_meta': val_normal_meta,\n",
        "        'pcdn_meta': val_pcdn_meta\n",
        "    }\n",
        "    print(f\"✅ 验证数据: {len(val_normal)}个正常 + {len(val_pcdn)}个PCDN = {len(val_data)}总样本\")\n",
        "\n",
        "# 📂 加载测试数据\n",
        "print(f\"\\n📂 加载测试数据...\")\n",
        "test_normal, test_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn, test_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查测试数据是否为空\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"⚠️ 测试集为空，将使用训练集的一部分作为测试集\")\n",
        "    test_data = pd.DataFrame()\n",
        "    datasets_meta['Testing_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "    datasets_meta['Testing_set'] = {\n",
        "        'final_columns': list(test_data.columns),\n",
        "        'dataset_path': 'Testing_set', \n",
        "        'shape': test_data.shape,\n",
        "        'normal_meta': test_normal_meta,\n",
        "        'pcdn_meta': test_pcdn_meta\n",
        "    }\n",
        "    print(f\"✅ 测试数据: {len(test_normal)}个正常 + {len(test_pcdn)}个PCDN = {len(test_data)}总样本\")\n",
        "\n",
        "# 📊 执行跨数据集列结构分析\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"🔍 开始跨数据集列结构分析...\")\n",
        "analyze_cross_dataset_columns(datasets_meta)\n",
        "\n",
        "# 📋 数据加载总结\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"📋 数据加载总结\")\n",
        "print(\"=\"*80)\n",
        "for dataset_name, meta in datasets_meta.items():\n",
        "    if 'status' in meta and meta['status'] == 'empty':\n",
        "        print(f\"📂 {dataset_name}: 空数据集\")\n",
        "    else:\n",
        "        print(f\"📂 {dataset_name}: {meta['shape'][0]}行 x {meta['shape'][1]}列\")\n",
        "\n",
        "print(\"🎉 智能数据加载和跨数据集分析完成！\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 CSV格式和字段探索分析\n",
        "\n",
        "print(\"📊 开始CSV格式和字段分析...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. 基本数据信息\n",
        "print(\"🔍 **数据集基本信息**:\")\n",
        "print(f\"  训练集形状: {train_data.shape}\")\n",
        "if len(val_data) > 0:\n",
        "    print(f\"  验证集形状: {val_data.shape}\")\n",
        "else:\n",
        "    print(f\"  验证集: 空 (将从训练集分割)\")\n",
        "if len(test_data) > 0:\n",
        "    print(f\"  测试集形状: {test_data.shape}\")\n",
        "else:\n",
        "    print(f\"  测试集: 空 (将从训练集分割)\")\n",
        "\n",
        "# 2. CSV文件详细信息展示\n",
        "print(f\"\\n📁 **CSV文件详细信息**:\")\n",
        "if 'Training_set' in datasets_meta:\n",
        "    train_meta = datasets_meta['Training_set']\n",
        "    print(f\"  训练集文件分布:\")\n",
        "    \n",
        "    # 显示APP_0文件信息\n",
        "    if 'normal_meta' in train_meta and 'original_columns_info' in train_meta['normal_meta']:\n",
        "        normal_files = train_meta['normal_meta']['original_columns_info']\n",
        "        print(f\"    📂 APP_0 (正常流量): {len(normal_files)}个CSV文件\")\n",
        "        for i, file_info in enumerate(normal_files):\n",
        "            filename = os.path.basename(file_info['file'])\n",
        "            col_count = file_info['count']\n",
        "            print(f\"      文件{i+1}: {filename} ({col_count}列)\")\n",
        "    \n",
        "    # 显示APP_1文件信息  \n",
        "    if 'pcdn_meta' in train_meta and 'original_columns_info' in train_meta['pcdn_meta']:\n",
        "        pcdn_files = train_meta['pcdn_meta']['original_columns_info']\n",
        "        print(f\"    📂 APP_1 (PCDN流量): {len(pcdn_files)}个CSV文件\")\n",
        "        for i, file_info in enumerate(pcdn_files):\n",
        "            filename = os.path.basename(file_info['file'])\n",
        "            col_count = file_info['count']\n",
        "            print(f\"      文件{i+1}: {filename} ({col_count}列)\")\n",
        "\n",
        "# 3. 列名和数据类型分析\n",
        "print(f\"\\n📋 **列名和数据类型** (训练集示例):\")\n",
        "print(f\"  总列数: {len(train_data.columns)}\")\n",
        "print(f\"  数据类型分布:\")\n",
        "dtype_counts = train_data.dtypes.value_counts()\n",
        "for dtype, count in dtype_counts.items():\n",
        "    print(f\"    {dtype}: {count}列\")\n",
        "\n",
        "print(f\"\\n📝 **所有列名列表**:\")\n",
        "all_columns = [col for col in train_data.columns if col not in ['label', 'source_file']]\n",
        "print(f\"  原始特征列数: {len(all_columns)}\")\n",
        "\n",
        "# 分组显示列名\n",
        "chunk_size = 10\n",
        "for i in range(0, len(all_columns), chunk_size):\n",
        "    chunk = all_columns[i:i+chunk_size]\n",
        "    print(f\"  {i+1:2d}-{min(i+chunk_size, len(all_columns)):2d}: {chunk}\")\n",
        "\n",
        "# 4. 数据样本展示\n",
        "print(f\"\\n📖 **数据样本展示** (前5行):\")\n",
        "display_columns = [col for col in train_data.columns if col not in ['source_file']][:15]  # 只显示前15列避免过宽\n",
        "print(\"选择显示前15列:\", display_columns)\n",
        "print(train_data[display_columns].head())\n",
        "\n",
        "# 5. 标签分布分析\n",
        "print(f\"\\n🏷️ **标签分布分析**:\")\n",
        "label_counts = train_data['label'].value_counts().sort_index()\n",
        "total_samples = len(train_data)\n",
        "\n",
        "for label, count in label_counts.items():\n",
        "    label_name = \"正常流量\" if label == 0 else \"PCDN流量\"\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"  标签 {label} ({label_name}): {count:,} 样本 ({percentage:.1f}%)\")\n",
        "\n",
        "# 6. 缺失值分析\n",
        "print(f\"\\n🔍 **缺失值分析**:\")\n",
        "missing_counts = train_data.isnull().sum()\n",
        "missing_columns = missing_counts[missing_counts > 0]\n",
        "\n",
        "if len(missing_columns) > 0:\n",
        "    print(f\"  发现 {len(missing_columns)} 列有缺失值:\")\n",
        "    for col, count in missing_columns.items():\n",
        "        percentage = (count / len(train_data)) * 100\n",
        "        print(f\"    {col}: {count} 缺失值 ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"  ✅ 没有发现缺失值\")\n",
        "\n",
        "# 7. 数值型特征统计描述\n",
        "print(f\"\\n📈 **数值型特征统计描述**:\")\n",
        "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols = [col for col in numeric_cols if col != 'label'][:10]  # 选择前10个数值列\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    print(f\"  选择前10个数值型特征进行统计:\")\n",
        "    stats_df = train_data[numeric_cols].describe()\n",
        "    print(stats_df)\n",
        "else:\n",
        "    print(\"  没有发现数值型特征\")\n",
        "\n",
        "# 8. 序列特征分析 (如果存在)\n",
        "sequence_features = ['ip_direction', 'pkt_len', 'iat']\n",
        "existing_seq_features = [col for col in sequence_features if col in train_data.columns]\n",
        "\n",
        "if existing_seq_features:\n",
        "    print(f\"\\n🔄 **序列特征分析**:\")\n",
        "    print(f\"  发现序列特征: {existing_seq_features}\")\n",
        "    \n",
        "    for col in existing_seq_features[:2]:  # 只分析前2个避免输出过长\n",
        "        print(f\"\\n  📊 {col} 特征样本:\")\n",
        "        # 显示几个序列样本\n",
        "        for i in range(min(3, len(train_data))):\n",
        "            sample_value = train_data[col].iloc[i]\n",
        "            print(f\"    样本{i+1}: {str(sample_value)[:100]}{'...' if len(str(sample_value)) > 100 else ''}\")\n",
        "else:\n",
        "    print(f\"\\n🔄 **序列特征**: 未发现序列特征 {sequence_features}\")\n",
        "\n",
        "# 9. 数据质量检查\n",
        "print(f\"\\n🔍 **数据质量检查**:\")\n",
        "print(f\"  无穷值检查:\")\n",
        "inf_cols = []\n",
        "for col in train_data.select_dtypes(include=[np.number]).columns:\n",
        "    inf_count = np.isinf(train_data[col]).sum()\n",
        "    if inf_count > 0:\n",
        "        inf_cols.append((col, inf_count))\n",
        "\n",
        "if inf_cols:\n",
        "    print(f\"    发现无穷值: {len(inf_cols)} 列\")\n",
        "    for col, count in inf_cols[:5]:  # 只显示前5个\n",
        "        print(f\"      {col}: {count} 个无穷值\")\n",
        "else:\n",
        "    print(\"    ✅ 没有发现无穷值\")\n",
        "\n",
        "# 10. 创建数据概览可视化\n",
        "print(f\"\\n📊 生成数据概览图表...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Dataset Overview Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 10.1 标签分布饼图\n",
        "ax1 = axes[0, 0]\n",
        "label_names = ['Normal Traffic', 'PCDN Traffic']\n",
        "label_values = [label_counts.get(0, 0), label_counts.get(1, 0)]\n",
        "colors = ['lightblue', 'lightcoral']\n",
        "ax1.pie(label_values, labels=label_names, autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "ax1.set_title('Label Distribution')\n",
        "\n",
        "# 10.2 数据类型分布\n",
        "ax2 = axes[0, 1]\n",
        "dtype_names = [str(dtype) for dtype in dtype_counts.index]\n",
        "ax2.bar(range(len(dtype_names)), dtype_counts.values, color='lightgreen')\n",
        "ax2.set_xticks(range(len(dtype_names)))\n",
        "ax2.set_xticklabels(dtype_names, rotation=45)\n",
        "ax2.set_title('Data Types Distribution')\n",
        "ax2.set_ylabel('Number of Columns')\n",
        "\n",
        "# 10.3 缺失值热图 (如果有缺失值)\n",
        "ax3 = axes[1, 0]\n",
        "if len(missing_columns) > 0:\n",
        "    missing_data = train_data[missing_columns.index[:10]].isnull()  # 最多显示10列\n",
        "    sns.heatmap(missing_data, ax=ax3, cbar=True, yticklabels=False, cmap='viridis')\n",
        "    ax3.set_title('Missing Values Pattern')\n",
        "else:\n",
        "    ax3.text(0.5, 0.5, 'No Missing Values Found', transform=ax3.transAxes, \n",
        "             fontsize=14, ha='center', va='center')\n",
        "    ax3.set_title('Missing Values Check')\n",
        "    ax3.set_xticks([])\n",
        "    ax3.set_yticks([])\n",
        "\n",
        "# 10.4 数值特征分布样本\n",
        "ax4 = axes[1, 1]\n",
        "if len(numeric_cols) >= 2:\n",
        "    # 选择两个数值特征绘制散点图\n",
        "    col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "    normal_data = train_data[train_data['label'] == 0]\n",
        "    pcdn_data = train_data[train_data['label'] == 1]\n",
        "    \n",
        "    ax4.scatter(normal_data[col1], normal_data[col2], alpha=0.6, label='Normal', s=20)\n",
        "    ax4.scatter(pcdn_data[col1], pcdn_data[col2], alpha=0.6, label='PCDN', s=20)\n",
        "    ax4.set_xlabel(col1)\n",
        "    ax4.set_ylabel(col2)\n",
        "    ax4.set_title(f'Feature Scatter: {col1} vs {col2}')\n",
        "    ax4.legend()\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'Insufficient Numeric Features', transform=ax4.transAxes, \n",
        "             fontsize=12, ha='center', va='center')\n",
        "    ax4.set_title('Feature Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ CSV格式和字段分析完成！\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 数据预处理和特征工程\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    数据预处理函数：删除指定列、处理缺失值等\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # 删除指定的不需要的列\n",
        "    columns_to_drop = [\n",
        "        'source_file',  # 源文件信息（添加的辅助列）\n",
        "        'frame.number', 'frame.time_relative', 'ip.version', 'ip.ttl', 'ip.src', 'ip.dst',\n",
        "        'ipv6.plen', 'ipv6.nxt', 'ipv6.src', 'ipv6.dst', '_ws.col.Protocol', \n",
        "        'ssl.handshake.extensions_server_name', 'eth.src', 'pcap_duration', 'app', 'os', 'date',\n",
        "        'flow_id', 'dpi_file_name', 'dpi_five_tuple', 'dpi_rule_result', 'dpi_label', \n",
        "        'ulProtoID', 'dpi_rule_pkt', 'dpi_packets', 'dpi_bytes', 'label_source', 'id', 'category'\n",
        "    ]\n",
        "    \n",
        "    # 删除存在的列\n",
        "    existing_columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    if existing_columns_to_drop:\n",
        "        df_processed = df_processed.drop(columns=existing_columns_to_drop)\n",
        "        print(f\"✅ 删除了 {len(existing_columns_to_drop)} 个指定列\")\n",
        "        \n",
        "    # 处理缺失值和无穷值\n",
        "    df_processed = df_processed.fillna(0)\n",
        "    df_processed = df_processed.replace([np.inf, -np.inf], 0)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "\n",
        "def process_sequence_features(df, enable_sequence_processing=True):\n",
        "    \"\"\"\n",
        "    处理序列类型的特征 (pkt_len, ip_direction, iat)\n",
        "    enable_sequence_processing: True=转换为统计特征, False=直接删除\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    sequence_columns = ['pkt_len', 'ip_direction', 'iat']\n",
        "    \n",
        "    if not enable_sequence_processing:\n",
        "        # 简单模式：直接删除序列特征\n",
        "        print(\"🗑️ 删除序列特征模式...\")\n",
        "        for col in sequence_columns:\n",
        "            if col in df_processed.columns:\n",
        "                df_processed = df_processed.drop(columns=[col])\n",
        "        print(f\"✅ 已删除序列特征: {sequence_columns}\")\n",
        "        return df_processed\n",
        "    \n",
        "    # 复杂模式：序列特征工程\n",
        "    print(\"🔧 序列特征工程模式...\")\n",
        "    \n",
        "    for col in sequence_columns:\n",
        "        if col in df_processed.columns:\n",
        "            print(f\"处理序列特征: {col}\")\n",
        "            \n",
        "            # 安全地解析序列数据\n",
        "            sequences = []\n",
        "            for idx, value in df_processed[col].items():\n",
        "                try:\n",
        "                    if pd.isna(value) or value == '' or value == 'nan':\n",
        "                        sequences.append([])\n",
        "                    else:\n",
        "                        # 使用 ast.literal_eval 安全解析\n",
        "                        if isinstance(value, str):\n",
        "                            parsed = ast.literal_eval(value)\n",
        "                            sequences.append(parsed if isinstance(parsed, list) else [])\n",
        "                        else:\n",
        "                            sequences.append([])\n",
        "                except:\n",
        "                    sequences.append([])\n",
        "            \n",
        "            # 提取统计特征\n",
        "            feature_dict = {}\n",
        "            \n",
        "            for i, seq in enumerate(sequences):\n",
        "                if len(seq) > 0:\n",
        "                    # 基础统计特征\n",
        "                    feature_dict.setdefault(f'{col}_mean', []).append(np.mean(seq))\n",
        "                    feature_dict.setdefault(f'{col}_std', []).append(np.std(seq))\n",
        "                    feature_dict.setdefault(f'{col}_min', []).append(np.min(seq))\n",
        "                    feature_dict.setdefault(f'{col}_max', []).append(np.max(seq))\n",
        "                    feature_dict.setdefault(f'{col}_median', []).append(np.median(seq))\n",
        "                    feature_dict.setdefault(f'{col}_range', []).append(np.max(seq) - np.min(seq))\n",
        "                    feature_dict.setdefault(f'{col}_q25', []).append(np.percentile(seq, 25))\n",
        "                    feature_dict.setdefault(f'{col}_q75', []).append(np.percentile(seq, 75))\n",
        "                    feature_dict.setdefault(f'{col}_iqr', []).append(np.percentile(seq, 75) - np.percentile(seq, 25))\n",
        "                    feature_dict.setdefault(f'{col}_len', []).append(len(seq))\n",
        "                    \n",
        "                    # 变异系数\n",
        "                    cv = np.std(seq) / np.mean(seq) if np.mean(seq) != 0 else 0\n",
        "                    feature_dict.setdefault(f'{col}_cv', []).append(cv)\n",
        "                    \n",
        "                    # 偏度和峰度\n",
        "                    try:\n",
        "                        feature_dict.setdefault(f'{col}_skew', []).append(stats.skew(seq))\n",
        "                        feature_dict.setdefault(f'{col}_kurtosis', []).append(stats.kurtosis(seq))\n",
        "                    except:\n",
        "                        feature_dict.setdefault(f'{col}_skew', []).append(0)\n",
        "                        feature_dict.setdefault(f'{col}_kurtosis', []).append(0)\n",
        "                    \n",
        "                    # 序列特定特征\n",
        "                    if col == 'ip_direction':\n",
        "                        # 出方向比例\n",
        "                        out_ratio = sum(1 for x in seq if x == 1) / len(seq)\n",
        "                        feature_dict.setdefault(f'{col}_out_ratio', []).append(out_ratio)\n",
        "                    elif col == 'pkt_len':\n",
        "                        # 小包比例 (<=64字节)\n",
        "                        small_pkt_ratio = sum(1 for x in seq if x <= 64) / len(seq)\n",
        "                        feature_dict.setdefault(f'{col}_small_pkt_ratio', []).append(small_pkt_ratio)\n",
        "                    elif col == 'iat':\n",
        "                        # 突发比例 (间隔<=0.1秒)\n",
        "                        burst_ratio = sum(1 for x in seq if x <= 0.1) / len(seq)\n",
        "                        feature_dict.setdefault(f'{col}_burst_ratio', []).append(burst_ratio)\n",
        "                else:\n",
        "                    # 处理空序列\n",
        "                    for feature_name in [f'{col}_mean', f'{col}_std', f'{col}_min', f'{col}_max', \n",
        "                                       f'{col}_median', f'{col}_range', f'{col}_q25', f'{col}_q75', \n",
        "                                       f'{col}_iqr', f'{col}_len', f'{col}_cv', f'{col}_skew', f'{col}_kurtosis']:\n",
        "                        feature_dict.setdefault(feature_name, []).append(0)\n",
        "                    \n",
        "                    # 序列特定特征的默认值\n",
        "                    if col == 'ip_direction':\n",
        "                        feature_dict.setdefault(f'{col}_out_ratio', []).append(0)\n",
        "                    elif col == 'pkt_len':\n",
        "                        feature_dict.setdefault(f'{col}_small_pkt_ratio', []).append(0)\n",
        "                    elif col == 'iat':\n",
        "                        feature_dict.setdefault(f'{col}_burst_ratio', []).append(0)\n",
        "            \n",
        "            # 添加新特征到DataFrame\n",
        "            for feature_name, feature_values in feature_dict.items():\n",
        "                df_processed[feature_name] = feature_values\n",
        "            \n",
        "            # 删除原始序列列\n",
        "            df_processed = df_processed.drop(columns=[col])\n",
        "            \n",
        "            print(f\"  ✅ {col} -> 生成了 {len(feature_dict)} 个统计特征\")\n",
        "    \n",
        "    # 处理非数值列\n",
        "    print(\"\\n🔧 处理非数值特征...\")\n",
        "    label_encoders = {}\n",
        "    \n",
        "    for col in df_processed.columns:\n",
        "        if col != 'label' and df_processed[col].dtype == 'object':\n",
        "            try:\n",
        "                le = LabelEncoder()\n",
        "                df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
        "                label_encoders[col] = le\n",
        "                print(f\"  ✅ 标签编码: {col}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ 无法编码 {col}, 删除该列: {e}\")\n",
        "                df_processed = df_processed.drop(columns=[col])\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "\n",
        "# 🔧 执行数据预处理\n",
        "print(\"开始数据预处理...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 预处理训练数据\n",
        "print(\"📊 处理训练数据...\")\n",
        "train_processed = preprocess_data(train_data)\n",
        "train_processed = process_sequence_features(train_processed, ENABLE_SEQUENCE_FEATURES)\n",
        "\n",
        "# 预处理验证数据\n",
        "if len(val_data) > 0:\n",
        "    print(\"\\n📊 处理验证数据...\")\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    val_processed = process_sequence_features(val_processed, ENABLE_SEQUENCE_FEATURES)\n",
        "else:\n",
        "    val_processed = pd.DataFrame()\n",
        "\n",
        "# 预处理测试数据\n",
        "if len(test_data) > 0:\n",
        "    print(\"\\n📊 处理测试数据...\")\n",
        "    test_processed = preprocess_data(test_data)\n",
        "    test_processed = process_sequence_features(test_processed, ENABLE_SEQUENCE_FEATURES)\n",
        "else:\n",
        "    test_processed = pd.DataFrame()\n",
        "\n",
        "print(\"\\n✅ 数据预处理完成！\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 最终特征对齐和训练数据准备\n",
        "\n",
        "print(\"🔧 执行最终特征对齐...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. 检查预处理后的数据集状态\n",
        "print(\"📊 预处理后数据集状态:\")\n",
        "train_cols = list(train_processed.columns)\n",
        "val_cols = list(val_processed.columns) if len(val_processed) > 0 else []\n",
        "test_cols = list(test_processed.columns) if len(test_processed) > 0 else []\n",
        "\n",
        "print(f\"  训练集: {train_processed.shape} (列数: {len(train_cols)})\")\n",
        "if len(val_processed) > 0:\n",
        "    print(f\"  验证集: {val_processed.shape} (列数: {len(val_cols)})\")\n",
        "else:\n",
        "    print(f\"  验证集: 空数据集\")\n",
        "if len(test_processed) > 0:\n",
        "    print(f\"  测试集: {test_processed.shape} (列数: {len(test_cols)})\")\n",
        "else:\n",
        "    print(f\"  测试集: 空数据集\")\n",
        "\n",
        "# 2. 特征对齐\n",
        "active_datasets = [train_processed]\n",
        "active_col_sets = [set(train_cols)]\n",
        "\n",
        "if len(val_processed) > 0:\n",
        "    active_datasets.append(val_processed)\n",
        "    active_col_sets.append(set(val_cols))\n",
        "if len(test_processed) > 0:\n",
        "    active_datasets.append(test_processed)\n",
        "    active_col_sets.append(set(test_cols))\n",
        "\n",
        "# 计算共同特征\n",
        "if len(active_col_sets) > 1:\n",
        "    common_features = set.intersection(*active_col_sets)\n",
        "    common_feature_list = sorted(list(common_features))\n",
        "    \n",
        "    print(f\"\\n🔧 特征对齐结果:\")\n",
        "    print(f\"  共同特征数: {len(common_feature_list)}\")\n",
        "    \n",
        "    # 检查是否需要对齐\n",
        "    needs_alignment = False\n",
        "    for i, dataset in enumerate(active_datasets):\n",
        "        current_cols = set(dataset.columns)\n",
        "        if len(current_cols) != len(common_features):\n",
        "            needs_alignment = True\n",
        "            unique_cols = current_cols - common_features\n",
        "            if unique_cols:\n",
        "                dataset_name = [\"训练集\", \"验证集\", \"测试集\"][i]\n",
        "                print(f\"  {dataset_name}独有列: {sorted(list(unique_cols))}\")\n",
        "    \n",
        "    if needs_alignment:\n",
        "        print(f\"\\n🔧 执行特征对齐...\")\n",
        "        train_processed = train_processed[common_feature_list]\n",
        "        if len(val_processed) > 0:\n",
        "            val_processed = val_processed[common_feature_list]\n",
        "        if len(test_processed) > 0:\n",
        "            test_processed = test_processed[common_feature_list]\n",
        "        print(\"✅ 特征对齐完成\")\n",
        "    else:\n",
        "        print(\"✅ 所有数据集特征已经一致\")\n",
        "else:\n",
        "    common_feature_list = train_cols\n",
        "    print(\"✅ 只有训练集，无需对齐\")\n",
        "\n",
        "# 3. 处理空的验证/测试集\n",
        "if len(val_processed) == 0 or len(test_processed) == 0:\n",
        "    print(f\"\\n🔧 处理空数据集...\")\n",
        "    \n",
        "    # 从训练集中分割出验证集和测试集\n",
        "    X_temp = train_processed.drop(columns=['label'])\n",
        "    y_temp = train_processed['label']\n",
        "    \n",
        "    if len(val_processed) == 0 and len(test_processed) == 0:\n",
        "        # 两个都为空，分割为 70% 训练, 15% 验证, 15% 测试\n",
        "        X_train_split, X_temp_split, y_train_split, y_temp_split = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp)\n",
        "        X_val_split, X_test_split, y_val_split, y_test_split = train_test_split(\n",
        "            X_temp_split, y_temp_split, test_size=0.5, random_state=42, stratify=y_temp_split)\n",
        "        \n",
        "        train_processed = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "        val_processed = pd.concat([X_val_split, y_val_split], axis=1)\n",
        "        test_processed = pd.concat([X_test_split, y_test_split], axis=1)\n",
        "        \n",
        "        print(f\"  ✅ 分割数据: 训练{len(train_processed)} 验证{len(val_processed)} 测试{len(test_processed)}\")\n",
        "        \n",
        "    elif len(val_processed) == 0:\n",
        "        # 只有验证集为空，分割为 80% 训练, 20% 验证\n",
        "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "        \n",
        "        train_processed = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "        val_processed = pd.concat([X_val_split, y_val_split], axis=1)\n",
        "        \n",
        "        print(f\"  ✅ 生成验证集: 训练{len(train_processed)} 验证{len(val_processed)}\")\n",
        "        \n",
        "    elif len(test_processed) == 0:\n",
        "        # 只有测试集为空，分割为 80% 训练, 20% 测试\n",
        "        X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "        \n",
        "        train_processed = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "        test_processed = pd.concat([X_test_split, y_test_split], axis=1)\n",
        "        \n",
        "        print(f\"  ✅ 生成测试集: 训练{len(train_processed)} 测试{len(test_processed)}\")\n",
        "\n",
        "# 4. 准备最终训练数据\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"\\n📈 最终数据准备:\")\n",
        "print(f\"  特征数量: {len(feature_columns)}\")\n",
        "\n",
        "X_train = train_processed[feature_columns]\n",
        "y_train = train_processed['label']\n",
        "\n",
        "X_val = val_processed[feature_columns]\n",
        "y_val = val_processed['label']\n",
        "\n",
        "X_test = test_processed[feature_columns]\n",
        "y_test = test_processed['label']\n",
        "\n",
        "# 确保所有数据都是数值型\n",
        "X_train = X_train.select_dtypes(include=[np.number])\n",
        "X_val = X_val.select_dtypes(include=[np.number])\n",
        "X_test = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\n📊 最终数据形状:\")\n",
        "print(f\"  训练集: {X_train.shape}\")\n",
        "print(f\"  验证集: {X_val.shape}\")\n",
        "print(f\"  测试集: {X_test.shape}\")\n",
        "\n",
        "# 5. 数据质量检查\n",
        "print(f\"\\n🔍 数据质量检查:\")\n",
        "print(f\"  特征维度一致性: {X_train.shape[1] == X_val.shape[1] == X_test.shape[1]}\")\n",
        "print(f\"  训练集NaN: {X_train.isnull().any().any()}\")\n",
        "print(f\"  验证集NaN: {X_val.isnull().any().any()}\")\n",
        "print(f\"  测试集NaN: {X_test.isnull().any().any()}\")\n",
        "\n",
        "print(f\"\\n📊 标签分布:\")\n",
        "print(f\"  训练集: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"  验证集: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"  测试集: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "if X_train.shape[1] == X_val.shape[1] == X_test.shape[1]:\n",
        "    print(f\"\\n🎉 数据准备完成！所有数据集特征维度一致: {X_train.shape[1]}个特征\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ 特征维度不一致，需要进一步检查\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 保存特征名称以供后续使用\n",
        "feature_names = list(X_train.columns)\n",
        "print(f\"💾 已保存 {len(feature_names)} 个特征名称\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🤖 XGBoost模型训练（版本兼容）\n",
        "\n",
        "print(\"🤖 开始XGBoost模型训练...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 检查XGBoost版本\n",
        "import xgboost\n",
        "print(f\"XGBoost版本: {xgboost.__version__}\")\n",
        "\n",
        "# 创建输出目录\n",
        "output_dir = \"output\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"✅ 创建输出目录: {output_dir}\")\n",
        "\n",
        "# 创建XGBoost分类器 - 使用保守参数确保稳定性\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    verbosity=0  # 减少输出\n",
        ")\n",
        "\n",
        "# 🔧 版本兼容的训练策略（三级回退机制）\n",
        "training_success = False\n",
        "training_method = \"\"\n",
        "\n",
        "print(\"\\n🔧 开始智能训练（版本兼容）...\")\n",
        "\n",
        "try:\n",
        "    # 方法1: 新版本XGBoost方式（推荐）\n",
        "    print(\"🔄 尝试方法1: 新版本训练方式...\")\n",
        "    xgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "    training_success = True\n",
        "    training_method = \"新版本方式\"\n",
        "    print(\"✅ 方法1成功\")\n",
        "    \n",
        "except TypeError as e:\n",
        "    if \"early_stopping_rounds\" in str(e):\n",
        "        print(\"⚠️ 方法1失败：early_stopping_rounds参数问题\")\n",
        "        \n",
        "        try:\n",
        "            # 方法2: 在模型初始化时设置early_stopping_rounds\n",
        "            print(\"🔄 尝试方法2: 初始化时设置early_stopping...\")\n",
        "            xgb_model = xgb.XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                eval_metric='logloss',\n",
        "                early_stopping_rounds=10,\n",
        "                verbosity=0\n",
        "            )\n",
        "            xgb_model.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "            training_success = True\n",
        "            training_method = \"初始化early_stopping方式\"\n",
        "            print(\"✅ 方法2成功\")\n",
        "            \n",
        "        except Exception as e2:\n",
        "            print(f\"⚠️ 方法2失败: {e2}\")\n",
        "            \n",
        "            try:\n",
        "                # 方法3: 禁用early stopping，增加训练轮数\n",
        "                print(\"🔄 尝试方法3: 禁用early stopping...\")\n",
        "                xgb_model = xgb.XGBClassifier(\n",
        "                    n_estimators=150,  # 增加轮数补偿\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42,\n",
        "                    eval_metric='logloss',\n",
        "                    verbosity=0\n",
        "                )\n",
        "                xgb_model.fit(X_train, y_train)\n",
        "                training_success = True\n",
        "                training_method = \"无early stopping方式\"\n",
        "                print(\"✅ 方法3成功\")\n",
        "                \n",
        "            except Exception as e3:\n",
        "                print(f\"❌ 方法3也失败: {e3}\")\n",
        "                training_success = False\n",
        "    else:\n",
        "        print(f\"❌ 训练失败，未知错误: {e}\")\n",
        "        training_success = False\n",
        "\n",
        "if training_success:\n",
        "    print(f\"\\n🎉 模型训练成功！\")\n",
        "    print(f\"📋 使用方法: {training_method}\")\n",
        "    print(f\"📊 训练数据: {X_train.shape[0]}样本, {X_train.shape[1]}特征\")\n",
        "    print(f\"📊 验证数据: {X_val.shape[0]}样本\")\n",
        "    print(f\"📊 测试数据: {X_test.shape[0]}样本\")\n",
        "else:\n",
        "    raise RuntimeError(\"❌ 所有训练方法都失败了，请检查XGBoost版本和数据\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 模型评估和性能分析\n",
        "\n",
        "print(\"📊 开始模型评估...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 模型预测\n",
        "print(\"🔄 执行预测...\")\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_train_pred_proba = xgb_model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "y_val_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "y_test_pred = xgb_model.predict(X_test)\n",
        "y_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"✅ 预测完成\")\n",
        "\n",
        "# 计算性能指标\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_pred_proba, dataset_name):\n",
        "    \"\"\"计算并显示性能指标\"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # 基础分类指标\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred)\n",
        "    metrics['recall'] = recall_score(y_true, y_pred)\n",
        "    metrics['f1'] = f1_score(y_true, y_pred)\n",
        "    metrics['auc'] = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    print(f\"\\n📋 {dataset_name}性能指标:\")\n",
        "    print(f\"  准确率 (Accuracy):  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  精确率 (Precision): {metrics['precision']:.4f}\")\n",
        "    print(f\"  召回率 (Recall):    {metrics['recall']:.4f}\")\n",
        "    print(f\"  F1分数 (F1-Score):  {metrics['f1']:.4f}\")\n",
        "    print(f\"  AUC分数:           {metrics['auc']:.4f}\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# 计算各数据集的性能指标\n",
        "train_metrics = calculate_metrics(y_train, y_train_pred, y_train_pred_proba, \"训练集\")\n",
        "val_metrics = calculate_metrics(y_val, y_val_pred, y_val_pred_proba, \"验证集\")\n",
        "test_metrics = calculate_metrics(y_test, y_test_pred, y_test_pred_proba, \"测试集\")\n",
        "\n",
        "# 检查过拟合情况\n",
        "print(f\"\\n🔍 过拟合检查:\")\n",
        "train_test_auc_diff = train_metrics['auc'] - test_metrics['auc']\n",
        "train_test_acc_diff = train_metrics['accuracy'] - test_metrics['accuracy']\n",
        "\n",
        "print(f\"  训练集 vs 测试集 AUC差异: {train_test_auc_diff:.4f}\")\n",
        "print(f\"  训练集 vs 测试集 准确率差异: {train_test_acc_diff:.4f}\")\n",
        "\n",
        "if train_test_auc_diff > 0.1 or train_test_acc_diff > 0.1:\n",
        "    print(\"  ⚠️ 可能存在过拟合，建议调整模型参数\")\n",
        "elif train_test_auc_diff < 0.02 and train_test_acc_diff < 0.02:\n",
        "    print(\"  ✅ 模型泛化能力良好\")\n",
        "else:\n",
        "    print(\"  ✅ 模型性能正常\")\n",
        "\n",
        "# 详细分类报告\n",
        "print(f\"\\n📈 详细分类报告 (测试集):\")\n",
        "test_report = classification_report(y_test, y_test_pred, target_names=['正常流量', 'PCDN流量'])\n",
        "print(test_report)\n",
        "\n",
        "# 创建性能总结字典\n",
        "performance_summary = {\n",
        "    'training_method': training_method,\n",
        "    'feature_count': X_train.shape[1],\n",
        "    'train_samples': X_train.shape[0],\n",
        "    'val_samples': X_val.shape[0],\n",
        "    'test_samples': X_test.shape[0],\n",
        "    'train_metrics': train_metrics,\n",
        "    'val_metrics': val_metrics,\n",
        "    'test_metrics': test_metrics,\n",
        "    'sequence_features_enabled': ENABLE_SEQUENCE_FEATURES\n",
        "}\n",
        "\n",
        "print(f\"\\n💾 性能总结已保存到变量 'performance_summary'\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 特征重要性分析和可视化\n",
        "\n",
        "print(\"🎯 开始特征重要性分析...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 获取特征重要性\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"📊 总特征数: {len(feature_names)}\")\n",
        "print(f\"📈 重要性分析完成\")\n",
        "\n",
        "# 显示前20个最重要的特征\n",
        "print(f\"\\n🔝 前20个最重要特征:\")\n",
        "top_features = feature_importance_df.head(20)\n",
        "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
        "    print(f\"  {i:2d}. {row['feature']:30} | 重要性: {row['importance']:.4f}\")\n",
        "\n",
        "# 分析序列特征vs非序列特征的重要性\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(f\"\\n🔍 序列特征 vs 原始特征重要性分析:\")\n",
        "    \n",
        "    sequence_keywords = ['pkt_len_', 'ip_direction_', 'iat_']\n",
        "    sequence_features = []\n",
        "    original_features = []\n",
        "    \n",
        "    for _, row in feature_importance_df.iterrows():\n",
        "        is_sequence = any(keyword in row['feature'] for keyword in sequence_keywords)\n",
        "        if is_sequence:\n",
        "            sequence_features.append(row['importance'])\n",
        "        else:\n",
        "            original_features.append(row['importance'])\n",
        "    \n",
        "    seq_avg_importance = np.mean(sequence_features) if sequence_features else 0\n",
        "    orig_avg_importance = np.mean(original_features) if original_features else 0\n",
        "    \n",
        "    print(f\"  序列特征数量: {len(sequence_features)}\")\n",
        "    print(f\"  原始特征数量: {len(original_features)}\")\n",
        "    print(f\"  序列特征平均重要性: {seq_avg_importance:.4f}\")\n",
        "    print(f\"  原始特征平均重要性: {orig_avg_importance:.4f}\")\n",
        "    \n",
        "    if seq_avg_importance > orig_avg_importance:\n",
        "        print(f\"  💡 序列特征工程显著提升了模型性能！\")\n",
        "    else:\n",
        "        print(f\"  💡 原始特征仍然是主要的预测因子\")\n",
        "\n",
        "# 创建特征重要性可视化\n",
        "print(f\"\\n📊 生成特征重要性图表...\")\n",
        "\n",
        "# 设置图表\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "fig.suptitle('XGBoost Model Analysis - Feature Importance & Performance', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. 前15个最重要特征条形图\n",
        "ax1 = axes[0, 0]\n",
        "top_15_features = feature_importance_df.head(15)\n",
        "bars = ax1.barh(range(len(top_15_features)), top_15_features['importance'], color='skyblue')\n",
        "ax1.set_yticks(range(len(top_15_features)))\n",
        "ax1.set_yticklabels(top_15_features['feature'], fontsize=9)\n",
        "ax1.set_xlabel('Feature Importance')\n",
        "ax1.set_title('Top 15 Most Important Features')\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 添加数值标签\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
        "             f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
        "\n",
        "# 2. 特征重要性分布直方图\n",
        "ax2 = axes[0, 1]\n",
        "ax2.hist(feature_importance, bins=30, color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "ax2.set_xlabel('Feature Importance')\n",
        "ax2.set_ylabel('Number of Features')\n",
        "ax2.set_title('Distribution of Feature Importance')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# 添加统计信息\n",
        "mean_importance = np.mean(feature_importance)\n",
        "median_importance = np.median(feature_importance)\n",
        "ax2.axvline(mean_importance, color='red', linestyle='--', label=f'Mean: {mean_importance:.4f}')\n",
        "ax2.axvline(median_importance, color='blue', linestyle='--', label=f'Median: {median_importance:.4f}')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. 序列特征vs原始特征对比（如果启用了序列特征）\n",
        "ax3 = axes[1, 0]\n",
        "if ENABLE_SEQUENCE_FEATURES and sequence_features and original_features:\n",
        "    # 箱线图比较\n",
        "    data_to_plot = [sequence_features, original_features]\n",
        "    bp = ax3.boxplot(data_to_plot, labels=['Sequence Features', 'Original Features'], patch_artist=True)\n",
        "    bp['boxes'][0].set_facecolor('lightcoral')\n",
        "    bp['boxes'][1].set_facecolor('lightblue')\n",
        "    ax3.set_ylabel('Feature Importance')\n",
        "    ax3.set_title('Sequence vs Original Features Importance')\n",
        "    ax3.grid(alpha=0.3)\n",
        "else:\n",
        "    # 如果没有序列特征，显示前10个特征的饼图\n",
        "    top_10 = feature_importance_df.head(10)\n",
        "    other_importance = feature_importance_df.iloc[10:]['importance'].sum()\n",
        "    \n",
        "    labels = list(top_10['feature']) + ['Others']\n",
        "    sizes = list(top_10['importance']) + [other_importance]\n",
        "    \n",
        "    ax3.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "    ax3.set_title('Top 10 Features Contribution')\n",
        "\n",
        "# 4. 累积重要性曲线\n",
        "ax4 = axes[1, 1]\n",
        "cumulative_importance = np.cumsum(feature_importance_df['importance'])\n",
        "ax4.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\n",
        "ax4.set_xlabel('Number of Features')\n",
        "ax4.set_ylabel('Cumulative Importance')\n",
        "ax4.set_title('Cumulative Feature Importance')\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "# 添加关键阈值线\n",
        "threshold_80 = np.where(cumulative_importance >= 0.8)[0]\n",
        "threshold_90 = np.where(cumulative_importance >= 0.9)[0]\n",
        "\n",
        "if len(threshold_80) > 0:\n",
        "    ax4.axvline(threshold_80[0] + 1, color='orange', linestyle='--', \n",
        "                label=f'80% importance: {threshold_80[0] + 1} features')\n",
        "if len(threshold_90) > 0:\n",
        "    ax4.axvline(threshold_90[0] + 1, color='red', linestyle='--', \n",
        "                label=f'90% importance: {threshold_90[0] + 1} features')\n",
        "\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 保存特征重要性数据\n",
        "feature_importance_summary = {\n",
        "    'top_20_features': top_features.to_dict('records'),\n",
        "    'total_features': len(feature_names),\n",
        "    'sequence_features_count': len(sequence_features) if ENABLE_SEQUENCE_FEATURES else 0,\n",
        "    'original_features_count': len(original_features) if ENABLE_SEQUENCE_FEATURES else len(feature_names),\n",
        "    'mean_importance': float(mean_importance),\n",
        "    'median_importance': float(median_importance)\n",
        "}\n",
        "\n",
        "print(f\"\\n💾 特征重要性分析已保存到变量 'feature_importance_summary'\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📈 ROC曲线和混淆矩阵可视化\n",
        "\n",
        "print(\"📈 生成ROC曲线和混淆矩阵...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 创建综合可视化图表\n",
        "fig, axes = plt.subplots(2, 3, figsize=(21, 14))\n",
        "fig.suptitle('XGBoost Model Performance Visualization', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. ROC曲线 - 训练集、验证集、测试集对比\n",
        "ax1 = axes[0, 0]\n",
        "\n",
        "# 计算ROC曲线数据\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
        "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_pred_proba)\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_proba)\n",
        "\n",
        "# 绘制ROC曲线\n",
        "ax1.plot(fpr_train, tpr_train, 'b-', label=f'Training (AUC = {train_metrics[\"auc\"]:.3f})', linewidth=2)\n",
        "ax1.plot(fpr_val, tpr_val, 'g-', label=f'Validation (AUC = {val_metrics[\"auc\"]:.3f})', linewidth=2)\n",
        "ax1.plot(fpr_test, tpr_test, 'r-', label=f'Test (AUC = {test_metrics[\"auc\"]:.3f})', linewidth=2)\n",
        "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC = 0.5)')\n",
        "\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate')\n",
        "ax1.set_title('ROC Curves Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. 训练集混淆矩阵\n",
        "ax2 = axes[0, 1]\n",
        "train_cm = confusion_matrix(y_train, y_train_pred)\n",
        "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
        "            xticklabels=['Normal', 'PCDN'], yticklabels=['Normal', 'PCDN'])\n",
        "ax2.set_title('Training Set - Confusion Matrix')\n",
        "ax2.set_xlabel('Predicted')\n",
        "ax2.set_ylabel('Actual')\n",
        "\n",
        "# 3. 验证集混淆矩阵\n",
        "ax3 = axes[0, 2]\n",
        "val_cm = confusion_matrix(y_val, y_val_pred)\n",
        "sns.heatmap(val_cm, annot=True, fmt='d', cmap='Greens', ax=ax3,\n",
        "            xticklabels=['Normal', 'PCDN'], yticklabels=['Normal', 'PCDN'])\n",
        "ax3.set_title('Validation Set - Confusion Matrix')\n",
        "ax3.set_xlabel('Predicted')\n",
        "ax3.set_ylabel('Actual')\n",
        "\n",
        "# 4. 测试集混淆矩阵\n",
        "ax4 = axes[1, 0]\n",
        "test_cm = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Reds', ax=ax4,\n",
        "            xticklabels=['Normal', 'PCDN'], yticklabels=['Normal', 'PCDN'])\n",
        "ax4.set_title('Test Set - Confusion Matrix')\n",
        "ax4.set_xlabel('Predicted')\n",
        "ax4.set_ylabel('Actual')\n",
        "\n",
        "# 5. 性能指标对比\n",
        "ax5 = axes[1, 1]\n",
        "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
        "train_values = [train_metrics['accuracy'], train_metrics['precision'], \n",
        "                train_metrics['recall'], train_metrics['f1'], train_metrics['auc']]\n",
        "val_values = [val_metrics['accuracy'], val_metrics['precision'], \n",
        "              val_metrics['recall'], val_metrics['f1'], val_metrics['auc']]\n",
        "test_values = [test_metrics['accuracy'], test_metrics['precision'], \n",
        "               test_metrics['recall'], test_metrics['f1'], test_metrics['auc']]\n",
        "\n",
        "x = np.arange(len(metrics_names))\n",
        "width = 0.25\n",
        "\n",
        "ax5.bar(x - width, train_values, width, label='Training', color='skyblue', alpha=0.8)\n",
        "ax5.bar(x, val_values, width, label='Validation', color='lightgreen', alpha=0.8)\n",
        "ax5.bar(x + width, test_values, width, label='Test', color='lightcoral', alpha=0.8)\n",
        "\n",
        "ax5.set_xlabel('Metrics')\n",
        "ax5.set_ylabel('Score')\n",
        "ax5.set_title('Performance Metrics Comparison')\n",
        "ax5.set_xticks(x)\n",
        "ax5.set_xticklabels(metrics_names, rotation=45)\n",
        "ax5.legend()\n",
        "ax5.grid(axis='y', alpha=0.3)\n",
        "ax5.set_ylim(0, 1.1)\n",
        "\n",
        "# 添加数值标签\n",
        "for i, (train_val, val_val, test_val) in enumerate(zip(train_values, val_values, test_values)):\n",
        "    ax5.text(i - width, train_val + 0.01, f'{train_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    ax5.text(i, val_val + 0.01, f'{val_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    ax5.text(i + width, test_val + 0.01, f'{test_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 6. 预测概率分布\n",
        "ax6 = axes[1, 2]\n",
        "\n",
        "# 分别获取正常流量和PCDN流量的预测概率\n",
        "normal_proba = y_test_pred_proba[y_test == 0]\n",
        "pcdn_proba = y_test_pred_proba[y_test == 1]\n",
        "\n",
        "ax6.hist(normal_proba, bins=30, alpha=0.6, label='Normal Traffic', color='blue', density=True)\n",
        "ax6.hist(pcdn_proba, bins=30, alpha=0.6, label='PCDN Traffic', color='red', density=True)\n",
        "ax6.set_xlabel('Predicted Probability (PCDN)')\n",
        "ax6.set_ylabel('Density')\n",
        "ax6.set_title('Prediction Probability Distribution')\n",
        "ax6.legend()\n",
        "ax6.grid(alpha=0.3)\n",
        "\n",
        "# 添加决策阈值线（默认0.5）\n",
        "ax6.axvline(0.5, color='green', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 计算详细的混淆矩阵统计\n",
        "def analyze_confusion_matrix(cm, dataset_name):\n",
        "    \"\"\"分析混淆矩阵并返回详细统计\"\"\"\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    # 计算各种率\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # 真正率/召回率\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # 真负率\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0         # 精确率/阳性预测值\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0         # 阴性预测值\n",
        "    \n",
        "    return {\n",
        "        'dataset': dataset_name,\n",
        "        'true_negative': int(tn),\n",
        "        'false_positive': int(fp),\n",
        "        'false_negative': int(fn),\n",
        "        'true_positive': int(tp),\n",
        "        'sensitivity_recall': sensitivity,\n",
        "        'specificity': specificity,\n",
        "        'precision_ppv': ppv,\n",
        "        'negative_predictive_value': npv\n",
        "    }\n",
        "\n",
        "# 分析各数据集的混淆矩阵\n",
        "print(f\"\\n📊 详细混淆矩阵分析:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cm_analysis = {\n",
        "    'train': analyze_confusion_matrix(train_cm, \"训练集\"),\n",
        "    'val': analyze_confusion_matrix(val_cm, \"验证集\"),\n",
        "    'test': analyze_confusion_matrix(test_cm, \"测试集\")\n",
        "}\n",
        "\n",
        "for dataset, analysis in cm_analysis.items():\n",
        "    print(f\"\\n📋 {analysis['dataset']}:\")\n",
        "    print(f\"  真负例 (TN): {analysis['true_negative']:4d} | 假正例 (FP): {analysis['false_positive']:4d}\")\n",
        "    print(f\"  假负例 (FN): {analysis['false_negative']:4d} | 真正例 (TP): {analysis['true_positive']:4d}\")\n",
        "    print(f\"  敏感性/召回率: {analysis['sensitivity_recall']:.4f}\")\n",
        "    print(f\"  特异性:       {analysis['specificity']:.4f}\")\n",
        "    print(f\"  精确率:       {analysis['precision_ppv']:.4f}\")\n",
        "    print(f\"  阴性预测值:   {analysis['negative_predictive_value']:.4f}\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💾 结果保存和项目总结\n",
        "\n",
        "print(\"💾 保存模型和分析结果...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# 创建时间戳\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 1. 保存XGBoost模型\n",
        "try:\n",
        "    model_path = os.path.join(output_dir, f\"xgboost_model_{timestamp}.pkl\")\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"✅ 模型已保存: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ 模型保存失败: {e}\")\n",
        "\n",
        "# 2. 保存特征重要性\n",
        "try:\n",
        "    feature_importance_path = os.path.join(output_dir, f\"feature_importance_{timestamp}.csv\")\n",
        "    feature_importance_df.to_csv(feature_importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 特征重要性已保存: {feature_importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ 特征重要性保存失败: {e}\")\n",
        "\n",
        "# 3. 保存完整性能报告\n",
        "performance_report = {\n",
        "    'timestamp': timestamp,\n",
        "    'experiment_info': {\n",
        "        'sequence_features_enabled': ENABLE_SEQUENCE_FEATURES,\n",
        "        'training_method': training_method,\n",
        "        'xgboost_version': xgboost.__version__,\n",
        "        'feature_count': len(feature_names),\n",
        "        'data_samples': {\n",
        "            'train': X_train.shape[0],\n",
        "            'validation': X_val.shape[0],\n",
        "            'test': X_test.shape[0]\n",
        "        }\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'train': train_metrics,\n",
        "        'validation': val_metrics,\n",
        "        'test': test_metrics\n",
        "    },\n",
        "    'feature_analysis': feature_importance_summary,\n",
        "    'confusion_matrix_analysis': cm_analysis,\n",
        "    'model_hyperparameters': {\n",
        "        'n_estimators': xgb_model.n_estimators,\n",
        "        'max_depth': xgb_model.max_depth,\n",
        "        'learning_rate': xgb_model.learning_rate,\n",
        "        'subsample': xgb_model.subsample,\n",
        "        'colsample_bytree': xgb_model.colsample_bytree\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    report_path = os.path.join(output_dir, f\"performance_report_{timestamp}.json\")\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(performance_report, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"✅ 性能报告已保存: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ 性能报告保存失败: {e}\")\n",
        "\n",
        "# 4. 保存预测结果\n",
        "try:\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'actual': y_test,\n",
        "        'predicted': y_test_pred,\n",
        "        'probability_pcdn': y_test_pred_proba,\n",
        "        'correct': y_test == y_test_pred\n",
        "    })\n",
        "    predictions_path = os.path.join(output_dir, f\"test_predictions_{timestamp}.csv\")\n",
        "    predictions_df.to_csv(predictions_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 测试集预测结果已保存: {predictions_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ 预测结果保存失败: {e}\")\n",
        "\n",
        "# 5. 生成项目总结报告\n",
        "print(f\"\\n📋 项目执行总结:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"🎯 **实验配置**\")\n",
        "print(f\"  序列特征处理: {'启用' if ENABLE_SEQUENCE_FEATURES else '禁用'}\")\n",
        "print(f\"  XGBoost版本: {xgboost.__version__}\")\n",
        "print(f\"  训练方式: {training_method}\")\n",
        "print(f\"  特征总数: {len(feature_names)}\")\n",
        "\n",
        "print(f\"\\n📊 **数据统计**\")\n",
        "print(f\"  训练样本: {X_train.shape[0]:,}\")\n",
        "print(f\"  验证样本: {X_val.shape[0]:,}\")\n",
        "print(f\"  测试样本: {X_test.shape[0]:,}\")\n",
        "\n",
        "print(f\"\\n🏆 **最佳性能 (测试集)**\")\n",
        "print(f\"  准确率: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  精确率: {test_metrics['precision']:.4f}\")\n",
        "print(f\"  召回率: {test_metrics['recall']:.4f}\")\n",
        "print(f\"  F1分数: {test_metrics['f1']:.4f}\")\n",
        "print(f\"  AUC分数: {test_metrics['auc']:.4f}\")\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(f\"\\n🔧 **序列特征工程效果**\")\n",
        "    print(f\"  生成序列特征数: {feature_importance_summary['sequence_features_count']}\")\n",
        "    print(f\"  原始特征数: {feature_importance_summary['original_features_count']}\")\n",
        "    \n",
        "    # 统计前10重要特征中序列特征的比例\n",
        "    top_10_features = feature_importance_df.head(10)['feature'].tolist()\n",
        "    sequence_in_top10 = sum(1 for feat in top_10_features \n",
        "                           if any(keyword in feat for keyword in ['pkt_len_', 'ip_direction_', 'iat_']))\n",
        "    print(f\"  前10重要特征中序列特征占比: {sequence_in_top10}/10 ({sequence_in_top10*10:.0f}%)\")\n",
        "\n",
        "print(f\"\\n📁 **输出文件**\")\n",
        "output_files = [f for f in os.listdir(output_dir) if timestamp in f]\n",
        "for file in sorted(output_files):\n",
        "    print(f\"  {file}\")\n",
        "\n",
        "print(f\"\\n💡 **关键发现**\")\n",
        "# 自动生成关键发现\n",
        "findings = []\n",
        "\n",
        "# 性能分析\n",
        "if test_metrics['auc'] >= 0.95:\n",
        "    findings.append(\"🌟 模型性能优秀 (AUC ≥ 0.95)\")\n",
        "elif test_metrics['auc'] >= 0.90:\n",
        "    findings.append(\"✅ 模型性能良好 (AUC ≥ 0.90)\")\n",
        "elif test_metrics['auc'] >= 0.80:\n",
        "    findings.append(\"📈 模型性能中等 (AUC ≥ 0.80)\")\n",
        "else:\n",
        "    findings.append(\"⚠️ 模型性能需要改进 (AUC < 0.80)\")\n",
        "\n",
        "# 泛化能力分析\n",
        "train_test_auc_diff = train_metrics['auc'] - test_metrics['auc']\n",
        "if train_test_auc_diff <= 0.02:\n",
        "    findings.append(\"🎯 模型泛化能力强，无过拟合\")\n",
        "elif train_test_auc_diff <= 0.05:\n",
        "    findings.append(\"✅ 模型泛化能力良好\")\n",
        "else:\n",
        "    findings.append(\"⚠️ 存在过拟合风险，建议调整模型复杂度\")\n",
        "\n",
        "# 序列特征价值分析\n",
        "if ENABLE_SEQUENCE_FEATURES and sequence_in_top10 >= 5:\n",
        "    findings.append(\"🚀 序列特征工程非常有效，显著提升了模型性能\")\n",
        "elif ENABLE_SEQUENCE_FEATURES and sequence_in_top10 >= 3:\n",
        "    findings.append(\"📊 序列特征工程有效，为模型提供了有价值的信息\")\n",
        "\n",
        "# 数据质量分析\n",
        "total_samples = X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
        "if total_samples >= 10000:\n",
        "    findings.append(\"📈 数据样本充足，模型训练稳定\")\n",
        "elif total_samples >= 5000:\n",
        "    findings.append(\"📊 数据样本适中，结果可信度良好\")\n",
        "else:\n",
        "    findings.append(\"⚠️ 数据样本较少，建议增加数据量\")\n",
        "\n",
        "for i, finding in enumerate(findings, 1):\n",
        "    print(f\"  {i}. {finding}\")\n",
        "\n",
        "print(f\"\\n🎉 **项目完成状态**\")\n",
        "print(\"✅ 数据加载和特征工程完成\")\n",
        "print(\"✅ 模型训练和版本兼容处理完成\")\n",
        "print(\"✅ 性能评估和可视化完成\")\n",
        "print(\"✅ 特征重要性分析完成\")\n",
        "print(\"✅ 结果保存和报告生成完成\")\n",
        "\n",
        "print(f\"\\n💼 **后续建议**\")\n",
        "print(\"1. 可以尝试调整XGBoost超参数进一步优化性能\")\n",
        "print(\"2. 考虑集成其他算法(如Random Forest, LightGBM)进行模型融合\")\n",
        "print(\"3. 深入分析错误预测案例，优化特征工程\")\n",
        "if not ENABLE_SEQUENCE_FEATURES:\n",
        "    print(\"4. 建议尝试启用序列特征工程以提升性能\")\n",
        "print(\"5. 在更大的数据集上验证模型的稳定性\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"🎊 PCDN vs Normal Traffic Classification 项目执行完成！\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bysj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
