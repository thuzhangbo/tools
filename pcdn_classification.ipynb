{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'bysj (Python 3.10.18)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n bysj ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDN vs Normal Traffic Classification using XGBoost\n",
        "\n",
        "这个项目使用XGBoost对正常流量和PCDN流量进行二分类。\n",
        "\n",
        "## 数据集结构\n",
        "- Training_set/APP_0: 正常流量\n",
        "- Training_set/APP_1: PCDN流量\n",
        "- Validation_set: 验证集\n",
        "- Testing_set: 测试集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from scipy import stats  # 用于序列特征的偏度和峰度计算\n",
        "import ast  # 用于安全解析数组字符串\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置中文字体支持\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"库导入完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据加载函数\n",
        "def load_data_from_directory(base_path, label):\n",
        "    \"\"\"\n",
        "    从指定目录加载所有CSV文件并添加标签\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(base_path, '*.csv'))\n",
        "    dataframes = []\n",
        "    \n",
        "    for file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df['label'] = label  # 添加标签列\n",
        "            df['source_file'] = os.path.basename(file)  # 添加源文件信息\n",
        "            dataframes.append(df)\n",
        "            print(f\"成功加载文件: {file}, 样本数: {len(df)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"加载文件 {file} 时出错: {e}\")\n",
        "    \n",
        "    if dataframes:\n",
        "        return pd.concat(dataframes, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# 使用相对路径定义数据目录\n",
        "# 数据集应该与此notebook在同一目录下\n",
        "data_dir = './pcdn_32_pkts_2class_feature_enhance_v17.4_dataset'\n",
        "\n",
        "# 检查数据目录是否存在\n",
        "if not os.path.exists(data_dir):\n",
        "    print(f\"❌ 数据目录不存在: {data_dir}\")\n",
        "    print(\"请确保数据集文件夹与notebook在同一目录下\")\n",
        "    print(\"当前工作目录:\", os.getcwd())\n",
        "    print(\"当前目录内容:\", [f for f in os.listdir('.') if not f.startswith('.')])\n",
        "    \n",
        "    # 尝试查找数据目录\n",
        "    possible_dirs = [d for d in os.listdir('.') if 'pcdn' in d.lower() and os.path.isdir(d)]\n",
        "    if possible_dirs:\n",
        "        print(f\"发现可能的数据目录: {possible_dirs}\")\n",
        "        data_dir = possible_dirs[0]\n",
        "        print(f\"使用数据目录: {data_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"找不到数据目录，请检查数据集位置\")\n",
        "else:\n",
        "    print(f\"✅ 找到数据目录: {data_dir}\")\n",
        "\n",
        "# 加载训练数据\n",
        "print(\"\\n开始加载训练数据...\")\n",
        "train_normal = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)  # 正常流量标签为0\n",
        "train_pcdn = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)    # PCDN流量标签为1\n",
        "\n",
        "# 合并训练数据\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "print(f\"\\n训练数据加载完成！\")\n",
        "print(f\"正常流量样本数: {len(train_normal)}\")\n",
        "print(f\"PCDN流量样本数: {len(train_pcdn)}\")\n",
        "print(f\"总训练样本数: {len(train_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载验证和测试数据\n",
        "print(\"开始加载验证数据...\")\n",
        "val_normal = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查验证数据是否为空\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"⚠️ 警告: 验证集为空，将使用训练集的一部分作为验证集\")\n",
        "    val_data = pd.DataFrame()\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "\n",
        "print(\"\\n开始加载测试数据...\")\n",
        "test_normal = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# 检查测试数据是否为空\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"⚠️ 警告: 测试集为空，将使用训练集的一部分作为测试集\")\n",
        "    test_data = pd.DataFrame()\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "\n",
        "print(f\"\\n验证集样本数: {len(val_data)} (正常: {len(val_normal)}, PCDN: {len(val_pcdn)})\")\n",
        "print(f\"测试集样本数: {len(test_data)} (正常: {len(test_normal)}, PCDN: {len(test_pcdn)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据探索和基本信息\n",
        "print(\"=== 训练数据基本信息 ===\")\n",
        "print(f\"数据形状: {train_data.shape}\")\n",
        "print(f\"\\n列名 ({len(train_data.columns)}个特征):\")\n",
        "print(train_data.columns.tolist())\n",
        "\n",
        "print(\"\\n=== 标签分布 ===\")\n",
        "label_counts = train_data['label'].value_counts()\n",
        "print(label_counts)\n",
        "print(f\"正常流量比例: {label_counts[0]/len(train_data)*100:.2f}%\")\n",
        "print(f\"PCDN流量比例: {label_counts[1]/len(train_data)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== 数据类型信息 ===\")\n",
        "print(train_data.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据预处理\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    数据预处理函数\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # 删除指定的不需要的列\n",
        "    columns_to_drop = [\n",
        "        'source_file',  # 源文件信息（添加的辅助列）\n",
        "        'frame.number', # 帧编号\n",
        "        'frame.time_relative', # 相对时间\n",
        "        'ip.version',   # IP版本\n",
        "        'ip.ttl',       # IP TTL\n",
        "        'ip.src', 'ip.dst',  # IP地址\n",
        "        'ipv6.plen',    # IPv6 payload长度\n",
        "        'ipv6.nxt',     # IPv6 下一个头\n",
        "        'ipv6.src', 'ipv6.dst',  # IPv6地址\n",
        "        '_ws.col.Protocol', # Wireshark协议列\n",
        "        'ssl.handshake.extensions_server_name',  # SSL扩展信息\n",
        "        'eth.src',      # MAC地址\n",
        "        'pcap_duration', # PCAP持续时间\n",
        "        'app',          # 应用程序\n",
        "        'os',           # 操作系统\n",
        "        'date',         # 日期\n",
        "        'flow_id',      # 流ID\n",
        "        'dpi_file_name', # DPI文件名\n",
        "        'dpi_five_tuple', # 五元组\n",
        "        'dpi_rule_result', # DPI规则结果\n",
        "        'dpi_label',    # DPI标签\n",
        "        'ulProtoID',    # 上层协议ID\n",
        "        'dpi_rule_pkt', # DPI规则包\n",
        "        'dpi_packets',  # DPI包数\n",
        "        'dpi_bytes',    # DPI字节数\n",
        "        'label_source', # 标签源\n",
        "        'id',           # ID字段\n",
        "        'category'      # category字段\n",
        "    ]\n",
        "    \n",
        "    # 删除存在的列\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
        "    \n",
        "    # 保留所有其他特征（包括数组特征），这些可能对分类有用\n",
        "    # 数组特征如 ip_direction, pkt_len, iat 等将在后续步骤中进行编码处理\n",
        "    \n",
        "    # 处理缺失值\n",
        "    df_processed = df_processed.fillna(0)\n",
        "    \n",
        "    # 处理无穷大值\n",
        "    df_processed = df_processed.replace([np.inf, -np.inf], 0)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# 预处理训练数据\n",
        "train_processed = preprocess_data(train_data)\n",
        "\n",
        "# 智能处理空数据集的情况\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "val_exists = len(val_data) > 0\n",
        "test_exists = len(test_data) > 0\n",
        "\n",
        "print(f\"验证集存在: {'是' if val_exists else '否'}\")\n",
        "print(f\"测试集存在: {'是' if test_exists else '否'}\")\n",
        "\n",
        "if not val_exists and not test_exists:\n",
        "    # 两个数据集都为空，进行60/20/20分割\n",
        "    print(\"验证集和测试集都为空，从训练数据分割为 60% 训练 / 20% 验证 / 20% 测试\")\n",
        "    train_temp, temp_split = train_test_split(\n",
        "        train_processed, test_size=0.4, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    val_processed, test_processed = train_test_split(\n",
        "        temp_split, test_size=0.5, random_state=42, \n",
        "        stratify=temp_split['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    \n",
        "elif not val_exists:\n",
        "    # 只有验证集为空，分割80/20\n",
        "    print(\"验证集为空，从训练数据分割为 80% 训练 / 20% 验证\")\n",
        "    train_temp, val_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    test_processed = preprocess_data(test_data)\n",
        "    \n",
        "elif not test_exists:\n",
        "    # 只有测试集为空，分割80/20\n",
        "    print(\"测试集为空，从训练数据分割为 80% 训练 / 20% 测试\")\n",
        "    train_temp, test_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    \n",
        "else:\n",
        "    # 两个数据集都存在，直接使用\n",
        "    print(\"使用原始的验证集和测试集\")\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    test_processed = preprocess_data(test_data)\n",
        "\n",
        "print(f\"预处理后训练数据形状: {train_processed.shape}\")\n",
        "print(f\"预处理后验证数据形状: {val_processed.shape}\")\n",
        "print(f\"预处理后测试数据形状: {test_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征工程和数据预处理\n",
        "# 提取特征和标签\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"预处理后特征数量: {len(feature_columns)}\")\n",
        "print(f\"特征列表前10个: {feature_columns[:10]}\")\n",
        "\n",
        "# 处理序列特征 - 这些字段包含网络包的时序信息\n",
        "def process_sequence_features(df):\n",
        "    \"\"\"\n",
        "    处理序列类型的特征 (pkt_len, ip_direction, iat)\n",
        "    \n",
        "    对于XGBoost这种基于树的算法，需要将序列数据转换为有意义的统计特征：\n",
        "    1. pkt_len: 包长度序列 - 反映流量的数据传输模式\n",
        "    2. ip_direction: IP方向序列 - 反映通信的方向模式  \n",
        "    3. iat: 包间到达时间间隔序列 - 反映流量的时间特征\n",
        "    \n",
        "    XGBoost无法直接处理变长序列，需要提取固定维度的特征\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    # 定义序列特征及其含义\n",
        "    sequence_columns = {\n",
        "        'ip_direction': '网络包方向序列 (0=出站, 1=入站)',\n",
        "        'pkt_len': '网络包长度序列',\n",
        "        'iat': '包间到达时间间隔序列'\n",
        "    }\n",
        "    \n",
        "    for col, description in sequence_columns.items():\n",
        "        if col in df_copy.columns:\n",
        "            print(f\"处理序列特征: {col} - {description}\")\n",
        "            \n",
        "            try:\n",
        "                # 安全地将字符串转换为数值列表（避免使用eval）\n",
        "                def safe_parse_array(x):\n",
        "                    \"\"\"安全解析数组字符串\"\"\"\n",
        "                    if pd.isna(x) or x == '' or x == '[]':\n",
        "                        return []\n",
        "                    if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "                        try:\n",
        "                            # 使用ast.literal_eval替代eval，更安全\n",
        "                            return ast.literal_eval(x)\n",
        "                        except (ValueError, SyntaxError):\n",
        "                            return []\n",
        "                    return []\n",
        "                \n",
        "                sequences = df_copy[col].apply(safe_parse_array)\n",
        "                \n",
        "                # === 基础统计特征 ===\n",
        "                df_copy[f'{col}_mean'] = sequences.apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_std'] = sequences.apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_min'] = sequences.apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_max'] = sequences.apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_median'] = sequences.apply(lambda x: np.median(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_range'] = sequences.apply(lambda x: (np.max(x) - np.min(x)) if len(x) > 0 else 0)\n",
        "                \n",
        "                # === 分位数特征 ===\n",
        "                df_copy[f'{col}_q25'] = sequences.apply(lambda x: np.percentile(x, 25) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_q75'] = sequences.apply(lambda x: np.percentile(x, 75) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_iqr'] = df_copy[f'{col}_q75'] - df_copy[f'{col}_q25']\n",
        "                \n",
        "                # === 序列长度特征 ===\n",
        "                df_copy[f'{col}_len'] = sequences.apply(lambda x: len(x))\n",
        "                \n",
        "                # === 序列模式特征 ===\n",
        "                # 变异系数 (标准差/均值) - 衡量序列的相对变化程度\n",
        "                df_copy[f'{col}_cv'] = sequences.apply(lambda x: np.std(x)/np.mean(x) if len(x) > 0 and np.mean(x) != 0 else 0)\n",
        "                \n",
        "                # 偏度和峰度 - 衡量序列分布形状\n",
        "                df_copy[f'{col}_skew'] = sequences.apply(lambda x: stats.skew(x) if len(x) > 1 else 0)\n",
        "                df_copy[f'{col}_kurtosis'] = sequences.apply(lambda x: stats.kurtosis(x) if len(x) > 1 else 0)\n",
        "                \n",
        "                # === 序列特有的特征 ===\n",
        "                if col == 'ip_direction':\n",
        "                    # 对于方向序列：统计出站/入站比例\n",
        "                    df_copy[f'{col}_out_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 0])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_in_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 1])/len(x) if len(x) > 0 else 0)\n",
        "                    # 方向变化次数 - 反映通信模式\n",
        "                    df_copy[f'{col}_changes'] = sequences.apply(lambda x: sum([1 for i in range(1, len(x)) if x[i] != x[i-1]]) if len(x) > 1 else 0)\n",
        "                \n",
        "                elif col == 'pkt_len':\n",
        "                    # 对于包长度序列：小包/大包比例\n",
        "                    df_copy[f'{col}_small_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i <= 64])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_large_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i >= 1400])/len(x) if len(x) > 0 else 0)\n",
        "                \n",
        "                elif col == 'iat':\n",
        "                    # 对于时间间隔序列：突发性检测\n",
        "                    df_copy[f'{col}_burst_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i < 0.01])/len(x) if len(x) > 0 else 0)  # 小于10ms的比例\n",
        "                    df_copy[f'{col}_long_gap_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i > 1.0])/len(x) if len(x) > 0 else 0)  # 大于1s的比例\n",
        "                \n",
        "                # === 趋势特征 ===\n",
        "                # 序列递增/递减趋势\n",
        "                def trend_analysis(seq):\n",
        "                    if len(seq) < 2:\n",
        "                        return 0, 0\n",
        "                    increasing = sum([1 for i in range(1, len(seq)) if seq[i] > seq[i-1]])\n",
        "                    decreasing = sum([1 for i in range(1, len(seq)) if seq[i] < seq[i-1]])\n",
        "                    return increasing/len(seq), decreasing/len(seq)\n",
        "                \n",
        "                trends = sequences.apply(trend_analysis)\n",
        "                df_copy[f'{col}_increasing_ratio'] = trends.apply(lambda x: x[0])\n",
        "                df_copy[f'{col}_decreasing_ratio'] = trends.apply(lambda x: x[1])\n",
        "                \n",
        "                # 删除原始序列列\n",
        "                df_copy = df_copy.drop(columns=[col])\n",
        "                print(f\"  -> 已从 {col} 提取 {len([c for c in df_copy.columns if c.startswith(col)])} 个特征\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  -> 处理 {col} 时出错，将直接编码: {e}\")\n",
        "                # 如果处理失败，就简单编码\n",
        "                df_copy[col] = LabelEncoder().fit_transform(df_copy[col].astype(str))\n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "# 处理所有数据集的序列特征\n",
        "print(\"开始处理序列特征...\")\n",
        "print(\"=\" * 60)\n",
        "train_processed = process_sequence_features(train_processed)\n",
        "val_processed = process_sequence_features(val_processed)\n",
        "test_processed = process_sequence_features(test_processed)\n",
        "\n",
        "# 更新特征列表\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"\\n处理序列特征后的特征数量: {len(feature_columns)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 检查剩余的非数值列\n",
        "non_numeric_cols = []\n",
        "for col in feature_columns:\n",
        "    if not pd.api.types.is_numeric_dtype(train_processed[col]):\n",
        "        non_numeric_cols.append(col)\n",
        "\n",
        "print(f\"\\n需要编码的非数值列数量: {len(non_numeric_cols)}\")\n",
        "if non_numeric_cols:\n",
        "    print(f\"非数值列: {non_numeric_cols[:5]}...\")  # 显示前5个\n",
        "\n",
        "# 对非数值列进行标签编码\n",
        "if non_numeric_cols:\n",
        "    print(\"开始对非数值列进行标签编码...\")\n",
        "    for col in non_numeric_cols:\n",
        "        try:\n",
        "            le = LabelEncoder()\n",
        "            # 合并所有数据集的该列值进行编码\n",
        "            all_values = pd.concat([\n",
        "                train_processed[col].fillna('missing').astype(str),\n",
        "                val_processed[col].fillna('missing').astype(str),\n",
        "                test_processed[col].fillna('missing').astype(str)\n",
        "            ])\n",
        "            le.fit(all_values)\n",
        "            \n",
        "            train_processed[col] = le.transform(train_processed[col].fillna('missing').astype(str))\n",
        "            val_processed[col] = le.transform(val_processed[col].fillna('missing').astype(str))\n",
        "            test_processed[col] = le.transform(test_processed[col].fillna('missing').astype(str))\n",
        "            print(f\"  ✓ 已编码: {col}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ 编码失败 {col}: {e}\")\n",
        "            # 编码失败的列直接删除\n",
        "            if col in train_processed.columns:\n",
        "                train_processed = train_processed.drop(columns=[col])\n",
        "                val_processed = val_processed.drop(columns=[col])\n",
        "                test_processed = test_processed.drop(columns=[col])\n",
        "\n",
        "# 确保所有数据集具有相同的特征列\n",
        "print(\"\\n🔧 检查数据集特征一致性...\")\n",
        "train_features = set(train_processed.columns) - {'label'}\n",
        "val_features = set(val_processed.columns) - {'label'}\n",
        "test_features = set(test_processed.columns) - {'label'}\n",
        "\n",
        "print(f\"训练集特征数: {len(train_features)}\")\n",
        "print(f\"验证集特征数: {len(val_features)}\")\n",
        "print(f\"测试集特征数: {len(test_features)}\")\n",
        "\n",
        "# 取三个数据集的特征交集，确保一致性\n",
        "common_features = train_features.intersection(val_features).intersection(test_features)\n",
        "print(f\"共同特征数: {len(common_features)}\")\n",
        "\n",
        "if len(common_features) < len(train_features):\n",
        "    print(\"⚠️ 警告: 数据集间特征不一致，使用共同特征\")\n",
        "    # 只保留共同特征\n",
        "    feature_cols_to_keep = list(common_features) + ['label']\n",
        "    train_processed = train_processed[feature_cols_to_keep]\n",
        "    val_processed = val_processed[feature_cols_to_keep]\n",
        "    test_processed = test_processed[feature_cols_to_keep]\n",
        "\n",
        "print(\"\\n数据预处理完成！\")\n",
        "print(f\"最终特征数量: {len(common_features)}\")\n",
        "print(\"✅ 所有数据集特征已对齐\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 序列特征处理策略说明\n",
        "\n",
        "### 为什么XGBoost需要特殊处理序列特征？\n",
        "\n",
        "**XGBoost的限制：**\n",
        "- XGBoost是基于树的算法，只能处理固定维度的表格数据\n",
        "- 无法直接处理变长序列（如[1,0,1,1,0]这样的数组）\n",
        "- 需要将序列转换为固定数量的数值特征\n",
        "\n",
        "### 📊 我们提取的序列特征类型\n",
        "\n",
        "**1. 基础统计特征**\n",
        "- 均值、标准差、最大/最小值、中位数、四分位数\n",
        "- 这些特征捕获序列的整体分布特性\n",
        "\n",
        "**2. 形状特征**\n",
        "- 偏度(skewness)：序列分布的对称性\n",
        "- 峰度(kurtosis)：序列分布的尖锐程度\n",
        "- 变异系数：相对变化程度\n",
        "\n",
        "**3. 序列模式特征**\n",
        "- **方向序列(ip_direction)**：出站/入站比例、方向变化次数\n",
        "- **包长度序列(pkt_len)**：小包/大包比例\n",
        "- **时间间隔序列(iat)**：突发传输/长间隔比例\n",
        "\n",
        "**4. 趋势特征**\n",
        "- 递增/递减趋势：反映序列的时间演化模式\n",
        "\n",
        "### 🎯 这些特征对PCDN检测的意义\n",
        "\n",
        "**正常流量 vs PCDN流量的区别：**\n",
        "- **包大小模式**：PCDN可能有特定的分块传输模式\n",
        "- **方向模式**：PCDN的上传/下载比例可能不同\n",
        "- **时间模式**：PCDN的传输节奏可能更规律或更突发\n",
        "- **序列长度**：PCDN会话可能有特定的包数量模式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 序列特征处理效果展示\n",
        "print(\"🔍 序列特征处理效果分析\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 统计每个序列特征生成了多少个新特征\n",
        "sequence_feature_counts = {}\n",
        "for original_col in ['ip_direction', 'pkt_len', 'iat']:\n",
        "    derived_features = [col for col in train_processed.columns if col.startswith(original_col)]\n",
        "    sequence_feature_counts[original_col] = len(derived_features)\n",
        "    print(f\"{original_col:15} -> 生成了 {len(derived_features):2d} 个特征\")\n",
        "    print(f\"                   包括: {', '.join(derived_features[:5])}{'...' if len(derived_features) > 5 else ''}\")\n",
        "\n",
        "total_sequence_features = sum(sequence_feature_counts.values())\n",
        "print(f\"\\n📈 总共从3个序列特征生成了 {total_sequence_features} 个数值特征\")\n",
        "\n",
        "# 展示一些关键特征的含义\n",
        "print(f\"\\n📋 关键特征含义示例:\")\n",
        "feature_meanings = {\n",
        "    'pkt_len_mean': '平均包大小 - 反映传输数据的粒度',\n",
        "    'pkt_len_cv': '包大小变异系数 - 反映传输的规律性',\n",
        "    'ip_direction_changes': '方向变化次数 - 反映交互模式',\n",
        "    'iat_burst_ratio': '突发传输比例 - 反映时间模式',\n",
        "    'pkt_len_small_pkt_ratio': '小包比例 - 反映协议特征'\n",
        "}\n",
        "\n",
        "for feat, meaning in feature_meanings.items():\n",
        "    if feat in train_processed.columns:\n",
        "        print(f\"  {feat:25}: {meaning}\")\n",
        "\n",
        "print(\"\\n✨ 这些特征将帮助XGBoost学习正常流量和PCDN流量的行为差异模式\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据分布可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 标签分布\n",
        "label_counts = train_processed['label'].value_counts()\n",
        "axes[0, 0].pie(label_counts.values, labels=['Normal Traffic', 'PCDN Traffic'], autopct='%1.1f%%')\n",
        "axes[0, 0].set_title('Training Data Label Distribution')\n",
        "\n",
        "# 选择几个重要的数值特征进行可视化（包括处理后的序列特征）\n",
        "numeric_features = ['ip.len', 'tcp.srcport', 'tcp.dstport', 'sum_pkt_len', 'total_pkts']\n",
        "sequence_features = ['pkt_len_mean', 'ip_direction_changes', 'iat_mean']  # 添加序列特征\n",
        "\n",
        "# 合并特征列表并检查可用性\n",
        "all_viz_features = numeric_features + sequence_features\n",
        "available_features = [f for f in all_viz_features if f in train_processed.columns]\n",
        "print(f\"可用于可视化的特征: {available_features}\")\n",
        "\n",
        "if len(available_features) >= 3:\n",
        "    # 特征分布对比\n",
        "    for i, feature in enumerate(available_features[:3]):\n",
        "        if i == 0:\n",
        "            ax = axes[0, 1]\n",
        "        elif i == 1:\n",
        "            ax = axes[1, 0]\n",
        "        else:\n",
        "            ax = axes[1, 1]\n",
        "        \n",
        "        normal_data = train_processed[train_processed['label'] == 0][feature]\n",
        "        pcdn_data = train_processed[train_processed['label'] == 1][feature]\n",
        "        \n",
        "        ax.hist(normal_data, alpha=0.7, label='Normal', bins=30, density=True)\n",
        "        ax.hist(pcdn_data, alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "        ax.set_title(f'Distribution of {feature}')\n",
        "        ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"数据分布可视化完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 序列特征处理总结\n",
        "\n",
        "### 🔄 完整处理流程\n",
        "\n",
        "**原始序列特征 → 提取统计特征 → XGBoost训练**\n",
        "\n",
        "1. **原始数据格式**：\n",
        "   - `pkt_len`: `\"[40, 40, 1432, 712, ...]\"` (包长度序列)\n",
        "   - `ip_direction`: `\"[0, 0, 1, 1, 0, ...]\"` (方向序列)  \n",
        "   - `iat`: `\"[0.0, 0.016, 0.083, ...]\"` (时间间隔序列)\n",
        "\n",
        "2. **特征提取策略**：\n",
        "   - **统计特征**：均值、方差、分位数等 (适用于所有序列)\n",
        "   - **领域特征**：根据序列含义设计的专门特征\n",
        "   - **模式特征**：变化趋势、突发性等时序特征\n",
        "\n",
        "3. **XGBoost优势**：\n",
        "   - 可以自动发现特征之间的复杂组合\n",
        "   - 通过树结构捕获非线性模式\n",
        "   - 特征重要性分析帮助理解哪些序列模式最重要\n",
        "\n",
        "### 📈 预期效果\n",
        "\n",
        "通过这种处理方式，我们将**3个变长序列**转换为**数十个固定长度的数值特征**，这些特征能够充分表达网络流量的时序行为模式，帮助XGBoost准确区分正常流量和PCDN流量。\n",
        "\n",
        "## 🛠️ 代码质量改进\n",
        "\n",
        "### 🔒 安全性修复\n",
        "- **替换 `eval()` 函数**：使用 `ast.literal_eval()` 安全解析数组字符串，避免代码注入风险\n",
        "- **增强错误处理**：添加完善的异常捕获和数据验证\n",
        "\n",
        "### 📊 数据一致性保证\n",
        "- **特征维度对齐**：确保训练、验证、测试集具有相同的特征列\n",
        "- **空数据集处理**：自动从训练集分割验证/测试集，防止数据缺失\n",
        "- **数据质量检查**：检测NaN值、无穷值和标签分布\n",
        "\n",
        "### ⚙️ 模型参数优化\n",
        "- **修复XGBoost参数**：正确使用 `early_stopping_rounds` 参数\n",
        "- **改进错误处理**：更robust的序列特征处理流程\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备训练数据 - 使用处理后的实际特征列\n",
        "final_feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"实际使用的特征数量: {len(final_feature_columns)}\")\n",
        "\n",
        "X_train = train_processed[final_feature_columns]\n",
        "y_train = train_processed['label']\n",
        "\n",
        "X_val = val_processed[final_feature_columns] \n",
        "y_val = val_processed['label']\n",
        "\n",
        "X_test = test_processed[final_feature_columns]\n",
        "y_test = test_processed['label']\n",
        "\n",
        "print(f\"训练集特征形状: {X_train.shape}\")\n",
        "print(f\"验证集特征形状: {X_val.shape}\")\n",
        "print(f\"测试集特征形状: {X_test.shape}\")\n",
        "\n",
        "# 检查是否还有非数值数据\n",
        "print(f\"\\n训练数据中的数据类型:\")\n",
        "print(X_train.dtypes.value_counts())\n",
        "\n",
        "# 确保所有数据都是数值型\n",
        "X_train = X_train.select_dtypes(include=[np.number])\n",
        "X_val = X_val.select_dtypes(include=[np.number])\n",
        "X_test = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\n最终特征数量: {X_train.shape[1]}\")\n",
        "\n",
        "# 数据质量检查\n",
        "print(\"\\n🔍 数据质量检查:\")\n",
        "print(f\"训练集是否包含NaN: {X_train.isnull().any().any()}\")\n",
        "print(f\"验证集是否包含NaN: {X_val.isnull().any().any()}\")\n",
        "print(f\"测试集是否包含NaN: {X_test.isnull().any().any()}\")\n",
        "print(f\"训练集是否包含无穷值: {np.isinf(X_train).any().any()}\")\n",
        "print(f\"标签分布 - 训练集: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"标签分布 - 验证集: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"标签分布 - 测试集: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# 检查特征维度是否一致\n",
        "assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"特征维度不一致！\"\n",
        "print(\"✅ 数据质量检查通过\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost模型训练\n",
        "print(\"开始训练XGBoost模型...\")\n",
        "\n",
        "# 创建XGBoost分类器\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# 训练模型（使用验证集进行早停）\n",
        "xgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=10,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nXGBoost模型训练完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模型评估\n",
        "def evaluate_model(model, X, y, data_name):\n",
        "    \"\"\"\n",
        "    评估模型性能\n",
        "    \"\"\"\n",
        "    # 预测\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "    \n",
        "    print(f\"\\n=== {data_name} 评估结果 ===\")\n",
        "    \n",
        "    # 分类报告\n",
        "    print(\"\\n分类报告:\")\n",
        "    print(classification_report(y, y_pred, target_names=['Normal', 'PCDN']))\n",
        "    \n",
        "    # AUC分数\n",
        "    auc_score = roc_auc_score(y, y_pred_proba)\n",
        "    print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
        "    \n",
        "    return y_pred, y_pred_proba, auc_score\n",
        "\n",
        "# 评估训练集\n",
        "train_pred, train_proba, train_auc = evaluate_model(xgb_model, X_train, y_train, \"训练集\")\n",
        "\n",
        "# 评估验证集\n",
        "val_pred, val_proba, val_auc = evaluate_model(xgb_model, X_val, y_val, \"验证集\")\n",
        "\n",
        "# 评估测试集\n",
        "test_pred, test_proba, test_auc = evaluate_model(xgb_model, X_test, y_test, \"测试集\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征重要性分析\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# 创建特征重要性DataFrame\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== Top 20 最重要特征 ===\")\n",
        "print(importance_df.head(20))\n",
        "\n",
        "# 特征重要性可视化\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = importance_df.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 20 Feature Importance in XGBoost Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 绘制ROC曲线和混淆矩阵\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# ROC曲线\n",
        "datasets = [\n",
        "    (y_train, train_proba, \"Training\", train_auc),\n",
        "    (y_val, val_proba, \"Validation\", val_auc),\n",
        "    (y_test, test_proba, \"Testing\", test_auc)\n",
        "]\n",
        "\n",
        "ax_roc = axes[0, 0]\n",
        "for y_true, y_prob, label, auc in datasets:\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    ax_roc.plot(fpr, tpr, label=f'{label} (AUC = {auc:.3f})')\n",
        "\n",
        "ax_roc.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "ax_roc.set_xlabel('False Positive Rate')\n",
        "ax_roc.set_ylabel('True Positive Rate')\n",
        "ax_roc.set_title('ROC Curves')\n",
        "ax_roc.legend()\n",
        "ax_roc.grid(True)\n",
        "\n",
        "# 混淆矩阵 - 测试集\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Normal', 'PCDN'], \n",
        "            yticklabels=['Normal', 'PCDN'],\n",
        "            ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Confusion Matrix - Test Set')\n",
        "axes[0, 1].set_ylabel('True Label')\n",
        "axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "# 预测概率分布\n",
        "axes[1, 0].hist(test_proba[y_test == 0], alpha=0.7, label='Normal', bins=30, density=True)\n",
        "axes[1, 0].hist(test_proba[y_test == 1], alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "axes[1, 0].set_xlabel('Prediction Probability')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Prediction Probability Distribution')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 学习曲线（训练历史）\n",
        "results = xgb_model.evals_result()\n",
        "if 'validation_0' in results:\n",
        "    epochs = len(results['validation_0']['logloss'])\n",
        "    x_axis = range(0, epochs)\n",
        "    axes[1, 1].plot(x_axis, results['validation_0']['logloss'], label='Validation')\n",
        "    axes[1, 1].set_xlabel('Epochs')\n",
        "    axes[1, 1].set_ylabel('Log Loss')\n",
        "    axes[1, 1].set_title('Model Learning Curve')\n",
        "    axes[1, 1].legend()\n",
        "else:\n",
        "    axes[1, 1].text(0.5, 0.5, 'Learning curve not available', \n",
        "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "    axes[1, 1].set_title('Learning Curve')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征相关性分析\n",
        "if len(importance_df) >= 10:\n",
        "    # 选择最重要的10个特征进行相关性分析\n",
        "    top_10_features = importance_df.head(10)['feature'].tolist()\n",
        "    corr_data = train_processed[top_10_features + ['label']]\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = corr_data.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f')\n",
        "    plt.title('Correlation Matrix of Top 10 Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n=== 与标签相关性最高的特征 ===\")\n",
        "    label_corr = correlation_matrix['label'].abs().sort_values(ascending=False)\n",
        "    print(label_corr[label_corr.index != 'label'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模型性能总结\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             模型性能总结\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "performance_summary = pd.DataFrame({\n",
        "    '数据集': ['训练集', '验证集', '测试集'],\n",
        "    'AUC Score': [train_auc, val_auc, test_auc],\n",
        "    '样本数量': [len(y_train), len(y_val), len(y_test)]\n",
        "})\n",
        "\n",
        "print(performance_summary.to_string(index=False))\n",
        "\n",
        "print(f\"\\n特征总数: {len(feature_names)}\")\n",
        "print(f\"最重要的5个特征:\")\n",
        "for i, (idx, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
        "    print(f\"  {i}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\n模型训练和评估完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型和结果\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 创建输出目录\n",
        "output_dir = './output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 保存模型\n",
        "model_path = os.path.join(output_dir, 'xgboost_pcdn_classifier.pkl')\n",
        "try:\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"✅ 模型已保存到: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 模型保存失败: {e}\")\n",
        "\n",
        "# 保存特征重要性\n",
        "importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
        "try:\n",
        "    importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 特征重要性已保存到: {importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 特征重要性保存失败: {e}\")\n",
        "\n",
        "# 保存性能报告\n",
        "performance_path = os.path.join(output_dir, 'model_performance.csv')\n",
        "try:\n",
        "    performance_summary.to_csv(performance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 模型性能报告已保存到: {performance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 性能报告保存失败: {e}\")\n",
        "\n",
        "# 保存详细分析报告\n",
        "analysis_report = {\n",
        "    'Project': 'PCDN vs Normal Traffic Classification',\n",
        "    'Algorithm': 'XGBoost',\n",
        "    'Total_Features': len(feature_names),\n",
        "    'Train_Samples': len(y_train),\n",
        "    'Val_Samples': len(y_val),\n",
        "    'Test_Samples': len(y_test),\n",
        "    'Train_AUC': train_auc,\n",
        "    'Val_AUC': val_auc,\n",
        "    'Test_AUC': test_auc,\n",
        "    'Top_5_Features': importance_df.head(5)['feature'].tolist()\n",
        "}\n",
        "\n",
        "report_path = os.path.join(output_dir, 'analysis_report.txt')\n",
        "try:\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PCDN流量分类项目分析报告\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        for key, value in analysis_report.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "    print(f\"✅ 分析报告已保存到: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 分析报告保存失败: {e}\")\n",
        "\n",
        "print(f\"\\n📁 所有输出文件已保存到目录: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 项目完成总结\n",
        "\n",
        "### ✅ 已完成的工作\n",
        "\n",
        "1. **数据加载与预处理** \n",
        "   - ✅ 智能路径检测和数据加载\n",
        "   - ✅ 自动处理缺失的验证/测试集\n",
        "   - ✅ 安全的序列特征解析\n",
        "\n",
        "2. **特征工程**\n",
        "   - ✅ 序列特征转统计特征 (45+ 新特征)\n",
        "   - ✅ 非数值特征自动编码\n",
        "   - ✅ 数据质量检查和清洗\n",
        "\n",
        "3. **模型训练**\n",
        "   - ✅ XGBoost分类器训练\n",
        "   - ✅ 早停机制防止过拟合\n",
        "   - ✅ 模型性能评估\n",
        "\n",
        "4. **结果分析**\n",
        "   - ✅ 特征重要性分析\n",
        "   - ✅ ROC曲线和混淆矩阵\n",
        "   - ✅ 相关性分析\n",
        "   - ✅ 可视化展示\n",
        "\n",
        "5. **输出管理**\n",
        "   - ✅ 模型文件保存\n",
        "   - ✅ 结果报告导出\n",
        "   - ✅ 项目文档完善\n",
        "\n",
        "### 🚀 使用方法\n",
        "\n",
        "1. **环境准备**: 确保安装所需Python包 (pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, scipy)\n",
        "\n",
        "2. **数据准备**: 将数据集文件夹放在notebook同目录下\n",
        "\n",
        "3. **运行项目**: 依次执行所有代码单元\n",
        "\n",
        "4. **查看结果**: 检查 `./output/` 目录中的输出文件\n",
        "\n",
        "### 🎯 项目价值\n",
        "\n",
        "- **实用性**: 可直接用于PCDN流量检测\n",
        "- **扩展性**: 可轻松适配其他网络流量分类任务  \n",
        "- **可解释性**: 详细的特征重要性分析\n",
        "- **可维护性**: 清晰的代码结构和文档\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bysj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
