{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDN流量与正常流量二分类任务\n",
        "\n",
        "## 项目概述\n",
        "本项目使用XGBoost对网络流量进行二分类：\n",
        "- **APP_0**: 正常流量 (标签: 0)\n",
        "- **APP_1**: PCDN流量 (标签: 1)\n",
        "\n",
        "## 数据集结构\n",
        "- `Training_set/`: 训练集\n",
        "- `Validation_set/`: 验证集  \n",
        "- `Testing_set/`: 测试集\n",
        "\n",
        "每个集合包含APP_0（正常流量）和APP_1（PCDN流量）两个类别的CSV文件。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import xgboost as xgb\n",
        "\n",
        "# 设置中文字体和样式\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "sns.set_style(\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ 所有库导入成功！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 数据加载与探索\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义数据加载函数\n",
        "def load_dataset(base_path, dataset_type):\n",
        "    \"\"\"\n",
        "    加载指定类型的数据集\n",
        "    \n",
        "    Args:\n",
        "        base_path: 数据集根目录\n",
        "        dataset_type: 数据集类型 ('Training_set', 'Validation_set', 'Testing_set')\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: 合并后的数据集\n",
        "    \"\"\"\n",
        "    data_list = []\n",
        "    \n",
        "    # 加载APP_0 (正常流量) 数据\n",
        "    app0_path = Path(base_path) / dataset_type / 'APP_0'\n",
        "    app0_files = list(app0_path.glob('*.csv'))\n",
        "    print(f\"📁 {dataset_type}/APP_0 找到 {len(app0_files)} 个文件\")\n",
        "    \n",
        "    for file in app0_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df['label'] = 0  # 正常流量标签\n",
        "            df['source_file'] = str(file.name)\n",
        "            data_list.append(df)\n",
        "            print(f\"  ✅ {file.name}: {len(df)} 行\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 读取 {file.name} 失败: {e}\")\n",
        "    \n",
        "    # 加载APP_1 (PCDN流量) 数据\n",
        "    app1_path = Path(base_path) / dataset_type / 'APP_1'\n",
        "    app1_files = list(app1_path.glob('*.csv'))\n",
        "    print(f\"📁 {dataset_type}/APP_1 找到 {len(app1_files)} 个文件\")\n",
        "    \n",
        "    for file in app1_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df['label'] = 1  # PCDN流量标签\n",
        "            df['source_file'] = str(file.name)\n",
        "            data_list.append(df)\n",
        "            print(f\"  ✅ {file.name}: {len(df)} 行\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 读取 {file.name} 失败: {e}\")\n",
        "    \n",
        "    if data_list:\n",
        "        combined_df = pd.concat(data_list, ignore_index=True)\n",
        "        print(f\"🎯 {dataset_type} 总计: {len(combined_df)} 行数据\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(f\"⚠️ {dataset_type} 没有加载到任何数据\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# 加载所有数据集\n",
        "base_path = 'pcdn_32_pkts_2class_feature_enhance_v17.4_dataset'\n",
        "\n",
        "print(\"🚀 开始加载数据集...\\n\")\n",
        "train_df = load_dataset(base_path, 'Training_set')\n",
        "print()\n",
        "val_df = load_dataset(base_path, 'Validation_set')\n",
        "print()\n",
        "test_df = load_dataset(base_path, 'Testing_set')\n",
        "print(\"\\n📊 数据加载完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据集基本信息\n",
        "print(\"=\"*60)\n",
        "print(\"📈 数据集概览\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "datasets = {'训练集': train_df, '验证集': val_df, '测试集': test_df}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if not df.empty:\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  📏 数据形状: {df.shape}\")\n",
        "        print(f\"  🏷️ 标签分布:\")\n",
        "        label_counts = df['label'].value_counts().sort_index()\n",
        "        for label, count in label_counts.items():\n",
        "            label_name = \"正常流量\" if label == 0 else \"PCDN流量\"\n",
        "            print(f\"    {label} ({label_name}): {count} 样本\")\n",
        "        print(f\"  📂 文件来源: {df['source_file'].unique()}\")\n",
        "    else:\n",
        "        print(f\"\\n{name}: 空数据集\")\n",
        "\n",
        "# 查看数据字段\n",
        "if not train_df.empty:\n",
        "    print(\"\\n🔍 数据字段分析\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"总字段数: {len(train_df.columns)}\")\n",
        "    print(f\"字段列表: {list(train_df.columns)}\")\n",
        "    \n",
        "    # 显示前几行数据\n",
        "    print(\"\\n📋 训练集前3行数据预览:\")\n",
        "    display(train_df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 数据预处理与特征工程\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_features(df):\n",
        "    \"\"\"\n",
        "    数据预处理和特征工程\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    print(\"🔧 开始数据预处理...\")\n",
        "    \n",
        "    # 1. 处理缺失值\n",
        "    print(f\"📊 缺失值统计:\")\n",
        "    missing_stats = df_processed.isnull().sum()\n",
        "    missing_cols = missing_stats[missing_stats > 0]\n",
        "    if len(missing_cols) > 0:\n",
        "        print(missing_cols)\n",
        "        # 填充数值型缺失值\n",
        "        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "        df_processed[numeric_cols] = df_processed[numeric_cols].fillna(0)\n",
        "        # 填充字符型缺失值\n",
        "        categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "        df_processed[categorical_cols] = df_processed[categorical_cols].fillna('unknown')\n",
        "    else:\n",
        "        print(\"✅ 无缺失值\")\n",
        "    \n",
        "    # 2. 处理特殊字段\n",
        "    special_fields = ['ip_direction', 'pkt_len', 'iat', 'payload']\n",
        "    \n",
        "    for field in special_fields:\n",
        "        if field in df_processed.columns:\n",
        "            print(f\"🔄 处理 {field} 字段...\")\n",
        "            \n",
        "            if field == 'payload':\n",
        "                # 载荷数据：计算长度特征\n",
        "                df_processed[f'{field}_length'] = df_processed[field].astype(str).str.len()\n",
        "                df_processed = df_processed.drop(columns=[field])\n",
        "            \n",
        "            elif field in ['ip_direction', 'pkt_len', 'iat']:\n",
        "                # 解析列表型特征\n",
        "                try:\n",
        "                    # 尝试解析为列表\n",
        "                    parsed_data = df_processed[field].apply(lambda x: ast.literal_eval(str(x)) if pd.notna(x) and str(x).strip() else [])\n",
        "                    \n",
        "                    # 提取统计特征\n",
        "                    df_processed[f'{field}_mean'] = parsed_data.apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_std'] = parsed_data.apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_max'] = parsed_data.apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_min'] = parsed_data.apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_sum'] = parsed_data.apply(lambda x: np.sum(x) if len(x) > 0 else 0)\n",
        "                    df_processed[f'{field}_count'] = parsed_data.apply(lambda x: len(x))\n",
        "                    \n",
        "                    # 删除原始字段\n",
        "                    df_processed = df_processed.drop(columns=[field])\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ 处理 {field} 时出错: {e}，保持原始数据\")\n",
        "    \n",
        "    # 3. 编码分类特征\n",
        "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "    categorical_cols = [col for col in categorical_cols if col not in ['source_file']]  # 排除辅助字段\n",
        "    \n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        if col in df_processed.columns:\n",
        "            le = LabelEncoder()\n",
        "            df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "    \n",
        "    print(f\"🎯 预处理完成！最终特征数: {df_processed.shape[1]}\")\n",
        "    \n",
        "    return df_processed, label_encoders\n",
        "\n",
        "# 处理训练数据\n",
        "if not train_df.empty:\n",
        "    train_processed, encoders = preprocess_features(train_df)\n",
        "    print(\"\\n✅ 训练集预处理完成\")\n",
        "    \n",
        "    # 处理验证和测试数据（使用相同的编码器）\n",
        "    if not val_df.empty:\n",
        "        val_processed, _ = preprocess_features(val_df)\n",
        "        print(\"✅ 验证集预处理完成\")\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        test_processed, _ = preprocess_features(test_df)\n",
        "        print(\"✅ 测试集预处理完成\")\n",
        "else:\n",
        "    print(\"❌ 训练数据为空，无法进行预处理\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备建模数据\n",
        "if not train_df.empty:\n",
        "    # 分离特征和标签\n",
        "    feature_cols = [col for col in train_processed.columns if col not in ['label', 'source_file']]\n",
        "    \n",
        "    X_train = train_processed[feature_cols]\n",
        "    y_train = train_processed['label']\n",
        "    \n",
        "    if not val_df.empty:\n",
        "        X_val = val_processed[feature_cols]\n",
        "        y_val = val_processed['label']\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        X_test = test_processed[feature_cols]\n",
        "        y_test = test_processed['label']\n",
        "    \n",
        "    print(f\"🎯 特征维度: {X_train.shape}\")\n",
        "    print(f\"📊 训练标签分布: {y_train.value_counts().to_dict()}\")\n",
        "    print(f\"🔧 使用的特征数量: {len(feature_cols)}\")\n",
        "    print(f\"📝 特征名称前10个: {feature_cols[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. XGBoost模型训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost模型训练\n",
        "if not train_df.empty and len(X_train) > 0:\n",
        "    print(\"🚀 开始XGBoost模型训练...\")\n",
        "    \n",
        "    # 确保所有特征都是数值型\n",
        "    X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
        "    \n",
        "    if len(X_train_numeric.columns) == 0:\n",
        "        print(\"❌ 没有可用的数值型特征\")\n",
        "    else:\n",
        "        print(f\"📊 使用 {len(X_train_numeric.columns)} 个数值型特征\")\n",
        "        \n",
        "        # 配置XGBoost参数\n",
        "        xgb_params = {\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'logloss',\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        # 创建和训练模型\n",
        "        model = xgb.XGBClassifier(**xgb_params)\n",
        "        \n",
        "        # 准备验证数据\n",
        "        eval_set = []\n",
        "        if not val_df.empty:\n",
        "            X_val_numeric = val_processed[X_train_numeric.columns]\n",
        "            eval_set = [(X_train_numeric, y_train), (X_val_numeric, y_val)]\n",
        "        else:\n",
        "            eval_set = [(X_train_numeric, y_train)]\n",
        "        \n",
        "        # 训练模型\n",
        "        model.fit(\n",
        "            X_train_numeric, y_train,\n",
        "            eval_set=eval_set,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        print(\"✅ 模型训练完成！\")\n",
        "        \n",
        "        # 更新特征列表\n",
        "        final_feature_cols = X_train_numeric.columns.tolist()\n",
        "else:\n",
        "    print(\"❌ 无法训练模型：数据不足\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 模型评估与可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模型预测和评估\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    print(\"🎯 模型评估开始...\")\n",
        "    \n",
        "    # 训练集预测\n",
        "    y_train_pred = model.predict(X_train_numeric)\n",
        "    y_train_proba = model.predict_proba(X_train_numeric)[:, 1]\n",
        "    train_accuracy = (y_train_pred == y_train).mean()\n",
        "    \n",
        "    print(f\"📊 训练集准确率: {train_accuracy:.4f}\")\n",
        "    \n",
        "    # 验证集预测（如果有）\n",
        "    if not val_df.empty:\n",
        "        y_val_pred = model.predict(X_val_numeric)\n",
        "        y_val_proba = model.predict_proba(X_val_numeric)[:, 1]\n",
        "        val_accuracy = (y_val_pred == y_val).mean()\n",
        "        print(f\"📊 验证集准确率: {val_accuracy:.4f}\")\n",
        "    \n",
        "    # 测试集预测（如果有）\n",
        "    if not test_df.empty:\n",
        "        X_test_numeric = test_processed[final_feature_cols]\n",
        "        y_test_pred = model.predict(X_test_numeric)\n",
        "        y_test_proba = model.predict_proba(X_test_numeric)[:, 1]\n",
        "        test_accuracy = (y_test_pred == y_test).mean()\n",
        "        print(f\"📊 测试集准确率: {test_accuracy:.4f}\")\n",
        "    \n",
        "    # 打印详细分类报告\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📋 详细分类报告\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    target_names = ['正常流量', 'PCDN流量']\n",
        "    \n",
        "    print(\"\\n🎯 训练集分类报告:\")\n",
        "    print(classification_report(y_train, y_train_pred, target_names=target_names))\n",
        "    \n",
        "    if not val_df.empty:\n",
        "        print(\"\\n✅ 验证集分类报告:\")\n",
        "        print(classification_report(y_val, y_val_pred, target_names=target_names))\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        print(\"\\n🧪 测试集分类报告:\")\n",
        "        print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "else:\n",
        "    print(\"❌ 模型未训练，无法进行评估\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化评估结果\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # 1. 混淆矩阵\n",
        "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
        "                xticklabels=['正常流量', 'PCDN流量'], \n",
        "                yticklabels=['正常流量', 'PCDN流量'])\n",
        "    axes[0,0].set_title('🔥 训练集混淆矩阵', fontsize=14, fontweight='bold')\n",
        "    axes[0,0].set_ylabel('实际标签')\n",
        "    axes[0,0].set_xlabel('预测标签')\n",
        "    \n",
        "    # 2. ROC曲线\n",
        "    if len(np.unique(y_train)) > 1:\n",
        "        fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
        "        auc_train = roc_auc_score(y_train, y_train_proba)\n",
        "        axes[0,1].plot(fpr_train, tpr_train, label=f'训练集 (AUC = {auc_train:.3f})', linewidth=2)\n",
        "        \n",
        "        if not val_df.empty and len(np.unique(y_val)) > 1:\n",
        "            fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba)\n",
        "            auc_val = roc_auc_score(y_val, y_val_proba)\n",
        "            axes[0,1].plot(fpr_val, tpr_val, label=f'验证集 (AUC = {auc_val:.3f})', linewidth=2)\n",
        "        \n",
        "        if not test_df.empty and len(np.unique(y_test)) > 1:\n",
        "            fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
        "            auc_test = roc_auc_score(y_test, y_test_proba)\n",
        "            axes[0,1].plot(fpr_test, tpr_test, label=f'测试集 (AUC = {auc_test:.3f})', linewidth=2)\n",
        "        \n",
        "        axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "        axes[0,1].set_xlabel('假正率 (FPR)')\n",
        "        axes[0,1].set_ylabel('真正率 (TPR)')\n",
        "        axes[0,1].set_title('📈 ROC曲线', fontsize=14, fontweight='bold')\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. 预测概率分布\n",
        "    axes[1,0].hist(y_train_proba[y_train==0], bins=20, alpha=0.7, label='正常流量', color='blue')\n",
        "    axes[1,0].hist(y_train_proba[y_train==1], bins=20, alpha=0.7, label='PCDN流量', color='red')\n",
        "    axes[1,0].set_xlabel('预测概率')\n",
        "    axes[1,0].set_ylabel('频次')\n",
        "    axes[1,0].set_title('🎯 预测概率分布', fontsize=14, fontweight='bold')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. 准确率对比\n",
        "    accuracies = [train_accuracy]\n",
        "    labels = ['训练集']\n",
        "    colors = ['#3498db']\n",
        "    \n",
        "    if not val_df.empty:\n",
        "        accuracies.append(val_accuracy)\n",
        "        labels.append('验证集')\n",
        "        colors.append('#2ecc71')\n",
        "    \n",
        "    if not test_df.empty:\n",
        "        accuracies.append(test_accuracy)\n",
        "        labels.append('测试集')\n",
        "        colors.append('#e74c3c')\n",
        "    \n",
        "    bars = axes[1,1].bar(labels, accuracies, color=colors)\n",
        "    axes[1,1].set_title('📊 各数据集准确率对比', fontsize=14, fontweight='bold')\n",
        "    axes[1,1].set_ylabel('准确率')\n",
        "    axes[1,1].set_ylim(0, 1.1)\n",
        "    \n",
        "    # 添加数值标签\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                      f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"❌ 无法生成评估可视化\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 特征重要性分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征重要性分析\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    print(\"🔍 特征重要性分析...\")\n",
        "    \n",
        "    # 获取特征重要性\n",
        "    feature_importance = model.feature_importances_\n",
        "    feature_names = final_feature_cols\n",
        "    \n",
        "    # 创建特征重要性DataFrame\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"📊 Top 10 最重要特征:\")\n",
        "    print(importance_df.head(10))\n",
        "    \n",
        "    # 创建特征重要性可视化\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "    \n",
        "    # 1. Top 20 特征重要性条形图\n",
        "    top_20 = importance_df.head(20)\n",
        "    axes[0,0].barh(range(len(top_20)), top_20['importance'], color='skyblue')\n",
        "    axes[0,0].set_yticks(range(len(top_20)))\n",
        "    axes[0,0].set_yticklabels(top_20['feature'])\n",
        "    axes[0,0].set_xlabel('重要性分数')\n",
        "    axes[0,0].set_title('🏆 Top 20 特征重要性', fontsize=14, fontweight='bold')\n",
        "    axes[0,0].invert_yaxis()\n",
        "    \n",
        "    # 2. 特征重要性分布直方图\n",
        "    axes[0,1].hist(feature_importance, bins=30, color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].set_xlabel('重要性分数')\n",
        "    axes[0,1].set_ylabel('特征数量')\n",
        "    axes[0,1].set_title('📊 特征重要性分布', fontsize=14, fontweight='bold')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. 累积重要性贡献\n",
        "    cumsum_importance = np.cumsum(importance_df['importance'].values)\n",
        "    axes[1,0].plot(range(1, len(cumsum_importance)+1), cumsum_importance, 'b-', linewidth=2)\n",
        "    axes[1,0].fill_between(range(1, len(cumsum_importance)+1), cumsum_importance, alpha=0.3)\n",
        "    axes[1,0].set_xlabel('特征数量')\n",
        "    axes[1,0].set_ylabel('累积重要性')\n",
        "    axes[1,0].set_title('📈 累积特征重要性贡献', fontsize=14, fontweight='bold')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 找到前80%重要性对应的特征数量\n",
        "    threshold_80 = 0.8 * cumsum_importance[-1]\n",
        "    features_80 = np.where(cumsum_importance >= threshold_80)[0][0] + 1\n",
        "    axes[1,0].axhline(y=threshold_80, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[1,0].axvline(x=features_80, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[1,0].text(features_80+1, threshold_80, f'前{features_80}个特征\\\\n贡献80%重要性', \n",
        "                  bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "    \n",
        "    # 4. Top 10 特征重要性饼图\n",
        "    top_10 = importance_df.head(10)\n",
        "    other_importance = importance_df.iloc[10:]['importance'].sum()\n",
        "    \n",
        "    pie_data = top_10['importance'].tolist() + [other_importance]\n",
        "    pie_labels = top_10['feature'].tolist() + ['其他特征']\n",
        "    \n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))\n",
        "    wedges, texts, autotexts = axes[1,1].pie(pie_data, labels=pie_labels, autopct='%1.1f%%', \n",
        "                                            colors=colors, startangle=90)\n",
        "    axes[1,1].set_title('🥧 Top 10 特征重要性占比', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # 调整文字大小\n",
        "    for text in texts:\n",
        "        text.set_fontsize(8)\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_fontsize(8)\n",
        "        autotext.set_fontweight('bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 特征重要性统计\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📈 特征重要性统计\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"🔢 总特征数量: {len(feature_importance)}\")\n",
        "    print(f\"📊 平均重要性: {np.mean(feature_importance):.6f}\")\n",
        "    print(f\"📊 重要性标准差: {np.std(feature_importance):.6f}\")\n",
        "    print(f\"🏆 最高重要性: {np.max(feature_importance):.6f} ({importance_df.iloc[0]['feature']})\")\n",
        "    print(f\"🔻 最低重要性: {np.min(feature_importance):.6f}\")\n",
        "    print(f\"🎯 前{features_80}个特征贡献80%重要性\")\n",
        "    \n",
        "    # 保存重要特征列表\n",
        "    top_features = importance_df.head(20)['feature'].tolist()\n",
        "    print(f\"\\n🌟 建议关注的Top 20特征:\")\n",
        "    for i, feature in enumerate(top_features, 1):\n",
        "        importance_score = importance_df[importance_df['feature'] == feature]['importance'].iloc[0]\n",
        "        print(f\"  {i:2d}. {feature:<30} (重要性: {importance_score:.6f})\")\n",
        "else:\n",
        "    print(\"❌ 无法进行特征重要性分析\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 模型性能总结与建议\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模型性能总结\n",
        "if 'model' in locals() and not train_df.empty:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🎯 PCDN流量分类模型性能总结\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\n📊 数据集信息:\")\n",
        "    print(f\"  🎓 训练样本: {len(X_train)} 个 (正常流量: {sum(y_train==0)}, PCDN流量: {sum(y_train==1)})\")\n",
        "    if not val_df.empty:\n",
        "        print(f\"  ✅ 验证样本: {len(X_val)} 个 (正常流量: {sum(y_val==0)}, PCDN流量: {sum(y_val==1)})\")\n",
        "    if not test_df.empty:\n",
        "        print(f\"  🧪 测试样本: {len(X_test)} 个 (正常流量: {sum(y_test==0)}, PCDN流量: {sum(y_test==1)})\")\n",
        "    print(f\"  🔧 使用特征: {len(final_feature_cols)} 个\")\n",
        "    \n",
        "    print(f\"\\n🏆 模型性能:\")\n",
        "    print(f\"  📈 训练集准确率: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "    if not val_df.empty:\n",
        "        print(f\"  📈 验证集准确率: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
        "    if not test_df.empty:\n",
        "        print(f\"  📈 测试集准确率: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "    \n",
        "    if len(np.unique(y_train)) > 1:\n",
        "        print(f\"  📊 训练集AUC: {auc_train:.4f}\")\n",
        "        if not val_df.empty and len(np.unique(y_val)) > 1:\n",
        "            print(f\"  📊 验证集AUC: {auc_val:.4f}\")\n",
        "        if not test_df.empty and len(np.unique(y_test)) > 1:\n",
        "            print(f\"  📊 测试集AUC: {auc_test:.4f}\")\n",
        "    \n",
        "    print(f\"\\n🌟 关键特征 (Top 5):\")\n",
        "    for i, (_, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
        "        print(f\"  {i}. {row['feature']:<25} (重要性: {row['importance']:.6f})\")\n",
        "    \n",
        "    print(f\"\\n💡 模型建议:\")\n",
        "    \n",
        "    # 数据量建议\n",
        "    total_samples = len(X_train)\n",
        "    if total_samples < 100:\n",
        "        print(f\"  ⚠️  数据量较小 ({total_samples}样本)，建议收集更多数据以提高模型稳定性\")\n",
        "    elif total_samples < 1000:\n",
        "        print(f\"  📊 数据量适中 ({total_samples}样本)，可考虑数据增强技术\")\n",
        "    else:\n",
        "        print(f\"  ✅ 数据量充足 ({total_samples}样本)\")\n",
        "    \n",
        "    # 类别平衡建议\n",
        "    class_ratio = min(sum(y_train==0), sum(y_train==1)) / max(sum(y_train==0), sum(y_train==1))\n",
        "    if class_ratio < 0.5:\n",
        "        print(f\"  ⚠️  类别不平衡 (比例: {class_ratio:.2f})，建议使用类别权重或采样技术\")\n",
        "    else:\n",
        "        print(f\"  ✅ 类别相对平衡 (比例: {class_ratio:.2f})\")\n",
        "    \n",
        "    # 性能建议\n",
        "    if not val_df.empty:\n",
        "        overfitting = train_accuracy - val_accuracy\n",
        "        if overfitting > 0.1:\n",
        "            print(f\"  ⚠️  可能存在过拟合 (训练-验证差距: {overfitting:.3f})，建议调整正则化参数\")\n",
        "        elif overfitting < -0.05:\n",
        "            print(f\"  🤔 验证集性能优于训练集，可能数据分布不一致\")\n",
        "        else:\n",
        "            print(f\"  ✅ 模型泛化能力良好 (训练-验证差距: {overfitting:.3f})\")\n",
        "    \n",
        "    # 特征建议\n",
        "    high_importance_features = len(importance_df[importance_df['importance'] > importance_df['importance'].mean()])\n",
        "    print(f\"  🔧 {high_importance_features}/{len(final_feature_cols)} 个特征高于平均重要性\")\n",
        "    \n",
        "    if features_80 < len(final_feature_cols) * 0.5:\n",
        "        print(f\"  💡 可考虑特征选择：前{features_80}个特征已贡献80%重要性\")\n",
        "    \n",
        "    print(f\"\\n🚀 下一步建议:\")\n",
        "    print(f\"  1. 📈 收集更多样本数据，特别是少数类别\")\n",
        "    print(f\"  2. 🔧 基于特征重要性进行特征选择和工程优化\")\n",
        "    print(f\"  3. ⚙️  尝试其他算法对比 (Random Forest, SVM, Neural Network)\")\n",
        "    print(f\"  4. 🎯 进行超参数调优以进一步提升性能\")\n",
        "    print(f\"  5. 📊 在实际场景中部署和监控模型性能\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"✅ 分析完成！模型已准备就绪。\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"❌ 无法生成模型总结\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 模型保存与加载\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存训练好的模型\n",
        "if 'model' in locals():\n",
        "    import joblib\n",
        "    \n",
        "    model_filename = 'pcdn_traffic_classifier.pkl'\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"💾 模型已保存为: {model_filename}\")\n",
        "    \n",
        "    # 保存特征列表\n",
        "    import json\n",
        "    features_filename = 'model_features.json'\n",
        "    with open(features_filename, 'w') as f:\n",
        "        json.dump(final_feature_cols, f, indent=2)\n",
        "    print(f\"📝 特征列表已保存为: {features_filename}\")\n",
        "    \n",
        "    # 保存特征重要性\n",
        "    importance_filename = 'feature_importance.csv'\n",
        "    importance_df.to_csv(importance_filename, index=False, encoding='utf-8-sig')\n",
        "    print(f\"📊 特征重要性已保存为: {importance_filename}\")\n",
        "    \n",
        "    print(\"\\n🎉 所有文件保存完成！\")\n",
        "    \n",
        "    # 演示如何加载模型\n",
        "    print(\"\\n📖 模型加载示例代码:\")\n",
        "    print(\"\"\"\n",
        "    import joblib\n",
        "    import json\n",
        "    \n",
        "    # 加载模型\n",
        "    loaded_model = joblib.load('pcdn_traffic_classifier.pkl')\n",
        "    \n",
        "    # 加载特征列表\n",
        "    with open('model_features.json', 'r') as f:\n",
        "        feature_columns = json.load(f)\n",
        "    \n",
        "    # 对新数据进行预测\n",
        "    # new_data = preprocess_new_data(raw_data)  # 需要相同的预处理\n",
        "    # predictions = loaded_model.predict(new_data[feature_columns])\n",
        "    \"\"\")\n",
        "else:\n",
        "    print(\"❌ 没有训练好的模型可保存\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
