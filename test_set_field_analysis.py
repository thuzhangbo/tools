#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Testing_setå­—æ®µåˆ†å¸ƒåˆ†æå·¥å…·
ç”¨äºæ£€æµ‹æ•°æ®æ³„éœ²å’Œå­—æ®µåˆ†å¸ƒå¼‚å¸¸
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

def load_testing_data():
    """
    åŠ è½½Testing_setæ•°æ®
    """
    base_path = "pcdn_32_pkts_2class_feature_enhance_v17.4_dataset/Testing_set"
    
    dataframes = []
    file_info = []
    
    for app_dir in ['APP_0', 'APP_1']:
        app_path = os.path.join(base_path, app_dir)
        if os.path.exists(app_path):
            csv_files = glob.glob(os.path.join(app_path, '*.csv'))
            
            for file_path in csv_files:
                try:
                    df = pd.read_csv(file_path)
                    df['source_file'] = os.path.basename(file_path)
                    df['app_category'] = app_dir
                    df['label'] = 0 if app_dir == 'APP_0' else 1
                    
                    dataframes.append(df)
                    file_info.append({
                        'file': os.path.basename(file_path),
                        'category': app_dir,
                        'rows': len(df),
                        'columns': len(df.columns)
                    })
                    print(f"âœ… åŠ è½½: {os.path.basename(file_path)} ({app_dir}) - {len(df)} è¡Œ")
                    
                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥: {file_path} - {e}")
    
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"\nğŸ“Š Testing_setæ€»è§ˆ:")
        print(f"  æ€»æ–‡ä»¶æ•°: {len(file_info)}")
        print(f"  æ€»è¡Œæ•°: {len(combined_df)}")
        print(f"  åˆ—æ•°: {len(combined_df.columns)}")
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: {combined_df['label'].value_counts().to_dict()}")
        
        return combined_df, file_info
    else:
        print("âŒ æœªæ‰¾åˆ°Testing_setæ•°æ®")
        return None, []

def analyze_field_distribution(df, field_name, top_n=20):
    """
    åˆ†ææŒ‡å®šå­—æ®µçš„åˆ†å¸ƒæƒ…å†µ
    """
    if field_name not in df.columns:
        print(f"âŒ å­—æ®µ '{field_name}' ä¸å­˜åœ¨")
        print(f"å¯ç”¨å­—æ®µ: {list(df.columns)}")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ” å­—æ®µåˆ†æ: {field_name}")
    print(f"{'='*60}")
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ•°æ®ç±»å‹: {df[field_name].dtype}")
    print(f"  éç©ºå€¼æ•°é‡: {df[field_name].notna().sum()}")
    print(f"  ç©ºå€¼æ•°é‡: {df[field_name].isna().sum()}")
    print(f"  å”¯ä¸€å€¼æ•°é‡: {df[field_name].nunique()}")
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†æ
    print(f"\nğŸ·ï¸ æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†æ:")
    for label in sorted(df['label'].unique()):
        label_name = "æ­£å¸¸æµé‡" if label == 0 else "PCDNæµé‡"
        subset = df[df['label'] == label]
        print(f"\n  {label_name} (æ ‡ç­¾={label}):")
        print(f"    æ ·æœ¬æ•°: {len(subset)}")
        print(f"    å”¯ä¸€å€¼æ•°: {subset[field_name].nunique()}")
        
        if df[field_name].dtype in ['object', 'string']:
            # å­—ç¬¦ä¸²ç±»å‹ - æ˜¾ç¤ºæœ€é¢‘ç¹çš„å€¼
            value_counts = subset[field_name].value_counts()
            print(f"    æœ€é¢‘ç¹çš„å€¼ (å‰{min(top_n, len(value_counts))}ä¸ª):")
            for i, (value, count) in enumerate(value_counts.head(top_n).items()):
                percentage = (count / len(subset)) * 100
                print(f"      {i+1:2d}. '{value}' - {count}æ¬¡ ({percentage:.1f}%)")
        else:
            # æ•°å€¼ç±»å‹ - æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"    ç»Ÿè®¡ä¿¡æ¯:")
            print(f"      æœ€å°å€¼: {subset[field_name].min()}")
            print(f"      æœ€å¤§å€¼: {subset[field_name].max()}")
            print(f"      å¹³å‡å€¼: {subset[field_name].mean():.4f}")
            print(f"      ä¸­ä½æ•°: {subset[field_name].median():.4f}")
            print(f"      æ ‡å‡†å·®: {subset[field_name].std():.4f}")
    
    # æ£€æŸ¥æ•°æ®æ³„éœ²é£é™©
    print(f"\nğŸš¨ æ•°æ®æ³„éœ²é£é™©æ£€æŸ¥:")
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„å€¼åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­
    if df[field_name].dtype in ['object', 'string']:
        # å­—ç¬¦ä¸²ç±»å‹ - æ£€æŸ¥å€¼é‡å 
        app0_values = set(df[df['label'] == 0][field_name].dropna().unique())
        app1_values = set(df[df['label'] == 1][field_name].dropna().unique())
        
        overlap = app0_values & app1_values
        print(f"  å€¼é‡å æ£€æŸ¥:")
        print(f"    APP_0å”¯ä¸€å€¼æ•°: {len(app0_values)}")
        print(f"    APP_1å”¯ä¸€å€¼æ•°: {len(app1_values)}")
        print(f"    é‡å å€¼æ•°: {len(overlap)}")
        
        if overlap:
            print(f"    âš ï¸ å‘ç°é‡å å€¼ (å‰10ä¸ª): {list(overlap)[:10]}")
        else:
            print(f"    âœ… æ— é‡å å€¼")
    
    # 2. æ£€æŸ¥å­—æ®µæ˜¯å¦åŒ…å«æ˜æ˜¾çš„åˆ†ç±»ä¿¡æ¯
    suspicious_patterns = []
    if df[field_name].dtype in ['object', 'string']:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«IPåœ°å€ã€URLç­‰å¯èƒ½æ³„éœ²ä¿¡æ¯çš„å†…å®¹
        sample_values = df[field_name].dropna().astype(str).head(100)
        for value in sample_values:
            if any(pattern in str(value).lower() for pattern in ['http', 'www', 'com', 'net', 'org']):
                suspicious_patterns.append('URLæ¨¡å¼')
                break
            if '.' in str(value) and str(value).count('.') >= 3:
                suspicious_patterns.append('IPåœ°å€æ¨¡å¼')
                break
    
    if suspicious_patterns:
        print(f"    âš ï¸ å‘ç°å¯ç–‘æ¨¡å¼: {suspicious_patterns}")
    else:
        print(f"    âœ… æœªå‘ç°æ˜æ˜¾å¯ç–‘æ¨¡å¼")
    
    # 3. æ£€æŸ¥å­—æ®µåˆ†å¸ƒæ˜¯å¦è¿‡äºç®€å•
    if df[field_name].nunique() <= 2:
        print(f"    âš ï¸ å­—æ®µåªæœ‰{df[field_name].nunique()}ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½è¿‡äºç®€å•")
    elif df[field_name].nunique() > len(df) * 0.9:
        print(f"    âš ï¸ å­—æ®µå”¯ä¸€å€¼è¿‡å¤š({df[field_name].nunique()}/{len(df)})ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©")
    else:
        print(f"    âœ… å­—æ®µå”¯ä¸€å€¼æ•°é‡åˆç†")

def visualize_field_distribution(df, field_name, max_categories=20):
    """
    å¯è§†åŒ–å­—æ®µåˆ†å¸ƒ
    """
    if field_name not in df.columns:
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'å­—æ®µåˆ†å¸ƒåˆ†æ: {field_name}', fontsize=16)
    
    # 1. æ•´ä½“åˆ†å¸ƒ
    ax1 = axes[0, 0]
    if df[field_name].dtype in ['object', 'string']:
        value_counts = df[field_name].value_counts().head(max_categories)
        value_counts.plot(kind='bar', ax=ax1)
        ax1.set_title('æ•´ä½“åˆ†å¸ƒ (å‰20ä¸ªå€¼)')
        ax1.tick_params(axis='x', rotation=45)
    else:
        df[field_name].hist(bins=30, ax=ax1)
        ax1.set_title('æ•´ä½“åˆ†å¸ƒ')
    ax1.set_ylabel('é¢‘æ¬¡')
    
    # 2. æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†å¸ƒ
    ax2 = axes[0, 1]
    for label in sorted(df['label'].unique()):
        label_name = "æ­£å¸¸æµé‡" if label == 0 else "PCDNæµé‡"
        subset = df[df['label'] == label]
        
        if df[field_name].dtype in ['object', 'string']:
            value_counts = subset[field_name].value_counts().head(max_categories)
            ax2.plot(value_counts.index, value_counts.values, 'o-', label=label_name, alpha=0.7)
        else:
            ax2.hist(subset[field_name].dropna(), alpha=0.7, label=label_name, bins=20)
    
    ax2.set_title('æŒ‰æ ‡ç­¾åˆ†ç»„åˆ†å¸ƒ')
    ax2.legend()
    ax2.tick_params(axis='x', rotation=45)
    
    # 3. æ ‡ç­¾åˆ†å¸ƒé¥¼å›¾
    ax3 = axes[1, 0]
    label_counts = df['label'].value_counts()
    labels = ['æ­£å¸¸æµé‡' if x == 0 else 'PCDNæµé‡' for x in label_counts.index]
    ax3.pie(label_counts.values, labels=labels, autopct='%1.1f%%')
    ax3.set_title('æ ‡ç­¾åˆ†å¸ƒ')
    
    # 4. å­—æ®µå€¼åˆ†å¸ƒé¥¼å›¾ (ä»…å¯¹åˆ†ç±»å­—æ®µ)
    ax4 = axes[1, 1]
    if df[field_name].dtype in ['object', 'string'] and df[field_name].nunique() <= 10:
        value_counts = df[field_name].value_counts()
        ax4.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')
        ax4.set_title(f'{field_name} å€¼åˆ†å¸ƒ')
    else:
        ax4.text(0.5, 0.5, f'å­—æ®µæœ‰{df[field_name].nunique()}ä¸ªå”¯ä¸€å€¼\næ— æ³•æ˜¾ç¤ºé¥¼å›¾', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title(f'{field_name} å€¼åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.show()

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ” Testing_setå­—æ®µåˆ†å¸ƒåˆ†æå·¥å…·")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    df, file_info = load_testing_data()
    if df is None:
        return
    
    # æ˜¾ç¤ºå¯ç”¨å­—æ®µ
    print(f"\nğŸ“‹ å¯ç”¨å­—æ®µåˆ—è¡¨:")
    for i, col in enumerate(df.columns, 1):
        print(f"  {i:2d}. {col}")
    
    # äº¤äº’å¼åˆ†æ
    while True:
        print(f"\n{'='*60}")
        field_name = input("è¯·è¾“å…¥è¦åˆ†æçš„å­—æ®µå (è¾“å…¥ 'quit' é€€å‡º): ").strip()
        
        if field_name.lower() == 'quit':
            break
        
        if field_name not in df.columns:
            print(f"âŒ å­—æ®µ '{field_name}' ä¸å­˜åœ¨")
            continue
        
        # åˆ†æå­—æ®µ
        analyze_field_distribution(df, field_name)
        
        # è¯¢é—®æ˜¯å¦å¯è§†åŒ–
        show_plot = input("\næ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨? (y/n): ").strip().lower()
        if show_plot == 'y':
            visualize_field_distribution(df, field_name)
    
    print("\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
