{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'bysj (Python 3.10.18)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n bysj ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDN vs Normal Traffic Classification using XGBoost\n",
        "\n",
        "è¿™ä¸ªé¡¹ç›®ä½¿ç”¨XGBoostå¯¹æ­£å¸¸æµé‡å’ŒPCDNæµé‡è¿›è¡ŒäºŒåˆ†ç±»ã€‚\n",
        "\n",
        "## æ•°æ®é›†ç»“æ„\n",
        "- Training_set/APP_0: æ­£å¸¸æµé‡\n",
        "- Training_set/APP_1: PCDNæµé‡\n",
        "- Validation_set: éªŒè¯é›†\n",
        "- Testing_set: æµ‹è¯•é›†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from scipy import stats  # ç”¨äºåºåˆ—ç‰¹å¾çš„ååº¦å’Œå³°åº¦è®¡ç®—\n",
        "import ast  # ç”¨äºå®‰å…¨è§£ææ•°ç»„å­—ç¬¦ä¸²\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"åº“å¯¼å…¥å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®åŠ è½½å‡½æ•°\n",
        "def load_data_from_directory(base_path, label):\n",
        "    \"\"\"\n",
        "    ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰CSVæ–‡ä»¶å¹¶æ·»åŠ æ ‡ç­¾\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(base_path, '*.csv'))\n",
        "    dataframes = []\n",
        "    \n",
        "    for file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df['label'] = label  # æ·»åŠ æ ‡ç­¾åˆ—\n",
        "            df['source_file'] = os.path.basename(file)  # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯\n",
        "            dataframes.append(df)\n",
        "            print(f\"æˆåŠŸåŠ è½½æ–‡ä»¶: {file}, æ ·æœ¬æ•°: {len(df)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"åŠ è½½æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}\")\n",
        "    \n",
        "    if dataframes:\n",
        "        return pd.concat(dataframes, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# ä½¿ç”¨ç›¸å¯¹è·¯å¾„å®šä¹‰æ•°æ®ç›®å½•\n",
        "# æ•°æ®é›†åº”è¯¥ä¸æ­¤notebookåœ¨åŒä¸€ç›®å½•ä¸‹\n",
        "data_dir = './pcdn_32_pkts_2class_feature_enhance_v17.4_dataset'\n",
        "\n",
        "# æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨\n",
        "if not os.path.exists(data_dir):\n",
        "    print(f\"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}\")\n",
        "    print(\"è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å¤¹ä¸notebookåœ¨åŒä¸€ç›®å½•ä¸‹\")\n",
        "    print(\"å½“å‰å·¥ä½œç›®å½•:\", os.getcwd())\n",
        "    print(\"å½“å‰ç›®å½•å†…å®¹:\", [f for f in os.listdir('.') if not f.startswith('.')])\n",
        "    \n",
        "    # å°è¯•æŸ¥æ‰¾æ•°æ®ç›®å½•\n",
        "    possible_dirs = [d for d in os.listdir('.') if 'pcdn' in d.lower() and os.path.isdir(d)]\n",
        "    if possible_dirs:\n",
        "        print(f\"å‘ç°å¯èƒ½çš„æ•°æ®ç›®å½•: {possible_dirs}\")\n",
        "        data_dir = possible_dirs[0]\n",
        "        print(f\"ä½¿ç”¨æ•°æ®ç›®å½•: {data_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"æ‰¾ä¸åˆ°æ•°æ®ç›®å½•ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ä½ç½®\")\n",
        "else:\n",
        "    print(f\"âœ… æ‰¾åˆ°æ•°æ®ç›®å½•: {data_dir}\")\n",
        "\n",
        "# åŠ è½½è®­ç»ƒæ•°æ®\n",
        "print(\"\\nå¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®...\")\n",
        "train_normal = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)  # æ­£å¸¸æµé‡æ ‡ç­¾ä¸º0\n",
        "train_pcdn = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)    # PCDNæµé‡æ ‡ç­¾ä¸º1\n",
        "\n",
        "# åˆå¹¶è®­ç»ƒæ•°æ®\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "print(f\"\\nè®­ç»ƒæ•°æ®åŠ è½½å®Œæˆï¼\")\n",
        "print(f\"æ­£å¸¸æµé‡æ ·æœ¬æ•°: {len(train_normal)}\")\n",
        "print(f\"PCDNæµé‡æ ·æœ¬æ•°: {len(train_pcdn)}\")\n",
        "print(f\"æ€»è®­ç»ƒæ ·æœ¬æ•°: {len(train_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½éªŒè¯å’Œæµ‹è¯•æ•°æ®\n",
        "print(\"å¼€å§‹åŠ è½½éªŒè¯æ•°æ®...\")\n",
        "val_normal = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥éªŒè¯æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"âš ï¸ è­¦å‘Š: éªŒè¯é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†\")\n",
        "    val_data = pd.DataFrame()\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "\n",
        "print(\"\\nå¼€å§‹åŠ è½½æµ‹è¯•æ•°æ®...\")\n",
        "test_normal = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"âš ï¸ è­¦å‘Š: æµ‹è¯•é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†\")\n",
        "    test_data = pd.DataFrame()\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "\n",
        "print(f\"\\néªŒè¯é›†æ ·æœ¬æ•°: {len(val_data)} (æ­£å¸¸: {len(val_normal)}, PCDN: {len(val_pcdn)})\")\n",
        "print(f\"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)} (æ­£å¸¸: {len(test_normal)}, PCDN: {len(test_pcdn)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®æ¢ç´¢å’ŒåŸºæœ¬ä¿¡æ¯\n",
        "print(\"=== è®­ç»ƒæ•°æ®åŸºæœ¬ä¿¡æ¯ ===\")\n",
        "print(f\"æ•°æ®å½¢çŠ¶: {train_data.shape}\")\n",
        "print(f\"\\nåˆ—å ({len(train_data.columns)}ä¸ªç‰¹å¾):\")\n",
        "print(train_data.columns.tolist())\n",
        "\n",
        "print(\"\\n=== æ ‡ç­¾åˆ†å¸ƒ ===\")\n",
        "label_counts = train_data['label'].value_counts()\n",
        "print(label_counts)\n",
        "print(f\"æ­£å¸¸æµé‡æ¯”ä¾‹: {label_counts[0]/len(train_data)*100:.2f}%\")\n",
        "print(f\"PCDNæµé‡æ¯”ä¾‹: {label_counts[1]/len(train_data)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== æ•°æ®ç±»å‹ä¿¡æ¯ ===\")\n",
        "print(train_data.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®é¢„å¤„ç†\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # åˆ é™¤æŒ‡å®šçš„ä¸éœ€è¦çš„åˆ—\n",
        "    columns_to_drop = [\n",
        "        'source_file',  # æºæ–‡ä»¶ä¿¡æ¯ï¼ˆæ·»åŠ çš„è¾…åŠ©åˆ—ï¼‰\n",
        "        'frame.number', # å¸§ç¼–å·\n",
        "        'frame.time_relative', # ç›¸å¯¹æ—¶é—´\n",
        "        'ip.version',   # IPç‰ˆæœ¬\n",
        "        'ip.ttl',       # IP TTL\n",
        "        'ip.src', 'ip.dst',  # IPåœ°å€\n",
        "        'ipv6.plen',    # IPv6 payloadé•¿åº¦\n",
        "        'ipv6.nxt',     # IPv6 ä¸‹ä¸€ä¸ªå¤´\n",
        "        'ipv6.src', 'ipv6.dst',  # IPv6åœ°å€\n",
        "        '_ws.col.Protocol', # Wiresharkåè®®åˆ—\n",
        "        'ssl.handshake.extensions_server_name',  # SSLæ‰©å±•ä¿¡æ¯\n",
        "        'eth.src',      # MACåœ°å€\n",
        "        'pcap_duration', # PCAPæŒç»­æ—¶é—´\n",
        "        'app',          # åº”ç”¨ç¨‹åº\n",
        "        'os',           # æ“ä½œç³»ç»Ÿ\n",
        "        'date',         # æ—¥æœŸ\n",
        "        'flow_id',      # æµID\n",
        "        'dpi_file_name', # DPIæ–‡ä»¶å\n",
        "        'dpi_five_tuple', # äº”å…ƒç»„\n",
        "        'dpi_rule_result', # DPIè§„åˆ™ç»“æœ\n",
        "        'dpi_label',    # DPIæ ‡ç­¾\n",
        "        'ulProtoID',    # ä¸Šå±‚åè®®ID\n",
        "        'dpi_rule_pkt', # DPIè§„åˆ™åŒ…\n",
        "        'dpi_packets',  # DPIåŒ…æ•°\n",
        "        'dpi_bytes',    # DPIå­—èŠ‚æ•°\n",
        "        'label_source', # æ ‡ç­¾æº\n",
        "        'id',           # IDå­—æ®µ\n",
        "        'category'      # categoryå­—æ®µ\n",
        "    ]\n",
        "    \n",
        "    # åˆ é™¤å­˜åœ¨çš„åˆ—\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
        "    \n",
        "    # ä¿ç•™æ‰€æœ‰å…¶ä»–ç‰¹å¾ï¼ˆåŒ…æ‹¬æ•°ç»„ç‰¹å¾ï¼‰ï¼Œè¿™äº›å¯èƒ½å¯¹åˆ†ç±»æœ‰ç”¨\n",
        "    # æ•°ç»„ç‰¹å¾å¦‚ ip_direction, pkt_len, iat ç­‰å°†åœ¨åç»­æ­¥éª¤ä¸­è¿›è¡Œç¼–ç å¤„ç†\n",
        "    \n",
        "    # å¤„ç†ç¼ºå¤±å€¼\n",
        "    df_processed = df_processed.fillna(0)\n",
        "    \n",
        "    # å¤„ç†æ— ç©·å¤§å€¼\n",
        "    df_processed = df_processed.replace([np.inf, -np.inf], 0)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# é¢„å¤„ç†è®­ç»ƒæ•°æ®\n",
        "train_processed = preprocess_data(train_data)\n",
        "\n",
        "# æ™ºèƒ½å¤„ç†ç©ºæ•°æ®é›†çš„æƒ…å†µ\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "val_exists = len(val_data) > 0\n",
        "test_exists = len(test_data) > 0\n",
        "\n",
        "print(f\"éªŒè¯é›†å­˜åœ¨: {'æ˜¯' if val_exists else 'å¦'}\")\n",
        "print(f\"æµ‹è¯•é›†å­˜åœ¨: {'æ˜¯' if test_exists else 'å¦'}\")\n",
        "\n",
        "if not val_exists and not test_exists:\n",
        "    # ä¸¤ä¸ªæ•°æ®é›†éƒ½ä¸ºç©ºï¼Œè¿›è¡Œ60/20/20åˆ†å‰²\n",
        "    print(\"éªŒè¯é›†å’Œæµ‹è¯•é›†éƒ½ä¸ºç©ºï¼Œä»è®­ç»ƒæ•°æ®åˆ†å‰²ä¸º 60% è®­ç»ƒ / 20% éªŒè¯ / 20% æµ‹è¯•\")\n",
        "    train_temp, temp_split = train_test_split(\n",
        "        train_processed, test_size=0.4, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    val_processed, test_processed = train_test_split(\n",
        "        temp_split, test_size=0.5, random_state=42, \n",
        "        stratify=temp_split['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    \n",
        "elif not val_exists:\n",
        "    # åªæœ‰éªŒè¯é›†ä¸ºç©ºï¼Œåˆ†å‰²80/20\n",
        "    print(\"éªŒè¯é›†ä¸ºç©ºï¼Œä»è®­ç»ƒæ•°æ®åˆ†å‰²ä¸º 80% è®­ç»ƒ / 20% éªŒè¯\")\n",
        "    train_temp, val_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    test_processed = preprocess_data(test_data)\n",
        "    \n",
        "elif not test_exists:\n",
        "    # åªæœ‰æµ‹è¯•é›†ä¸ºç©ºï¼Œåˆ†å‰²80/20\n",
        "    print(\"æµ‹è¯•é›†ä¸ºç©ºï¼Œä»è®­ç»ƒæ•°æ®åˆ†å‰²ä¸º 80% è®­ç»ƒ / 20% æµ‹è¯•\")\n",
        "    train_temp, test_processed = train_test_split(\n",
        "        train_processed, test_size=0.2, random_state=42, \n",
        "        stratify=train_processed['label']\n",
        "    )\n",
        "    train_processed = train_temp\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    \n",
        "else:\n",
        "    # ä¸¤ä¸ªæ•°æ®é›†éƒ½å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨\n",
        "    print(\"ä½¿ç”¨åŸå§‹çš„éªŒè¯é›†å’Œæµ‹è¯•é›†\")\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    test_processed = preprocess_data(test_data)\n",
        "\n",
        "print(f\"é¢„å¤„ç†åè®­ç»ƒæ•°æ®å½¢çŠ¶: {train_processed.shape}\")\n",
        "print(f\"é¢„å¤„ç†åéªŒè¯æ•°æ®å½¢çŠ¶: {val_processed.shape}\")\n",
        "print(f\"é¢„å¤„ç†åæµ‹è¯•æ•°æ®å½¢çŠ¶: {test_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†\n",
        "# æå–ç‰¹å¾å’Œæ ‡ç­¾\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"é¢„å¤„ç†åç‰¹å¾æ•°é‡: {len(feature_columns)}\")\n",
        "print(f\"ç‰¹å¾åˆ—è¡¨å‰10ä¸ª: {feature_columns[:10]}\")\n",
        "\n",
        "# å¤„ç†åºåˆ—ç‰¹å¾ - è¿™äº›å­—æ®µåŒ…å«ç½‘ç»œåŒ…çš„æ—¶åºä¿¡æ¯\n",
        "def process_sequence_features(df):\n",
        "    \"\"\"\n",
        "    å¤„ç†åºåˆ—ç±»å‹çš„ç‰¹å¾ (pkt_len, ip_direction, iat)\n",
        "    \n",
        "    å¯¹äºXGBoostè¿™ç§åŸºäºæ ‘çš„ç®—æ³•ï¼Œéœ€è¦å°†åºåˆ—æ•°æ®è½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„ç»Ÿè®¡ç‰¹å¾ï¼š\n",
        "    1. pkt_len: åŒ…é•¿åº¦åºåˆ— - åæ˜ æµé‡çš„æ•°æ®ä¼ è¾“æ¨¡å¼\n",
        "    2. ip_direction: IPæ–¹å‘åºåˆ— - åæ˜ é€šä¿¡çš„æ–¹å‘æ¨¡å¼  \n",
        "    3. iat: åŒ…é—´åˆ°è¾¾æ—¶é—´é—´éš”åºåˆ— - åæ˜ æµé‡çš„æ—¶é—´ç‰¹å¾\n",
        "    \n",
        "    XGBoostæ— æ³•ç›´æ¥å¤„ç†å˜é•¿åºåˆ—ï¼Œéœ€è¦æå–å›ºå®šç»´åº¦çš„ç‰¹å¾\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    # å®šä¹‰åºåˆ—ç‰¹å¾åŠå…¶å«ä¹‰\n",
        "    sequence_columns = {\n",
        "        'ip_direction': 'ç½‘ç»œåŒ…æ–¹å‘åºåˆ— (0=å‡ºç«™, 1=å…¥ç«™)',\n",
        "        'pkt_len': 'ç½‘ç»œåŒ…é•¿åº¦åºåˆ—',\n",
        "        'iat': 'åŒ…é—´åˆ°è¾¾æ—¶é—´é—´éš”åºåˆ—'\n",
        "    }\n",
        "    \n",
        "    for col, description in sequence_columns.items():\n",
        "        if col in df_copy.columns:\n",
        "            print(f\"å¤„ç†åºåˆ—ç‰¹å¾: {col} - {description}\")\n",
        "            \n",
        "            try:\n",
        "                # å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å€¼åˆ—è¡¨ï¼ˆé¿å…ä½¿ç”¨evalï¼‰\n",
        "                def safe_parse_array(x):\n",
        "                    \"\"\"å®‰å…¨è§£ææ•°ç»„å­—ç¬¦ä¸²\"\"\"\n",
        "                    if pd.isna(x) or x == '' or x == '[]':\n",
        "                        return []\n",
        "                    if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "                        try:\n",
        "                            # ä½¿ç”¨ast.literal_evalæ›¿ä»£evalï¼Œæ›´å®‰å…¨\n",
        "                            return ast.literal_eval(x)\n",
        "                        except (ValueError, SyntaxError):\n",
        "                            return []\n",
        "                    return []\n",
        "                \n",
        "                sequences = df_copy[col].apply(safe_parse_array)\n",
        "                \n",
        "                # === åŸºç¡€ç»Ÿè®¡ç‰¹å¾ ===\n",
        "                df_copy[f'{col}_mean'] = sequences.apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_std'] = sequences.apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_min'] = sequences.apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_max'] = sequences.apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_median'] = sequences.apply(lambda x: np.median(x) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_range'] = sequences.apply(lambda x: (np.max(x) - np.min(x)) if len(x) > 0 else 0)\n",
        "                \n",
        "                # === åˆ†ä½æ•°ç‰¹å¾ ===\n",
        "                df_copy[f'{col}_q25'] = sequences.apply(lambda x: np.percentile(x, 25) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_q75'] = sequences.apply(lambda x: np.percentile(x, 75) if len(x) > 0 else 0)\n",
        "                df_copy[f'{col}_iqr'] = df_copy[f'{col}_q75'] - df_copy[f'{col}_q25']\n",
        "                \n",
        "                # === åºåˆ—é•¿åº¦ç‰¹å¾ ===\n",
        "                df_copy[f'{col}_len'] = sequences.apply(lambda x: len(x))\n",
        "                \n",
        "                # === åºåˆ—æ¨¡å¼ç‰¹å¾ ===\n",
        "                # å˜å¼‚ç³»æ•° (æ ‡å‡†å·®/å‡å€¼) - è¡¡é‡åºåˆ—çš„ç›¸å¯¹å˜åŒ–ç¨‹åº¦\n",
        "                df_copy[f'{col}_cv'] = sequences.apply(lambda x: np.std(x)/np.mean(x) if len(x) > 0 and np.mean(x) != 0 else 0)\n",
        "                \n",
        "                # ååº¦å’Œå³°åº¦ - è¡¡é‡åºåˆ—åˆ†å¸ƒå½¢çŠ¶\n",
        "                df_copy[f'{col}_skew'] = sequences.apply(lambda x: stats.skew(x) if len(x) > 1 else 0)\n",
        "                df_copy[f'{col}_kurtosis'] = sequences.apply(lambda x: stats.kurtosis(x) if len(x) > 1 else 0)\n",
        "                \n",
        "                # === åºåˆ—ç‰¹æœ‰çš„ç‰¹å¾ ===\n",
        "                if col == 'ip_direction':\n",
        "                    # å¯¹äºæ–¹å‘åºåˆ—ï¼šç»Ÿè®¡å‡ºç«™/å…¥ç«™æ¯”ä¾‹\n",
        "                    df_copy[f'{col}_out_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 0])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_in_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i == 1])/len(x) if len(x) > 0 else 0)\n",
        "                    # æ–¹å‘å˜åŒ–æ¬¡æ•° - åæ˜ é€šä¿¡æ¨¡å¼\n",
        "                    df_copy[f'{col}_changes'] = sequences.apply(lambda x: sum([1 for i in range(1, len(x)) if x[i] != x[i-1]]) if len(x) > 1 else 0)\n",
        "                \n",
        "                elif col == 'pkt_len':\n",
        "                    # å¯¹äºåŒ…é•¿åº¦åºåˆ—ï¼šå°åŒ…/å¤§åŒ…æ¯”ä¾‹\n",
        "                    df_copy[f'{col}_small_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i <= 64])/len(x) if len(x) > 0 else 0)\n",
        "                    df_copy[f'{col}_large_pkt_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i >= 1400])/len(x) if len(x) > 0 else 0)\n",
        "                \n",
        "                elif col == 'iat':\n",
        "                    # å¯¹äºæ—¶é—´é—´éš”åºåˆ—ï¼šçªå‘æ€§æ£€æµ‹\n",
        "                    df_copy[f'{col}_burst_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i < 0.01])/len(x) if len(x) > 0 else 0)  # å°äº10msçš„æ¯”ä¾‹\n",
        "                    df_copy[f'{col}_long_gap_ratio'] = sequences.apply(lambda x: sum([1 for i in x if i > 1.0])/len(x) if len(x) > 0 else 0)  # å¤§äº1sçš„æ¯”ä¾‹\n",
        "                \n",
        "                # === è¶‹åŠ¿ç‰¹å¾ ===\n",
        "                # åºåˆ—é€’å¢/é€’å‡è¶‹åŠ¿\n",
        "                def trend_analysis(seq):\n",
        "                    if len(seq) < 2:\n",
        "                        return 0, 0\n",
        "                    increasing = sum([1 for i in range(1, len(seq)) if seq[i] > seq[i-1]])\n",
        "                    decreasing = sum([1 for i in range(1, len(seq)) if seq[i] < seq[i-1]])\n",
        "                    return increasing/len(seq), decreasing/len(seq)\n",
        "                \n",
        "                trends = sequences.apply(trend_analysis)\n",
        "                df_copy[f'{col}_increasing_ratio'] = trends.apply(lambda x: x[0])\n",
        "                df_copy[f'{col}_decreasing_ratio'] = trends.apply(lambda x: x[1])\n",
        "                \n",
        "                # åˆ é™¤åŸå§‹åºåˆ—åˆ—\n",
        "                df_copy = df_copy.drop(columns=[col])\n",
        "                print(f\"  -> å·²ä» {col} æå– {len([c for c in df_copy.columns if c.startswith(col)])} ä¸ªç‰¹å¾\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  -> å¤„ç† {col} æ—¶å‡ºé”™ï¼Œå°†ç›´æ¥ç¼–ç : {e}\")\n",
        "                # å¦‚æœå¤„ç†å¤±è´¥ï¼Œå°±ç®€å•ç¼–ç \n",
        "                df_copy[col] = LabelEncoder().fit_transform(df_copy[col].astype(str))\n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "# å¤„ç†æ‰€æœ‰æ•°æ®é›†çš„åºåˆ—ç‰¹å¾\n",
        "print(\"å¼€å§‹å¤„ç†åºåˆ—ç‰¹å¾...\")\n",
        "print(\"=\" * 60)\n",
        "train_processed = process_sequence_features(train_processed)\n",
        "val_processed = process_sequence_features(val_processed)\n",
        "test_processed = process_sequence_features(test_processed)\n",
        "\n",
        "# æ›´æ–°ç‰¹å¾åˆ—è¡¨\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"\\nå¤„ç†åºåˆ—ç‰¹å¾åçš„ç‰¹å¾æ•°é‡: {len(feature_columns)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# æ£€æŸ¥å‰©ä½™çš„éæ•°å€¼åˆ—\n",
        "non_numeric_cols = []\n",
        "for col in feature_columns:\n",
        "    if not pd.api.types.is_numeric_dtype(train_processed[col]):\n",
        "        non_numeric_cols.append(col)\n",
        "\n",
        "print(f\"\\néœ€è¦ç¼–ç çš„éæ•°å€¼åˆ—æ•°é‡: {len(non_numeric_cols)}\")\n",
        "if non_numeric_cols:\n",
        "    print(f\"éæ•°å€¼åˆ—: {non_numeric_cols[:5]}...\")  # æ˜¾ç¤ºå‰5ä¸ª\n",
        "\n",
        "# å¯¹éæ•°å€¼åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç \n",
        "if non_numeric_cols:\n",
        "    print(\"å¼€å§‹å¯¹éæ•°å€¼åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç ...\")\n",
        "    for col in non_numeric_cols:\n",
        "        try:\n",
        "            le = LabelEncoder()\n",
        "            # åˆå¹¶æ‰€æœ‰æ•°æ®é›†çš„è¯¥åˆ—å€¼è¿›è¡Œç¼–ç \n",
        "            all_values = pd.concat([\n",
        "                train_processed[col].fillna('missing').astype(str),\n",
        "                val_processed[col].fillna('missing').astype(str),\n",
        "                test_processed[col].fillna('missing').astype(str)\n",
        "            ])\n",
        "            le.fit(all_values)\n",
        "            \n",
        "            train_processed[col] = le.transform(train_processed[col].fillna('missing').astype(str))\n",
        "            val_processed[col] = le.transform(val_processed[col].fillna('missing').astype(str))\n",
        "            test_processed[col] = le.transform(test_processed[col].fillna('missing').astype(str))\n",
        "            print(f\"  âœ“ å·²ç¼–ç : {col}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— ç¼–ç å¤±è´¥ {col}: {e}\")\n",
        "            # ç¼–ç å¤±è´¥çš„åˆ—ç›´æ¥åˆ é™¤\n",
        "            if col in train_processed.columns:\n",
        "                train_processed = train_processed.drop(columns=[col])\n",
        "                val_processed = val_processed.drop(columns=[col])\n",
        "                test_processed = test_processed.drop(columns=[col])\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰æ•°æ®é›†å…·æœ‰ç›¸åŒçš„ç‰¹å¾åˆ—\n",
        "print(\"\\nğŸ”§ æ£€æŸ¥æ•°æ®é›†ç‰¹å¾ä¸€è‡´æ€§...\")\n",
        "train_features = set(train_processed.columns) - {'label'}\n",
        "val_features = set(val_processed.columns) - {'label'}\n",
        "test_features = set(test_processed.columns) - {'label'}\n",
        "\n",
        "print(f\"è®­ç»ƒé›†ç‰¹å¾æ•°: {len(train_features)}\")\n",
        "print(f\"éªŒè¯é›†ç‰¹å¾æ•°: {len(val_features)}\")\n",
        "print(f\"æµ‹è¯•é›†ç‰¹å¾æ•°: {len(test_features)}\")\n",
        "\n",
        "# å–ä¸‰ä¸ªæ•°æ®é›†çš„ç‰¹å¾äº¤é›†ï¼Œç¡®ä¿ä¸€è‡´æ€§\n",
        "common_features = train_features.intersection(val_features).intersection(test_features)\n",
        "print(f\"å…±åŒç‰¹å¾æ•°: {len(common_features)}\")\n",
        "\n",
        "if len(common_features) < len(train_features):\n",
        "    print(\"âš ï¸ è­¦å‘Š: æ•°æ®é›†é—´ç‰¹å¾ä¸ä¸€è‡´ï¼Œä½¿ç”¨å…±åŒç‰¹å¾\")\n",
        "    # åªä¿ç•™å…±åŒç‰¹å¾\n",
        "    feature_cols_to_keep = list(common_features) + ['label']\n",
        "    train_processed = train_processed[feature_cols_to_keep]\n",
        "    val_processed = val_processed[feature_cols_to_keep]\n",
        "    test_processed = test_processed[feature_cols_to_keep]\n",
        "\n",
        "print(\"\\næ•°æ®é¢„å¤„ç†å®Œæˆï¼\")\n",
        "print(f\"æœ€ç»ˆç‰¹å¾æ•°é‡: {len(common_features)}\")\n",
        "print(\"âœ… æ‰€æœ‰æ•°æ®é›†ç‰¹å¾å·²å¯¹é½\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” åºåˆ—ç‰¹å¾å¤„ç†ç­–ç•¥è¯´æ˜\n",
        "\n",
        "### ä¸ºä»€ä¹ˆXGBoostéœ€è¦ç‰¹æ®Šå¤„ç†åºåˆ—ç‰¹å¾ï¼Ÿ\n",
        "\n",
        "**XGBoostçš„é™åˆ¶ï¼š**\n",
        "- XGBoostæ˜¯åŸºäºæ ‘çš„ç®—æ³•ï¼Œåªèƒ½å¤„ç†å›ºå®šç»´åº¦çš„è¡¨æ ¼æ•°æ®\n",
        "- æ— æ³•ç›´æ¥å¤„ç†å˜é•¿åºåˆ—ï¼ˆå¦‚[1,0,1,1,0]è¿™æ ·çš„æ•°ç»„ï¼‰\n",
        "- éœ€è¦å°†åºåˆ—è½¬æ¢ä¸ºå›ºå®šæ•°é‡çš„æ•°å€¼ç‰¹å¾\n",
        "\n",
        "### ğŸ“Š æˆ‘ä»¬æå–çš„åºåˆ—ç‰¹å¾ç±»å‹\n",
        "\n",
        "**1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾**\n",
        "- å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§/æœ€å°å€¼ã€ä¸­ä½æ•°ã€å››åˆ†ä½æ•°\n",
        "- è¿™äº›ç‰¹å¾æ•è·åºåˆ—çš„æ•´ä½“åˆ†å¸ƒç‰¹æ€§\n",
        "\n",
        "**2. å½¢çŠ¶ç‰¹å¾**\n",
        "- ååº¦(skewness)ï¼šåºåˆ—åˆ†å¸ƒçš„å¯¹ç§°æ€§\n",
        "- å³°åº¦(kurtosis)ï¼šåºåˆ—åˆ†å¸ƒçš„å°–é”ç¨‹åº¦\n",
        "- å˜å¼‚ç³»æ•°ï¼šç›¸å¯¹å˜åŒ–ç¨‹åº¦\n",
        "\n",
        "**3. åºåˆ—æ¨¡å¼ç‰¹å¾**\n",
        "- **æ–¹å‘åºåˆ—(ip_direction)**ï¼šå‡ºç«™/å…¥ç«™æ¯”ä¾‹ã€æ–¹å‘å˜åŒ–æ¬¡æ•°\n",
        "- **åŒ…é•¿åº¦åºåˆ—(pkt_len)**ï¼šå°åŒ…/å¤§åŒ…æ¯”ä¾‹\n",
        "- **æ—¶é—´é—´éš”åºåˆ—(iat)**ï¼šçªå‘ä¼ è¾“/é•¿é—´éš”æ¯”ä¾‹\n",
        "\n",
        "**4. è¶‹åŠ¿ç‰¹å¾**\n",
        "- é€’å¢/é€’å‡è¶‹åŠ¿ï¼šåæ˜ åºåˆ—çš„æ—¶é—´æ¼”åŒ–æ¨¡å¼\n",
        "\n",
        "### ğŸ¯ è¿™äº›ç‰¹å¾å¯¹PCDNæ£€æµ‹çš„æ„ä¹‰\n",
        "\n",
        "**æ­£å¸¸æµé‡ vs PCDNæµé‡çš„åŒºåˆ«ï¼š**\n",
        "- **åŒ…å¤§å°æ¨¡å¼**ï¼šPCDNå¯èƒ½æœ‰ç‰¹å®šçš„åˆ†å—ä¼ è¾“æ¨¡å¼\n",
        "- **æ–¹å‘æ¨¡å¼**ï¼šPCDNçš„ä¸Šä¼ /ä¸‹è½½æ¯”ä¾‹å¯èƒ½ä¸åŒ\n",
        "- **æ—¶é—´æ¨¡å¼**ï¼šPCDNçš„ä¼ è¾“èŠ‚å¥å¯èƒ½æ›´è§„å¾‹æˆ–æ›´çªå‘\n",
        "- **åºåˆ—é•¿åº¦**ï¼šPCDNä¼šè¯å¯èƒ½æœ‰ç‰¹å®šçš„åŒ…æ•°é‡æ¨¡å¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åºåˆ—ç‰¹å¾å¤„ç†æ•ˆæœå±•ç¤º\n",
        "print(\"ğŸ” åºåˆ—ç‰¹å¾å¤„ç†æ•ˆæœåˆ†æ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ç»Ÿè®¡æ¯ä¸ªåºåˆ—ç‰¹å¾ç”Ÿæˆäº†å¤šå°‘ä¸ªæ–°ç‰¹å¾\n",
        "sequence_feature_counts = {}\n",
        "for original_col in ['ip_direction', 'pkt_len', 'iat']:\n",
        "    derived_features = [col for col in train_processed.columns if col.startswith(original_col)]\n",
        "    sequence_feature_counts[original_col] = len(derived_features)\n",
        "    print(f\"{original_col:15} -> ç”Ÿæˆäº† {len(derived_features):2d} ä¸ªç‰¹å¾\")\n",
        "    print(f\"                   åŒ…æ‹¬: {', '.join(derived_features[:5])}{'...' if len(derived_features) > 5 else ''}\")\n",
        "\n",
        "total_sequence_features = sum(sequence_feature_counts.values())\n",
        "print(f\"\\nğŸ“ˆ æ€»å…±ä»3ä¸ªåºåˆ—ç‰¹å¾ç”Ÿæˆäº† {total_sequence_features} ä¸ªæ•°å€¼ç‰¹å¾\")\n",
        "\n",
        "# å±•ç¤ºä¸€äº›å…³é”®ç‰¹å¾çš„å«ä¹‰\n",
        "print(f\"\\nğŸ“‹ å…³é”®ç‰¹å¾å«ä¹‰ç¤ºä¾‹:\")\n",
        "feature_meanings = {\n",
        "    'pkt_len_mean': 'å¹³å‡åŒ…å¤§å° - åæ˜ ä¼ è¾“æ•°æ®çš„ç²’åº¦',\n",
        "    'pkt_len_cv': 'åŒ…å¤§å°å˜å¼‚ç³»æ•° - åæ˜ ä¼ è¾“çš„è§„å¾‹æ€§',\n",
        "    'ip_direction_changes': 'æ–¹å‘å˜åŒ–æ¬¡æ•° - åæ˜ äº¤äº’æ¨¡å¼',\n",
        "    'iat_burst_ratio': 'çªå‘ä¼ è¾“æ¯”ä¾‹ - åæ˜ æ—¶é—´æ¨¡å¼',\n",
        "    'pkt_len_small_pkt_ratio': 'å°åŒ…æ¯”ä¾‹ - åæ˜ åè®®ç‰¹å¾'\n",
        "}\n",
        "\n",
        "for feat, meaning in feature_meanings.items():\n",
        "    if feat in train_processed.columns:\n",
        "        print(f\"  {feat:25}: {meaning}\")\n",
        "\n",
        "print(\"\\nâœ¨ è¿™äº›ç‰¹å¾å°†å¸®åŠ©XGBoostå­¦ä¹ æ­£å¸¸æµé‡å’ŒPCDNæµé‡çš„è¡Œä¸ºå·®å¼‚æ¨¡å¼\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®åˆ†å¸ƒå¯è§†åŒ–\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# æ ‡ç­¾åˆ†å¸ƒ\n",
        "label_counts = train_processed['label'].value_counts()\n",
        "axes[0, 0].pie(label_counts.values, labels=['Normal Traffic', 'PCDN Traffic'], autopct='%1.1f%%')\n",
        "axes[0, 0].set_title('Training Data Label Distribution')\n",
        "\n",
        "# é€‰æ‹©å‡ ä¸ªé‡è¦çš„æ•°å€¼ç‰¹å¾è¿›è¡Œå¯è§†åŒ–ï¼ˆåŒ…æ‹¬å¤„ç†åçš„åºåˆ—ç‰¹å¾ï¼‰\n",
        "numeric_features = ['ip.len', 'tcp.srcport', 'tcp.dstport', 'sum_pkt_len', 'total_pkts']\n",
        "sequence_features = ['pkt_len_mean', 'ip_direction_changes', 'iat_mean']  # æ·»åŠ åºåˆ—ç‰¹å¾\n",
        "\n",
        "# åˆå¹¶ç‰¹å¾åˆ—è¡¨å¹¶æ£€æŸ¥å¯ç”¨æ€§\n",
        "all_viz_features = numeric_features + sequence_features\n",
        "available_features = [f for f in all_viz_features if f in train_processed.columns]\n",
        "print(f\"å¯ç”¨äºå¯è§†åŒ–çš„ç‰¹å¾: {available_features}\")\n",
        "\n",
        "if len(available_features) >= 3:\n",
        "    # ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”\n",
        "    for i, feature in enumerate(available_features[:3]):\n",
        "        if i == 0:\n",
        "            ax = axes[0, 1]\n",
        "        elif i == 1:\n",
        "            ax = axes[1, 0]\n",
        "        else:\n",
        "            ax = axes[1, 1]\n",
        "        \n",
        "        normal_data = train_processed[train_processed['label'] == 0][feature]\n",
        "        pcdn_data = train_processed[train_processed['label'] == 1][feature]\n",
        "        \n",
        "        ax.hist(normal_data, alpha=0.7, label='Normal', bins=30, density=True)\n",
        "        ax.hist(pcdn_data, alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "        ax.set_title(f'Distribution of {feature}')\n",
        "        ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"æ•°æ®åˆ†å¸ƒå¯è§†åŒ–å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ åºåˆ—ç‰¹å¾å¤„ç†æ€»ç»“\n",
        "\n",
        "### ğŸ”„ å®Œæ•´å¤„ç†æµç¨‹\n",
        "\n",
        "**åŸå§‹åºåˆ—ç‰¹å¾ â†’ æå–ç»Ÿè®¡ç‰¹å¾ â†’ XGBoostè®­ç»ƒ**\n",
        "\n",
        "1. **åŸå§‹æ•°æ®æ ¼å¼**ï¼š\n",
        "   - `pkt_len`: `\"[40, 40, 1432, 712, ...]\"` (åŒ…é•¿åº¦åºåˆ—)\n",
        "   - `ip_direction`: `\"[0, 0, 1, 1, 0, ...]\"` (æ–¹å‘åºåˆ—)  \n",
        "   - `iat`: `\"[0.0, 0.016, 0.083, ...]\"` (æ—¶é—´é—´éš”åºåˆ—)\n",
        "\n",
        "2. **ç‰¹å¾æå–ç­–ç•¥**ï¼š\n",
        "   - **ç»Ÿè®¡ç‰¹å¾**ï¼šå‡å€¼ã€æ–¹å·®ã€åˆ†ä½æ•°ç­‰ (é€‚ç”¨äºæ‰€æœ‰åºåˆ—)\n",
        "   - **é¢†åŸŸç‰¹å¾**ï¼šæ ¹æ®åºåˆ—å«ä¹‰è®¾è®¡çš„ä¸“é—¨ç‰¹å¾\n",
        "   - **æ¨¡å¼ç‰¹å¾**ï¼šå˜åŒ–è¶‹åŠ¿ã€çªå‘æ€§ç­‰æ—¶åºç‰¹å¾\n",
        "\n",
        "3. **XGBoostä¼˜åŠ¿**ï¼š\n",
        "   - å¯ä»¥è‡ªåŠ¨å‘ç°ç‰¹å¾ä¹‹é—´çš„å¤æ‚ç»„åˆ\n",
        "   - é€šè¿‡æ ‘ç»“æ„æ•è·éçº¿æ€§æ¨¡å¼\n",
        "   - ç‰¹å¾é‡è¦æ€§åˆ†æå¸®åŠ©ç†è§£å“ªäº›åºåˆ—æ¨¡å¼æœ€é‡è¦\n",
        "\n",
        "### ğŸ“ˆ é¢„æœŸæ•ˆæœ\n",
        "\n",
        "é€šè¿‡è¿™ç§å¤„ç†æ–¹å¼ï¼Œæˆ‘ä»¬å°†**3ä¸ªå˜é•¿åºåˆ—**è½¬æ¢ä¸º**æ•°åä¸ªå›ºå®šé•¿åº¦çš„æ•°å€¼ç‰¹å¾**ï¼Œè¿™äº›ç‰¹å¾èƒ½å¤Ÿå……åˆ†è¡¨è¾¾ç½‘ç»œæµé‡çš„æ—¶åºè¡Œä¸ºæ¨¡å¼ï¼Œå¸®åŠ©XGBoostå‡†ç¡®åŒºåˆ†æ­£å¸¸æµé‡å’ŒPCDNæµé‡ã€‚\n",
        "\n",
        "## ğŸ› ï¸ ä»£ç è´¨é‡æ”¹è¿›\n",
        "\n",
        "### ğŸ”’ å®‰å…¨æ€§ä¿®å¤\n",
        "- **æ›¿æ¢ `eval()` å‡½æ•°**ï¼šä½¿ç”¨ `ast.literal_eval()` å®‰å…¨è§£ææ•°ç»„å­—ç¬¦ä¸²ï¼Œé¿å…ä»£ç æ³¨å…¥é£é™©\n",
        "- **å¢å¼ºé”™è¯¯å¤„ç†**ï¼šæ·»åŠ å®Œå–„çš„å¼‚å¸¸æ•è·å’Œæ•°æ®éªŒè¯\n",
        "\n",
        "### ğŸ“Š æ•°æ®ä¸€è‡´æ€§ä¿è¯\n",
        "- **ç‰¹å¾ç»´åº¦å¯¹é½**ï¼šç¡®ä¿è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†å…·æœ‰ç›¸åŒçš„ç‰¹å¾åˆ—\n",
        "- **ç©ºæ•°æ®é›†å¤„ç†**ï¼šè‡ªåŠ¨ä»è®­ç»ƒé›†åˆ†å‰²éªŒè¯/æµ‹è¯•é›†ï¼Œé˜²æ­¢æ•°æ®ç¼ºå¤±\n",
        "- **æ•°æ®è´¨é‡æ£€æŸ¥**ï¼šæ£€æµ‹NaNå€¼ã€æ— ç©·å€¼å’Œæ ‡ç­¾åˆ†å¸ƒ\n",
        "\n",
        "### âš™ï¸ æ¨¡å‹å‚æ•°ä¼˜åŒ–\n",
        "- **ä¿®å¤XGBoostå‚æ•°**ï¼šæ­£ç¡®ä½¿ç”¨ `early_stopping_rounds` å‚æ•°\n",
        "- **æ”¹è¿›é”™è¯¯å¤„ç†**ï¼šæ›´robustçš„åºåˆ—ç‰¹å¾å¤„ç†æµç¨‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å‡†å¤‡è®­ç»ƒæ•°æ® - ä½¿ç”¨å¤„ç†åçš„å®é™…ç‰¹å¾åˆ—\n",
        "final_feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"å®é™…ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {len(final_feature_columns)}\")\n",
        "\n",
        "X_train = train_processed[final_feature_columns]\n",
        "y_train = train_processed['label']\n",
        "\n",
        "X_val = val_processed[final_feature_columns] \n",
        "y_val = val_processed['label']\n",
        "\n",
        "X_test = test_processed[final_feature_columns]\n",
        "y_test = test_processed['label']\n",
        "\n",
        "print(f\"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train.shape}\")\n",
        "print(f\"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val.shape}\")\n",
        "print(f\"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test.shape}\")\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éæ•°å€¼æ•°æ®\n",
        "print(f\"\\nè®­ç»ƒæ•°æ®ä¸­çš„æ•°æ®ç±»å‹:\")\n",
        "print(X_train.dtypes.value_counts())\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æ•°å€¼å‹\n",
        "X_train = X_train.select_dtypes(include=[np.number])\n",
        "X_val = X_val.select_dtypes(include=[np.number])\n",
        "X_test = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\næœ€ç»ˆç‰¹å¾æ•°é‡: {X_train.shape[1]}\")\n",
        "\n",
        "# æ•°æ®è´¨é‡æ£€æŸ¥\n",
        "print(\"\\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:\")\n",
        "print(f\"è®­ç»ƒé›†æ˜¯å¦åŒ…å«NaN: {X_train.isnull().any().any()}\")\n",
        "print(f\"éªŒè¯é›†æ˜¯å¦åŒ…å«NaN: {X_val.isnull().any().any()}\")\n",
        "print(f\"æµ‹è¯•é›†æ˜¯å¦åŒ…å«NaN: {X_test.isnull().any().any()}\")\n",
        "print(f\"è®­ç»ƒé›†æ˜¯å¦åŒ…å«æ— ç©·å€¼: {np.isinf(X_train).any().any()}\")\n",
        "print(f\"æ ‡ç­¾åˆ†å¸ƒ - è®­ç»ƒé›†: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"æ ‡ç­¾åˆ†å¸ƒ - éªŒè¯é›†: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"æ ‡ç­¾åˆ†å¸ƒ - æµ‹è¯•é›†: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ä¸€è‡´\n",
        "assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´ï¼\"\n",
        "print(\"âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoostæ¨¡å‹è®­ç»ƒ\n",
        "print(\"å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...\")\n",
        "\n",
        "# åˆ›å»ºXGBooståˆ†ç±»å™¨\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœï¼‰\n",
        "xgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=10,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nXGBoostæ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡å‹è¯„ä¼°\n",
        "def evaluate_model(model, X, y, data_name):\n",
        "    \"\"\"\n",
        "    è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
        "    \"\"\"\n",
        "    # é¢„æµ‹\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "    \n",
        "    print(f\"\\n=== {data_name} è¯„ä¼°ç»“æœ ===\")\n",
        "    \n",
        "    # åˆ†ç±»æŠ¥å‘Š\n",
        "    print(\"\\nåˆ†ç±»æŠ¥å‘Š:\")\n",
        "    print(classification_report(y, y_pred, target_names=['Normal', 'PCDN']))\n",
        "    \n",
        "    # AUCåˆ†æ•°\n",
        "    auc_score = roc_auc_score(y, y_pred_proba)\n",
        "    print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
        "    \n",
        "    return y_pred, y_pred_proba, auc_score\n",
        "\n",
        "# è¯„ä¼°è®­ç»ƒé›†\n",
        "train_pred, train_proba, train_auc = evaluate_model(xgb_model, X_train, y_train, \"è®­ç»ƒé›†\")\n",
        "\n",
        "# è¯„ä¼°éªŒè¯é›†\n",
        "val_pred, val_proba, val_auc = evaluate_model(xgb_model, X_val, y_val, \"éªŒè¯é›†\")\n",
        "\n",
        "# è¯„ä¼°æµ‹è¯•é›†\n",
        "test_pred, test_proba, test_auc = evaluate_model(xgb_model, X_test, y_test, \"æµ‹è¯•é›†\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== Top 20 æœ€é‡è¦ç‰¹å¾ ===\")\n",
        "print(importance_df.head(20))\n",
        "\n",
        "# ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = importance_df.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 20 Feature Importance in XGBoost Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶ROCæ›²çº¿å’Œæ··æ·†çŸ©é˜µ\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# ROCæ›²çº¿\n",
        "datasets = [\n",
        "    (y_train, train_proba, \"Training\", train_auc),\n",
        "    (y_val, val_proba, \"Validation\", val_auc),\n",
        "    (y_test, test_proba, \"Testing\", test_auc)\n",
        "]\n",
        "\n",
        "ax_roc = axes[0, 0]\n",
        "for y_true, y_prob, label, auc in datasets:\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    ax_roc.plot(fpr, tpr, label=f'{label} (AUC = {auc:.3f})')\n",
        "\n",
        "ax_roc.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "ax_roc.set_xlabel('False Positive Rate')\n",
        "ax_roc.set_ylabel('True Positive Rate')\n",
        "ax_roc.set_title('ROC Curves')\n",
        "ax_roc.legend()\n",
        "ax_roc.grid(True)\n",
        "\n",
        "# æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Normal', 'PCDN'], \n",
        "            yticklabels=['Normal', 'PCDN'],\n",
        "            ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Confusion Matrix - Test Set')\n",
        "axes[0, 1].set_ylabel('True Label')\n",
        "axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "# é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n",
        "axes[1, 0].hist(test_proba[y_test == 0], alpha=0.7, label='Normal', bins=30, density=True)\n",
        "axes[1, 0].hist(test_proba[y_test == 1], alpha=0.7, label='PCDN', bins=30, density=True)\n",
        "axes[1, 0].set_xlabel('Prediction Probability')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Prediction Probability Distribution')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# å­¦ä¹ æ›²çº¿ï¼ˆè®­ç»ƒå†å²ï¼‰\n",
        "results = xgb_model.evals_result()\n",
        "if 'validation_0' in results:\n",
        "    epochs = len(results['validation_0']['logloss'])\n",
        "    x_axis = range(0, epochs)\n",
        "    axes[1, 1].plot(x_axis, results['validation_0']['logloss'], label='Validation')\n",
        "    axes[1, 1].set_xlabel('Epochs')\n",
        "    axes[1, 1].set_ylabel('Log Loss')\n",
        "    axes[1, 1].set_title('Model Learning Curve')\n",
        "    axes[1, 1].legend()\n",
        "else:\n",
        "    axes[1, 1].text(0.5, 0.5, 'Learning curve not available', \n",
        "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "    axes[1, 1].set_title('Learning Curve')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾ç›¸å…³æ€§åˆ†æ\n",
        "if len(importance_df) >= 10:\n",
        "    # é€‰æ‹©æœ€é‡è¦çš„10ä¸ªç‰¹å¾è¿›è¡Œç›¸å…³æ€§åˆ†æ\n",
        "    top_10_features = importance_df.head(10)['feature'].tolist()\n",
        "    corr_data = train_processed[top_10_features + ['label']]\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = corr_data.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f')\n",
        "    plt.title('Correlation Matrix of Top 10 Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n=== ä¸æ ‡ç­¾ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾ ===\")\n",
        "    label_corr = correlation_matrix['label'].abs().sort_values(ascending=False)\n",
        "    print(label_corr[label_corr.index != 'label'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡å‹æ€§èƒ½æ€»ç»“\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             æ¨¡å‹æ€§èƒ½æ€»ç»“\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "performance_summary = pd.DataFrame({\n",
        "    'æ•°æ®é›†': ['è®­ç»ƒé›†', 'éªŒè¯é›†', 'æµ‹è¯•é›†'],\n",
        "    'AUC Score': [train_auc, val_auc, test_auc],\n",
        "    'æ ·æœ¬æ•°é‡': [len(y_train), len(y_val), len(y_test)]\n",
        "})\n",
        "\n",
        "print(performance_summary.to_string(index=False))\n",
        "\n",
        "print(f\"\\nç‰¹å¾æ€»æ•°: {len(feature_names)}\")\n",
        "print(f\"æœ€é‡è¦çš„5ä¸ªç‰¹å¾:\")\n",
        "for i, (idx, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
        "    print(f\"  {i}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\næ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜æ¨¡å‹å’Œç»“æœ\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "output_dir = './output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ä¿å­˜æ¨¡å‹\n",
        "model_path = os.path.join(output_dir, 'xgboost_pcdn_classifier.pkl')\n",
        "try:\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜ç‰¹å¾é‡è¦æ€§\n",
        "importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
        "try:\n",
        "    importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ°: {importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ç‰¹å¾é‡è¦æ€§ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜æ€§èƒ½æŠ¥å‘Š\n",
        "performance_path = os.path.join(output_dir, 'model_performance.csv')\n",
        "try:\n",
        "    performance_summary.to_csv(performance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… æ¨¡å‹æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {performance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ€§èƒ½æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š\n",
        "analysis_report = {\n",
        "    'Project': 'PCDN vs Normal Traffic Classification',\n",
        "    'Algorithm': 'XGBoost',\n",
        "    'Total_Features': len(feature_names),\n",
        "    'Train_Samples': len(y_train),\n",
        "    'Val_Samples': len(y_val),\n",
        "    'Test_Samples': len(y_test),\n",
        "    'Train_AUC': train_auc,\n",
        "    'Val_AUC': val_auc,\n",
        "    'Test_AUC': test_auc,\n",
        "    'Top_5_Features': importance_df.head(5)['feature'].tolist()\n",
        "}\n",
        "\n",
        "report_path = os.path.join(output_dir, 'analysis_report.txt')\n",
        "try:\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PCDNæµé‡åˆ†ç±»é¡¹ç›®åˆ†ææŠ¥å‘Š\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        for key, value in analysis_report.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "    print(f\"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åˆ†ææŠ¥å‘Šä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "print(f\"\\nğŸ“ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°ç›®å½•: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ é¡¹ç›®å®Œæˆæ€»ç»“\n",
        "\n",
        "### âœ… å·²å®Œæˆçš„å·¥ä½œ\n",
        "\n",
        "1. **æ•°æ®åŠ è½½ä¸é¢„å¤„ç†** \n",
        "   - âœ… æ™ºèƒ½è·¯å¾„æ£€æµ‹å’Œæ•°æ®åŠ è½½\n",
        "   - âœ… è‡ªåŠ¨å¤„ç†ç¼ºå¤±çš„éªŒè¯/æµ‹è¯•é›†\n",
        "   - âœ… å®‰å…¨çš„åºåˆ—ç‰¹å¾è§£æ\n",
        "\n",
        "2. **ç‰¹å¾å·¥ç¨‹**\n",
        "   - âœ… åºåˆ—ç‰¹å¾è½¬ç»Ÿè®¡ç‰¹å¾ (45+ æ–°ç‰¹å¾)\n",
        "   - âœ… éæ•°å€¼ç‰¹å¾è‡ªåŠ¨ç¼–ç \n",
        "   - âœ… æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—\n",
        "\n",
        "3. **æ¨¡å‹è®­ç»ƒ**\n",
        "   - âœ… XGBooståˆ†ç±»å™¨è®­ç»ƒ\n",
        "   - âœ… æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "   - âœ… æ¨¡å‹æ€§èƒ½è¯„ä¼°\n",
        "\n",
        "4. **ç»“æœåˆ†æ**\n",
        "   - âœ… ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "   - âœ… ROCæ›²çº¿å’Œæ··æ·†çŸ©é˜µ\n",
        "   - âœ… ç›¸å…³æ€§åˆ†æ\n",
        "   - âœ… å¯è§†åŒ–å±•ç¤º\n",
        "\n",
        "5. **è¾“å‡ºç®¡ç†**\n",
        "   - âœ… æ¨¡å‹æ–‡ä»¶ä¿å­˜\n",
        "   - âœ… ç»“æœæŠ¥å‘Šå¯¼å‡º\n",
        "   - âœ… é¡¹ç›®æ–‡æ¡£å®Œå–„\n",
        "\n",
        "### ğŸš€ ä½¿ç”¨æ–¹æ³•\n",
        "\n",
        "1. **ç¯å¢ƒå‡†å¤‡**: ç¡®ä¿å®‰è£…æ‰€éœ€PythonåŒ… (pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, scipy)\n",
        "\n",
        "2. **æ•°æ®å‡†å¤‡**: å°†æ•°æ®é›†æ–‡ä»¶å¤¹æ”¾åœ¨notebookåŒç›®å½•ä¸‹\n",
        "\n",
        "3. **è¿è¡Œé¡¹ç›®**: ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰ä»£ç å•å…ƒ\n",
        "\n",
        "4. **æŸ¥çœ‹ç»“æœ**: æ£€æŸ¥ `./output/` ç›®å½•ä¸­çš„è¾“å‡ºæ–‡ä»¶\n",
        "\n",
        "### ğŸ¯ é¡¹ç›®ä»·å€¼\n",
        "\n",
        "- **å®ç”¨æ€§**: å¯ç›´æ¥ç”¨äºPCDNæµé‡æ£€æµ‹\n",
        "- **æ‰©å±•æ€§**: å¯è½»æ¾é€‚é…å…¶ä»–ç½‘ç»œæµé‡åˆ†ç±»ä»»åŠ¡  \n",
        "- **å¯è§£é‡Šæ€§**: è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "- **å¯ç»´æŠ¤æ€§**: æ¸…æ™°çš„ä»£ç ç»“æ„å’Œæ–‡æ¡£\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bysj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
