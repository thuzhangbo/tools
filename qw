# PCDN vs Normal Traffic Classification using XGBoost
# 使用 ip.proto 和 tcp.srcport 特征进行二分类

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
from sklearn.preprocessing import StandardScaler
import os
import glob
import warnings
warnings.filterwarnings('ignore')

# 设置中文字体支持
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# 设置图表样式
try:
    plt.style.use('seaborn-v0_8')
except:
    plt.style.use('default')
sns.set_palette("husl")

print("🚀 开始PCDN流量分类任务")
print("📋 使用特征: ip.proto, tcp.srcport")
print("=" * 60)

# 定义数据路径
base_path = "pcdn_32_pkts_2class_feature_enhance_v17.4_dataset"
train_path = os.path.join(base_path, "Training_set")
val_path = os.path.join(base_path, "Validation_set") 
test_path = os.path.join(base_path, "Testing_set")

# 选择的特征
selected_features = ['ip.proto', 'tcp.srcport']

def load_data_from_directory(directory_path, label):
    """加载指定目录下的所有CSV文件"""
    csv_files = glob.glob(os.path.join(directory_path, "*.csv"))
    dataframes = []
    
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            df['label'] = label  # 添加标签
            df['source_file'] = os.path.basename(file)  # 记录来源文件
            dataframes.append(df)
            print(f"✅ 加载文件: {file} (样本数: {len(df)})")
        except Exception as e:
            print(f"❌ 加载文件失败: {file}, 错误: {e}")
    
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"📊 {directory_path} 总样本数: {len(combined_df)}")
        return combined_df
    else:
        print(f"⚠️  {directory_path} 没有找到有效数据")
        return pd.DataFrame()

# 1. 加载训练集数据
print("\n1️⃣ 加载训练集数据...")
train_normal = load_data_from_directory(os.path.join(train_path, "APP_0"), 0)  # 正常流量
train_pcdn = load_data_from_directory(os.path.join(train_path, "APP_1"), 1)    # PCDN流量

# 2. 加载验证集数据
print("\n2️⃣ 加载验证集数据...")
val_normal = load_data_from_directory(os.path.join(val_path, "APP_0"), 0)
val_pcdn = load_data_from_directory(os.path.join(val_path, "APP_1"), 1)

# 3. 加载测试集数据
print("\n3️⃣ 加载测试集数据...")
test_normal = load_data_from_directory(os.path.join(test_path, "APP_0"), 0)
test_pcdn = load_data_from_directory(os.path.join(test_path, "APP_1"), 1)

# 4. 合并数据集
print("\n4️⃣ 合并数据集...")

# 检查是否有空的数据集
def safe_concat(dataframes, set_name):
    """安全合并数据集，处理空数据集的情况"""
    non_empty_dfs = [df for df in dataframes if not df.empty]
    if not non_empty_dfs:
        print(f"⚠️  {set_name} 没有有效数据!")
        return pd.DataFrame()
    return pd.concat(non_empty_dfs, ignore_index=True)

train_data = safe_concat([train_normal, train_pcdn], "训练集")
val_data = safe_concat([val_normal, val_pcdn], "验证集")
test_data = safe_concat([test_normal, test_pcdn], "测试集")

# 检查数据集是否为空
if train_data.empty or val_data.empty or test_data.empty:
    print("❌ 数据集为空，请检查数据路径和文件")
    exit()

print(f"训练集: {len(train_data)} 样本 (正常: {len(train_normal)}, PCDN: {len(train_pcdn)})")
print(f"验证集: {len(val_data)} 样本 (正常: {len(val_normal)}, PCDN: {len(val_pcdn)})")
print(f"测试集: {len(test_data)} 样本 (正常: {len(test_normal)}, PCDN: {len(test_pcdn)})")

# 5. 特征提取和预处理
print(f"\n5️⃣ 特征提取和预处理...")
print(f"选择的特征: {selected_features}")

def preprocess_features(data, features):
    """预处理特征数据"""
    processed_data = data.copy()
    
    # 检查特征是否存在
    missing_features = [f for f in features if f not in processed_data.columns]
    if missing_features:
        print(f"⚠️  缺失特征: {missing_features}")
        return None
    
    # 提取选择的特征
    feature_data = processed_data[features].copy()
    
    # 处理缺失值
    print(f"📊 特征缺失值统计:")
    for feature in features:
        missing_count = feature_data[feature].isna().sum()
        print(f"  {feature}: {missing_count} 个缺失值")
        
        # 对于数值型特征，用中位数填充缺失值
        if missing_count > 0:
            median_val = feature_data[feature].median()
            if pd.isna(median_val):  # 如果中位数也是NaN（全部都是缺失值）
                # 使用0填充或者特征的典型值
                if feature == 'ip.proto':
                    fill_val = 6  # TCP协议
                elif feature == 'tcp.srcport':
                    fill_val = 0  # 默认端口
                else:
                    fill_val = 0  # 通用默认值
                print(f"    特征全部缺失，使用默认值 {fill_val} 填充")
            else:
                fill_val = median_val
                print(f"    已用中位数 {fill_val} 填充")
            
            feature_data[feature].fillna(fill_val, inplace=True)
    
    return feature_data

# 预处理各数据集的特征
X_train = preprocess_features(train_data, selected_features)
X_val = preprocess_features(val_data, selected_features)
X_test = preprocess_features(test_data, selected_features)

if X_train is None or X_val is None or X_test is None:
    print("❌ 特征预处理失败，请检查特征名称")
    exit()

# 提取标签
y_train = train_data['label'].values
y_val = val_data['label'].values  
y_test = test_data['label'].values

# 6. 特征标准化（重要：确保训练集、验证集、测试集使用相同的标准化参数）
print(f"\n6️⃣ 特征标准化...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # 只在训练集上fit
X_val_scaled = scaler.transform(X_val)          # 验证集使用训练集的参数
X_test_scaled = scaler.transform(X_test)        # 测试集使用训练集的参数

print(f"✅ 标准化完成")
print(f"训练集特征形状: {X_train_scaled.shape}")
print(f"验证集特征形状: {X_val_scaled.shape}")
print(f"测试集特征形状: {X_test_scaled.shape}")

# 7. 数据集基本统计
print(f"\n7️⃣ 数据集统计信息...")
print(f"训练集标签分布: {np.bincount(y_train)}")
print(f"验证集标签分布: {np.bincount(y_val)}")
print(f"测试集标签分布: {np.bincount(y_test)}")

# 显示特征统计
print(f"\n📊 特征统计 (训练集):")
feature_stats = pd.DataFrame(X_train, columns=selected_features).describe()
print(feature_stats)

print(f"\n🎯 数据准备完成，开始模型训练...")

# 8. XGBoost模型训练（版本兼容）
print(f"\n8️⃣ 开始XGBoost模型训练...")
print("=" * 80)

# 检查XGBoost版本
print(f"XGBoost版本: {xgb.__version__}")

# 创建输出目录
output_dir = "output"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"✅ 创建输出目录: {output_dir}")

# 🔧 版本兼容的训练策略（三级回退机制）
training_success = False
training_method = ""
xgb_model = None

print("\n🔧 开始智能训练（版本兼容）...")

try:
    # 方法1: 新版本XGBoost方式（推荐）
    print("🔄 尝试方法1: 新版本训练方式...")
    xgb_model = xgb.XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        max_depth=6,
        learning_rate=0.1,
        n_estimators=100,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=0  # 减少输出
    )
    xgb_model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_val_scaled, y_val)],
        verbose=False
    )
    training_success = True
    training_method = "新版本方式"
    print("✅ 方法1成功")
    
except Exception as e:
    print(f"⚠️ 方法1失败: {e}")
    
    try:
        # 方法2: 在模型初始化时设置early_stopping_rounds
        print("🔄 尝试方法2: 初始化时设置early_stopping...")
        xgb_model = xgb.XGBClassifier(
            objective='binary:logistic',
            eval_metric='logloss',
            max_depth=6,
            learning_rate=0.1,
            n_estimators=100,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            early_stopping_rounds=10,
            verbosity=0
        )
        xgb_model.fit(
            X_train_scaled, y_train,
            eval_set=[(X_val_scaled, y_val)],
            verbose=False
        )
        training_success = True
        training_method = "初始化early_stopping方式"
        print("✅ 方法2成功")
        
    except Exception as e2:
        print(f"⚠️ 方法2失败: {e2}")
        
        try:
            # 方法3: 禁用early stopping，增加训练轮数
            print("🔄 尝试方法3: 禁用early stopping...")
            xgb_model = xgb.XGBClassifier(
                objective='binary:logistic',
                eval_metric='logloss',
                max_depth=6,
                learning_rate=0.1,
                n_estimators=150,  # 增加轮数补偿
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbosity=0
            )
            xgb_model.fit(X_train_scaled, y_train)
            training_success = True
            training_method = "无early stopping方式"
            print("✅ 方法3成功")
            
        except Exception as e3:
            print(f"❌ 方法3也失败: {e3}")
            training_success = False

if training_success:
    print(f"\n🎉 模型训练成功！")
    print(f"📋 使用方法: {training_method}")
    print(f"📊 训练数据: {X_train_scaled.shape[0]}样本, {X_train_scaled.shape[1]}特征")
    print(f"📊 验证数据: {X_val_scaled.shape[0]}样本")
    print(f"📊 测试数据: {X_test_scaled.shape[0]}样本")
else:
    print("❌ 所有训练方法都失败了，请检查XGBoost版本和数据")
    exit()

print("=" * 80)

# 9. 模型预测
print(f"\n9️⃣ 模型预测...")

# 在各数据集上进行预测
y_train_pred = xgb_model.predict(X_train_scaled)
y_train_prob = xgb_model.predict_proba(X_train_scaled)[:, 1]

y_val_pred = xgb_model.predict(X_val_scaled)
y_val_prob = xgb_model.predict_proba(X_val_scaled)[:, 1]

y_test_pred = xgb_model.predict(X_test_scaled)
y_test_prob = xgb_model.predict_proba(X_test_scaled)[:, 1]

# 10. 模型评估
print(f"\n🔟 模型评估结果...")

# 准确率
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# AUC (安全计算，处理单类别情况)
def safe_auc_score(y_true, y_prob, set_name):
    """安全计算AUC，处理单类别情况"""
    if len(np.unique(y_true)) < 2:
        print(f"⚠️  {set_name} 只有一个类别，无法计算AUC")
        return 0.5  # 返回默认值
    return roc_auc_score(y_true, y_prob)

train_auc = safe_auc_score(y_train, y_train_prob, "训练集")
val_auc = safe_auc_score(y_val, y_val_prob, "验证集")
test_auc = safe_auc_score(y_test, y_test_prob, "测试集")

print(f"📊 准确率 (Accuracy):")
print(f"  训练集: {train_acc:.4f}")
print(f"  验证集: {val_acc:.4f}")
print(f"  测试集: {test_acc:.4f}")

print(f"\n📊 AUC值:")
print(f"  训练集: {train_auc:.4f}")
print(f"  验证集: {val_auc:.4f}")
print(f"  测试集: {test_auc:.4f}")

# 详细分类报告
print(f"\n📋 测试集详细分类报告:")
print(classification_report(y_test, y_test_pred, target_names=['正常流量', 'PCDN流量']))

# 11. 特征重要性分析
print(f"\n1️⃣1️⃣ 特征重要性分析...")

# 获取特征重要性
feature_importance = xgb_model.feature_importances_
feature_names = selected_features

# 创建特征重要性DataFrame
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print(f"📊 特征重要性排序:")
for idx, row in importance_df.iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

# 12. 可视化结果
print(f"\n1️⃣2️⃣ 生成可视化图表...")

# 创建图表
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('PCDN流量分类结果分析', fontsize=16, fontweight='bold')

# 1. 特征重要性图
ax1 = axes[0, 0]
# 动态生成颜色，确保有足够的颜色
colors = plt.cm.Set3(np.linspace(0, 1, len(importance_df)))
bars = ax1.bar(importance_df['feature'], importance_df['importance'], 
               color=colors)
ax1.set_title('特征重要性分析', fontweight='bold')
ax1.set_xlabel('特征名称')
ax1.set_ylabel('重要性分数')
ax1.tick_params(axis='x', rotation=45)

# 在柱状图上添加数值标签
for bar, importance in zip(bars, importance_df['importance']):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{importance:.3f}', ha='center', va='bottom', fontweight='bold')

# 2. ROC曲线
ax2 = axes[0, 1]

# 安全绘制ROC曲线
def safe_plot_roc(y_true, y_prob, label, ax):
    """安全绘制ROC曲线"""
    if len(np.unique(y_true)) < 2:
        return  # 跳过单类别情况
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    ax.plot(fpr, tpr, label=label, linewidth=2)

safe_plot_roc(y_train, y_train_prob, f'训练集 (AUC = {train_auc:.3f})', ax2)
safe_plot_roc(y_val, y_val_prob, f'验证集 (AUC = {val_auc:.3f})', ax2)
safe_plot_roc(y_test, y_test_prob, f'测试集 (AUC = {test_auc:.3f})', ax2)

ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='随机分类')
ax2.set_title('ROC曲线比较', fontweight='bold')
ax2.set_xlabel('假正率 (FPR)')
ax2.set_ylabel('真正率 (TPR)')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. 混淆矩阵
ax3 = axes[1, 0]
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3,
            xticklabels=['正常流量', 'PCDN流量'],
            yticklabels=['正常流量', 'PCDN流量'])
ax3.set_title('测试集混淆矩阵', fontweight='bold')
ax3.set_xlabel('预测标签')
ax3.set_ylabel('真实标签')

# 4. 准确率对比
ax4 = axes[1, 1]
datasets = ['训练集', '验证集', '测试集']
accuracies = [train_acc, val_acc, test_acc]
colors = ['#FF9999', '#66B2FF', '#99FF99']

bars = ax4.bar(datasets, accuracies, color=colors, alpha=0.8)
ax4.set_title('各数据集准确率对比', fontweight='bold')
ax4.set_ylabel('准确率')
ax4.set_ylim(0, 1.1)

# 在柱状图上添加数值标签
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()

# 安全保存图表
try:
    plt.savefig('pcdn_classification_results.png', dpi=300, bbox_inches='tight')
    print("📊 图表已保存为: pcdn_classification_results.png")
except Exception as e:
    print(f"⚠️  图表保存失败: {e}")
    print("📊 图表仍在内存中显示")

plt.show()

# 13. 总结报告
print(f"\n1️⃣3️⃣ 分类任务总结报告")
print("=" * 60)
print(f"🎯 任务: PCDN流量与正常流量二分类")
print(f"🔧 使用特征: {', '.join(selected_features)}")
print(f"📊 数据规模:")
print(f"  训练集: {len(X_train)} 样本")
print(f"  验证集: {len(X_val)} 样本") 
print(f"  测试集: {len(X_test)} 样本")

print(f"\n🏆 最终性能指标:")
print(f"  测试集准确率: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"  测试集AUC值: {test_auc:.4f}")

print(f"\n📈 特征重要性:")
for idx, row in importance_df.iterrows():
    percentage = (row['importance'] / importance_df['importance'].sum()) * 100
    print(f"  {row['feature']}: {row['importance']:.4f} ({percentage:.1f}%)")

print(f"\n✅ 分析完成! 结果已保存到 pcdn_classification_results.png")
print("=" * 60)
