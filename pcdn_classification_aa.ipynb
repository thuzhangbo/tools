{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCDN vs Normal Traffic Classification using XGBoost\n",
        "\n",
        "è¿™ä¸ªé¡¹ç›®ä½¿ç”¨XGBoostå¯¹æ­£å¸¸æµé‡å’ŒPCDNæµé‡è¿›è¡ŒäºŒåˆ†ç±»ã€‚\n",
        "\n",
        "## ğŸ“Š æ•°æ®é›†ç»“æ„\n",
        "- **Training_set/APP_0**: æ­£å¸¸æµé‡ (æ ‡ç­¾=0)\n",
        "- **Training_set/APP_1**: PCDNæµé‡ (æ ‡ç­¾=1)  \n",
        "- **Validation_set**: éªŒè¯é›†\n",
        "- **Testing_set**: æµ‹è¯•é›†\n",
        "\n",
        "## ğŸ”§ ä¸»è¦åŠŸèƒ½\n",
        "- âœ… **æ™ºèƒ½æ•°æ®åŠ è½½**: å››çº§åˆ—ä¸€è‡´æ€§æ£€æŸ¥ï¼Œè‡ªåŠ¨å‘ç°å¹¶ä¿®å¤CSVæ–‡ä»¶é—´çš„åˆ—å·®å¼‚\n",
        "- âœ… **åºåˆ—ç‰¹å¾å·¥ç¨‹**: å°†`ip_direction`ã€`pkt_len`ã€`iat`è½¬æ¢ä¸ºä¸°å¯Œçš„ç»Ÿè®¡ç‰¹å¾\n",
        "- âœ… **XGBooståˆ†ç±»**: ç‰ˆæœ¬å…¼å®¹è®­ç»ƒï¼Œç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "- âœ… **å®Œæ•´å¯è§†åŒ–**: ROCæ›²çº¿ã€æ··æ·†çŸ©é˜µã€ç‰¹å¾é‡è¦æ€§å›¾è¡¨\n",
        "- âœ… **è·¨æ•°æ®é›†è¯Šæ–­**: ç²¾ç¡®è¯†åˆ«\"ä¸ºä»€ä¹ˆæµ‹è¯•é›†æ¯”è®­ç»ƒé›†å¤š1ä¸ªç‰¹å¾\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“¦ å¯¼å…¥å¿…è¦çš„åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from scipy import stats  # ç”¨äºåºåˆ—ç‰¹å¾çš„ååº¦å’Œå³°åº¦è®¡ç®—\n",
        "import ast  # ç”¨äºå®‰å…¨è§£ææ•°ç»„å­—ç¬¦ä¸²\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… åº“å¯¼å…¥å®Œæˆï¼\")\n",
        "\n",
        "# ===== ğŸ”§ é…ç½®å‚æ•° =====\n",
        "ENABLE_SEQUENCE_FEATURES = True  # æ§åˆ¶åºåˆ—ç‰¹å¾å¤„ç†ï¼šTrue=ç‰¹å¾å·¥ç¨‹, False=åˆ é™¤åºåˆ—ç‰¹å¾\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ”§ åºåˆ—ç‰¹å¾å¤„ç†æ¨¡å¼: {'å¯ç”¨ âœ…' if ENABLE_SEQUENCE_FEATURES else 'ç¦ç”¨ âŒ'}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(\"ğŸ“Š å¯ç”¨æ¨¡å¼ - æ‰§è¡Œå¤æ‚çš„åºåˆ—ç‰¹å¾å·¥ç¨‹:\")\n",
        "    print(\"   â€¢ ip_direction â†’ 15ä¸ªç»Ÿè®¡ç‰¹å¾ (æ–¹å‘æ¨¡å¼åˆ†æ)\")\n",
        "    print(\"   â€¢ pkt_len â†’ 15ä¸ªç»Ÿè®¡ç‰¹å¾ (åŒ…å¤§å°æ¨¡å¼åˆ†æ)\")  \n",
        "    print(\"   â€¢ iat â†’ 15ä¸ªç»Ÿè®¡ç‰¹å¾ (æ—¶é—´é—´éš”æ¨¡å¼åˆ†æ)\")\n",
        "    print(\"   â€¢ æ€»è®¡ç”Ÿæˆ 45+ ä¸ªæ–°ç‰¹å¾\")\n",
        "    print(\"   ğŸ’¡ é€‚åˆ: è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œå……åˆ†åˆ©ç”¨æ—¶åºä¿¡æ¯\")\n",
        "else:\n",
        "    print(\"ğŸ—‘ï¸ ç¦ç”¨æ¨¡å¼ - ç›´æ¥åˆ é™¤åºåˆ—ç‰¹å¾:\")\n",
        "    print(\"   â€¢ åˆ é™¤ ip_direction, pkt_len, iat\")\n",
        "    print(\"   â€¢ ä»…ä½¿ç”¨å…¶ä»–ç½‘ç»œæµé‡ç‰¹å¾è®­ç»ƒ\")\n",
        "    print(\"   â€¢ æ¨¡å‹æ›´ç®€å•ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«\")\n",
        "    print(\"   ğŸ’¡ é€‚åˆ: èµ„æºæœ‰é™ï¼Œæˆ–å¸Œæœ›ç®€åŒ–æ¨¡å‹çš„åœºæ™¯\")\n",
        "\n",
        "print(\"\\nğŸ’­ è¦åˆ‡æ¢æ¨¡å¼ï¼Œè¯·ä¿®æ”¹ä¸Šæ–¹ ENABLE_SEQUENCE_FEATURES çš„å€¼\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ æ•°æ®ç±»å‹ç»Ÿä¸€å·¥å…·ï¼šè§£å†³æ•°å€¼åˆ— 0 ä¸ 0.0 ä¸ä¸€è‡´é—®é¢˜\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "NUMERIC_FIELDS_CANONICAL = [\n",
        "    'frame.number', 'frame.time_relative', 'ip.version', 'ip.hdr_len',\n",
        "    'ip.dsfield', 'ip.len', 'ip.ttl', 'ip.proto', 'ipv6.plen', 'ipv6.nxt',\n",
        "    'tcp.srcport', 'tcp.dstport', 'tcp.hdr_len', 'tcp.flags.syn', 'tcp.flags.ack',\n",
        "    'tcp.payload', 'udp.srcport', 'udp.dstport', 'udp.length', 'pcap_duration',\n",
        "    'srcport', 'dstport', 'ulProtoID', 'dpi_rule_pkt', 'dpi_packets', 'dpi_bytes',\n",
        "    'flow_uplink_traffic', 'flow_downlink_traffic', 'sum_pkt_len', 'total_pkts',\n",
        "    'srcport_cls', 'dstport_cls', 'pkt_len_avg', 'pkt_len_max', 'pkt_len_min',\n",
        "    'up_pkts', 'up_bytes', 'down_pkts', 'down_bytes', 'up_pkt_ratio', 'down_pkt_ratio',\n",
        "    'up_byte_ratio', 'down_byte_ratio', 'up_down_pkt_ratio', 'up_down_byte_ratio',\n",
        "    'iat_avg', 'iat_max', 'iat_min', 'avg_speed', 'avg_pkt_speed', 'max_burst'\n",
        "]\n",
        "\n",
        "def normalize_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    å°†å¸¸è§æ•°å€¼ç‰¹å¾ç»Ÿä¸€ä¸º float64ï¼Œé¿å… 0 ä¸ 0.0 ç±»å‹ä¸ä¸€è‡´å¸¦æ¥çš„éšå«åå·®ã€‚\n",
        "    - ä»…å¯¹èƒ½æˆåŠŸè½¬æ¢ä¸ºæ•°å€¼çš„åˆ—åšç»Ÿä¸€å¤„ç†\n",
        "    - å…¶ä»–åˆ—ä¿æŒä¸å˜\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "    df_norm = df.copy()\n",
        "\n",
        "    # ä¼˜å…ˆå¯¹é¢„å®šä¹‰å­—æ®µåšç»Ÿä¸€ï¼›è‹¥å­—æ®µä¸å­˜åœ¨åˆ™è·³è¿‡\n",
        "    for col in NUMERIC_FIELDS_CANONICAL:\n",
        "        if col in df_norm.columns:\n",
        "            df_norm[col] = pd.to_numeric(df_norm[col], errors='coerce')\n",
        "            if pd.api.types.is_numeric_dtype(df_norm[col]):\n",
        "                df_norm[col] = df_norm[col].astype('float64')\n",
        "\n",
        "    # å…œåº•ï¼šå¯¹å…¶ä½™å·²æ˜¯æ•°å€¼ç±»å‹ä½†ä¸æ˜¯ float64 çš„åˆ—ï¼Œä¹Ÿç»Ÿä¸€ä¸º float64\n",
        "    for col in df_norm.columns:\n",
        "        if col in ['label', 'source_file', 'app_category']:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(df_norm[col]) and df_norm[col].dtype != 'float64':\n",
        "            df_norm[col] = df_norm[col].astype('float64')\n",
        "\n",
        "    return df_norm\n",
        "\n",
        "\n",
        "def check_dtype_consistency(df: pd.DataFrame, fields=None):\n",
        "    \"\"\"ç®€å•æ‰“å°å…³é”®å­—æ®µçš„æ•°æ®ç±»å‹ã€‚\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print('âš ï¸ DataFrameä¸ºç©ºï¼Œè·³è¿‡ç±»å‹æ£€æŸ¥')\n",
        "        return\n",
        "    fields = fields or ['tcp.payload', 'tcp.srcport', 'tcp.dstport', 'ip.len', 'udp.length']\n",
        "    present = [f for f in fields if f in df.columns]\n",
        "    if not present:\n",
        "        print('â„¹ï¸ å…³é”®å­—æ®µåœ¨å½“å‰DataFrameä¸­ä¸å­˜åœ¨')\n",
        "        return\n",
        "    print('å­—æ®µç±»å‹æ£€æŸ¥:')\n",
        "    for f in present:\n",
        "        print(f\"  - {f}: {df[f].dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ” åœ¨æ•°æ®åŠ è½½åç»Ÿä¸€æ•°æ®ç±»å‹ï¼ˆè¯·åœ¨è·å–åˆ° train_data/val_data/test_data åè¿è¡Œï¼‰\n",
        "\n",
        "# å‡è®¾å·²å¾—åˆ°ï¼štrain_data, val_data, test_data\n",
        "try:\n",
        "    print(\"ç»Ÿä¸€æ•°å€¼ç‰¹å¾ä¸º float64 ...\")\n",
        "    train_data = normalize_data_types(train_data)\n",
        "    val_data = normalize_data_types(val_data) if 'val_data' in globals() else val_data\n",
        "    test_data = normalize_data_types(test_data) if 'test_data' in globals() else test_data\n",
        "\n",
        "    print(\"æ£€æŸ¥å…³é”®å­—æ®µçš„æ•°æ®ç±»å‹ (è®­ç»ƒé›†):\")\n",
        "    check_dtype_consistency(train_data)\n",
        "    if val_data is not None and len(val_data) > 0:\n",
        "        print(\"æ£€æŸ¥å…³é”®å­—æ®µçš„æ•°æ®ç±»å‹ (éªŒè¯é›†):\")\n",
        "        check_dtype_consistency(val_data)\n",
        "    if test_data is not None and len(test_data) > 0:\n",
        "        print(\"æ£€æŸ¥å…³é”®å­—æ®µçš„æ•°æ®ç±»å‹ (æµ‹è¯•é›†):\")\n",
        "        check_dtype_consistency(test_data)\n",
        "except NameError:\n",
        "    print(\"å°šæœªå®šä¹‰ train_data/val_data/test_dataã€‚è¯·åœ¨æ•°æ®åŠ è½½å®Œæˆåå†è¿è¡Œæœ¬å•å…ƒã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ æ™ºèƒ½æ•°æ®åŠ è½½å‡½æ•°ï¼ˆæ”¯æŒå››çº§åˆ—ä¸€è‡´æ€§æ£€æŸ¥ï¼‰\n",
        "\n",
        "def load_data_from_directory(base_path, label):\n",
        "    \"\"\"\n",
        "    ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰CSVæ–‡ä»¶å¹¶æ·»åŠ æ ‡ç­¾ - å¢å¼ºç‰ˆï¼šç¡®ä¿åˆ—ä¸€è‡´æ€§\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(base_path, '*.csv'))\n",
        "    print(f\"åœ¨ {base_path} ä¸­æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶\")\n",
        "    \n",
        "    dataframes = []\n",
        "    all_columns_info = []  # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„åˆ—ä¿¡æ¯\n",
        "    \n",
        "    # ğŸ” ç¬¬ä¸€éæ‰«æï¼šæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„åˆ—ç»“æ„\n",
        "    print(f\"\\nğŸ” æ£€æŸ¥CSVæ–‡ä»¶åˆ—ç»“æ„ ({os.path.basename(base_path)}):\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            # åªè¯»å–ç¬¬ä¸€è¡Œæ¥è·å–åˆ—åï¼Œé¿å…åŠ è½½æ•´ä¸ªæ–‡ä»¶\n",
        "            df_sample = pd.read_csv(file, nrows=1)\n",
        "            file_columns = list(df_sample.columns)\n",
        "            all_columns_info.append({\n",
        "                'file': file,\n",
        "                'columns': file_columns,\n",
        "                'count': len(file_columns)\n",
        "            })\n",
        "            \n",
        "            print(f\"  æ–‡ä»¶{i+1}: {os.path.basename(file)} -> {len(file_columns)}åˆ—\")\n",
        "            \n",
        "            # æ˜¾ç¤ºå‰å‡ ä¸ªå’Œåå‡ ä¸ªåˆ—å\n",
        "            if len(file_columns) <= 6:\n",
        "                print(f\"    åˆ—å: {file_columns}\")\n",
        "            else:\n",
        "                print(f\"    åˆ—å: {file_columns[:3]} ... {file_columns[-3:]}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ æ£€æŸ¥æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}\")\n",
        "    \n",
        "    # ğŸ” åˆ†æåˆ—æ•°å·®å¼‚\n",
        "    column_counts = [info['count'] for info in all_columns_info]\n",
        "    unique_counts = sorted(set(column_counts))\n",
        "    \n",
        "    if len(unique_counts) > 1:\n",
        "        print(f\"\\nâŒ å‘ç°åˆ—æ•°ä¸ä¸€è‡´é—®é¢˜ï¼\")\n",
        "        for count in unique_counts:\n",
        "            files_with_count = [f\"æ–‡ä»¶{i+1}({os.path.basename(info['file'])})\" \n",
        "                              for i, info in enumerate(all_columns_info) \n",
        "                              if info['count'] == count]\n",
        "            print(f\"  ğŸ“Š {count}åˆ—: {', '.join(files_with_count)}\")\n",
        "        \n",
        "        # ğŸ”§ è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„å…±åŒåˆ—\n",
        "        if all_columns_info:\n",
        "            common_columns = set(all_columns_info[0]['columns'])\n",
        "            for info in all_columns_info[1:]:\n",
        "                common_columns = common_columns.intersection(set(info['columns']))\n",
        "            \n",
        "            common_columns_list = sorted(list(common_columns))\n",
        "            print(f\"\\nğŸ”§ æ‰€æœ‰æ–‡ä»¶çš„å…±åŒåˆ—æ•°: {len(common_columns_list)}\")\n",
        "            \n",
        "            # æ˜¾ç¤ºå„æ–‡ä»¶å°†è¢«æ’é™¤çš„åˆ—\n",
        "            print(\"ğŸ“ å„æ–‡ä»¶ç‹¬æœ‰çš„åˆ—:\")\n",
        "            for i, info in enumerate(all_columns_info):\n",
        "                excluded = set(info['columns']) - common_columns\n",
        "                if excluded:\n",
        "                    print(f\"  æ–‡ä»¶{i+1}: {sorted(list(excluded))}\")\n",
        "                else:\n",
        "                    print(f\"  æ–‡ä»¶{i+1}: æ— ç‹¬æœ‰åˆ—\")\n",
        "                    \n",
        "            print(f\"âœ… å°†ç»Ÿä¸€ä½¿ç”¨ {len(common_columns_list)} ä¸ªå…±åŒåˆ—\")\n",
        "            target_columns = common_columns_list\n",
        "        else:\n",
        "            target_columns = None\n",
        "            print(\"âŒ æ— æ³•ç¡®å®šå…±åŒåˆ—\")\n",
        "    else:\n",
        "        print(f\"âœ… æ‰€æœ‰æ–‡ä»¶åˆ—æ•°ä¸€è‡´: {column_counts[0]}åˆ—\")\n",
        "        target_columns = None\n",
        "    \n",
        "    # ğŸ“Š ç¬¬äºŒéï¼šä½¿ç”¨ç»Ÿä¸€åˆ—ç»“æ„åŠ è½½æ•°æ®\n",
        "    print(f\"\\nğŸ“Š æ­£å¼åŠ è½½æ•°æ®:\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            original_shape = df.shape\n",
        "            \n",
        "            # å¦‚æœéœ€è¦ï¼Œåªä¿ç•™å…±åŒåˆ—\n",
        "            if target_columns is not None:\n",
        "                df = df[target_columns]\n",
        "                print(f\"  ğŸ”§ æ–‡ä»¶{i+1}: {original_shape} -> {df.shape} (åˆ—å¯¹é½)\")\n",
        "            \n",
        "            df['label'] = label  # æ·»åŠ æ ‡ç­¾åˆ—\n",
        "            df['source_file'] = os.path.basename(file)  # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯\n",
        "            dataframes.append(df)\n",
        "            print(f\"  âœ… {os.path.basename(file)}: {len(df)}è¡Œ x {len(df.columns)}åˆ—\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ åŠ è½½æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}\")\n",
        "    \n",
        "    # ğŸ¯ æœ€ç»ˆåˆå¹¶å’ŒéªŒè¯\n",
        "    if dataframes:\n",
        "        # æœ€åæ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰dataframeåˆ—æ•°ä¸€è‡´\n",
        "        final_shapes = [df.shape for df in dataframes]\n",
        "        final_col_counts = [shape[1] for shape in final_shapes]\n",
        "        \n",
        "        if len(set(final_col_counts)) == 1:\n",
        "            print(f\"âœ… åˆå¹¶å‰æœ€ç»ˆæ£€æŸ¥é€šè¿‡: æ‰€æœ‰æ–‡ä»¶å‡ä¸º {final_col_counts[0]} åˆ—\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ åˆå¹¶å‰å‘ç°åˆ—æ•°å·®å¼‚: {final_col_counts}\")\n",
        "        \n",
        "        result = pd.concat(dataframes, ignore_index=True)\n",
        "        \n",
        "        print(f\"ğŸ‰ æ•°æ®åŠ è½½å®Œæˆ:\")\n",
        "        print(f\"  ğŸ“ æ–‡ä»¶æ•°: {len(csv_files)}\")\n",
        "        print(f\"  ğŸ“Š æœ€ç»ˆå½¢çŠ¶: {result.shape}\")\n",
        "        print(f\"  ğŸ·ï¸ æ ‡ç­¾å€¼: {label}\")\n",
        "        \n",
        "        # ğŸ“‹ è¿”å›ç»“æœå’Œåˆ—ä¿¡æ¯\n",
        "        columns_meta = {\n",
        "            'final_columns': list(result.columns),\n",
        "            'original_columns_info': all_columns_info,\n",
        "            'target_columns': target_columns,\n",
        "            'dataset_path': base_path\n",
        "        }\n",
        "        \n",
        "        return result, columns_meta\n",
        "    else:\n",
        "        print(\"âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡ä»¶\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "\n",
        "def analyze_cross_dataset_columns(datasets_info):\n",
        "    \"\"\"\n",
        "    åˆ†æä¸åŒæ•°æ®é›†ä¹‹é—´çš„åˆ—å·®å¼‚ï¼ŒåŒ…æ‹¬CSVæ–‡ä»¶çº§åˆ«çš„è¯¦ç»†åˆ†æ\n",
        "    datasets_info: å­—å…¸ï¼Œæ ¼å¼ä¸º {'æ•°æ®é›†åç§°': columns_meta}\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ” è·¨æ•°æ®é›†åˆ—ç»“æ„ä¸€è‡´æ€§åˆ†æ\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„åˆ—ä¿¡æ¯\n",
        "    dataset_columns = {}\n",
        "    dataset_csv_info = {}  # æ–°å¢ï¼šæ”¶é›†æ¯ä¸ªæ•°æ®é›†çš„CSVæ–‡ä»¶è¯¦ç»†ä¿¡æ¯\n",
        "    \n",
        "    for dataset_name, meta in datasets_info.items():\n",
        "        if 'final_columns' in meta and meta['final_columns']:\n",
        "            # æ’é™¤æˆ‘ä»¬æ·»åŠ çš„è¾…åŠ©åˆ—\n",
        "            original_cols = [col for col in meta['final_columns'] \n",
        "                           if col not in ['label', 'source_file']]\n",
        "            dataset_columns[dataset_name] = set(original_cols)\n",
        "            print(f\"ğŸ“Š {dataset_name}: {len(original_cols)}ä¸ªåŸå§‹åˆ—\")\n",
        "            \n",
        "            # æ”¶é›†CSVæ–‡ä»¶çº§åˆ«çš„ä¿¡æ¯\n",
        "            dataset_csv_info[dataset_name] = {\n",
        "                'normal_meta': meta.get('normal_meta', {}),\n",
        "                'pcdn_meta': meta.get('pcdn_meta', {})\n",
        "            }\n",
        "    \n",
        "    if len(dataset_columns) < 2:\n",
        "        print(\"âš ï¸ æ•°æ®é›†æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè·¨æ•°æ®é›†æ¯”è¾ƒ\")\n",
        "        return\n",
        "    \n",
        "    # è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„åˆ—ç»Ÿè®¡\n",
        "    all_datasets = list(dataset_columns.keys())\n",
        "    column_counts = {name: len(cols) for name, cols in dataset_columns.items()}\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ åˆ—æ•°ç»Ÿè®¡:\")\n",
        "    for dataset, count in column_counts.items():\n",
        "        print(f\"  {dataset}: {count}åˆ—\")\n",
        "    \n",
        "    # æ£€æŸ¥åˆ—æ•°æ˜¯å¦ä¸€è‡´\n",
        "    unique_counts = set(column_counts.values())\n",
        "    if len(unique_counts) == 1:\n",
        "        print(\"âœ… æ‰€æœ‰æ•°æ®é›†åˆ—æ•°ä¸€è‡´\")\n",
        "    else:\n",
        "        print(f\"âŒ å‘ç°åˆ—æ•°ä¸ä¸€è‡´: {sorted(unique_counts)}\")\n",
        "    \n",
        "    # è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„å…±åŒåˆ—å’Œç‹¬æœ‰åˆ—\n",
        "    all_columns = set()\n",
        "    for cols in dataset_columns.values():\n",
        "        all_columns = all_columns.union(cols)\n",
        "    \n",
        "    common_columns = set.intersection(*dataset_columns.values()) if dataset_columns else set()\n",
        "    \n",
        "    print(f\"\\nğŸ”§ åˆ—åˆ†æç»“æœ:\")\n",
        "    print(f\"  æ‰€æœ‰å”¯ä¸€åˆ—æ•°: {len(all_columns)}\")\n",
        "    print(f\"  å…±åŒåˆ—æ•°: {len(common_columns)}\")\n",
        "    print(f\"  å·®å¼‚åˆ—æ•°: {len(all_columns) - len(common_columns)}\")\n",
        "    \n",
        "    # ğŸ†• CSVæ–‡ä»¶çº§åˆ«çš„åˆ—å·®å¼‚æ£€æŸ¥ï¼ˆä»…æ˜¾ç¤ºä¸ä¸€è‡´çš„æƒ…å†µï¼‰\n",
        "    print(f\"\\nğŸ“ CSVæ–‡ä»¶åˆ—å·®å¼‚æ£€æŸ¥:\")\n",
        "    found_issues = False\n",
        "    for dataset, cols in dataset_columns.items():\n",
        "        csv_info = dataset_csv_info.get(dataset, {})\n",
        "        \n",
        "        # æ£€æŸ¥APP_0 (æ­£å¸¸æµé‡) çš„CSVæ–‡ä»¶\n",
        "        normal_meta = csv_info.get('normal_meta', {})\n",
        "        if 'original_columns_info' in normal_meta:\n",
        "            csv_files_info = normal_meta['original_columns_info']\n",
        "            print(f\"\\n  ğŸ” {dataset} - APP_0:\")\n",
        "            _analyze_csv_files_columns(csv_files_info, \"APP_0\")\n",
        "            found_issues = True\n",
        "        \n",
        "        # æ£€æŸ¥APP_1 (PCDNæµé‡) çš„CSVæ–‡ä»¶  \n",
        "        pcdn_meta = csv_info.get('pcdn_meta', {})\n",
        "        if 'original_columns_info' in pcdn_meta:\n",
        "            csv_files_info = pcdn_meta['original_columns_info']\n",
        "            print(f\"  ğŸ” {dataset} - APP_1:\")\n",
        "            _analyze_csv_files_columns(csv_files_info, \"APP_1\")\n",
        "            found_issues = True\n",
        "    \n",
        "    if not found_issues:\n",
        "        print(\"  âœ… æ‰€æœ‰CSVæ–‡ä»¶åˆ—ç»“æ„ä¸€è‡´\")\n",
        "    \n",
        "    # è·¨æ•°æ®é›†ç‹¬æœ‰åˆ—åˆ†æï¼ˆä»…æ˜¾ç¤ºæœ‰å·®å¼‚çš„æ•°æ®é›†ï¼‰\n",
        "    print(f\"\\nğŸ“ è·¨æ•°æ®é›†ç‹¬æœ‰åˆ—åˆ†æ:\")\n",
        "    has_differences = False\n",
        "    \n",
        "    for dataset, cols in dataset_columns.items():\n",
        "        unique_to_dataset = cols - common_columns\n",
        "        missing_columns = common_columns - cols\n",
        "        \n",
        "        if unique_to_dataset or missing_columns:\n",
        "            has_differences = True\n",
        "            print(f\"\\n  âš ï¸ {dataset} ({len(cols)}åˆ—):\")\n",
        "            \n",
        "            if unique_to_dataset:\n",
        "                print(f\"    ğŸ”¸ ç‹¬æœ‰åˆ—({len(unique_to_dataset)}): {sorted(list(unique_to_dataset))}\")\n",
        "                \n",
        "                # æ˜¾ç¤ºç‹¬æœ‰åˆ—æ¥æºçš„CSVæ–‡ä»¶\n",
        "                csv_info = dataset_csv_info.get(dataset, {})\n",
        "                source_files = []\n",
        "                \n",
        "                # æ£€æŸ¥APP_0æ–‡ä»¶\n",
        "                normal_meta = csv_info.get('normal_meta', {})\n",
        "                if 'original_columns_info' in normal_meta:\n",
        "                    for i, file_info in enumerate(normal_meta['original_columns_info']):\n",
        "                        file_columns = set(file_info['columns'])\n",
        "                        file_unique = file_columns.intersection(unique_to_dataset)\n",
        "                        if file_unique:\n",
        "                            filename = os.path.basename(file_info['file'])\n",
        "                            source_files.append(f\"APP_0/{filename}({sorted(list(file_unique))})\")\n",
        "                \n",
        "                # æ£€æŸ¥APP_1æ–‡ä»¶\n",
        "                pcdn_meta = csv_info.get('pcdn_meta', {})\n",
        "                if 'original_columns_info' in pcdn_meta:\n",
        "                    for i, file_info in enumerate(pcdn_meta['original_columns_info']):\n",
        "                        file_columns = set(file_info['columns'])\n",
        "                        file_unique = file_columns.intersection(unique_to_dataset)\n",
        "                        if file_unique:\n",
        "                            filename = os.path.basename(file_info['file'])\n",
        "                            source_files.append(f\"APP_1/{filename}({sorted(list(file_unique))})\")\n",
        "                \n",
        "                if source_files:\n",
        "                    print(f\"    ğŸ“ æ¥æº: {', '.join(source_files)}\")\n",
        "            \n",
        "            if missing_columns:\n",
        "                print(f\"    ğŸ”¸ ç¼ºå¤±åˆ—({len(missing_columns)}): {sorted(list(missing_columns))}\")\n",
        "    \n",
        "    if not has_differences:\n",
        "        print(\"  âœ… æ‰€æœ‰æ•°æ®é›†åˆ—å®Œå…¨ä¸€è‡´\")\n",
        "    \n",
        "    # ä¸¤ä¸¤æ•°æ®é›†å¯¹æ¯”ï¼ˆä»…æ˜¾ç¤ºæœ‰å·®å¼‚çš„å¯¹æ¯”ï¼‰\n",
        "    if len(dataset_columns) > 1:\n",
        "        print(f\"\\nğŸ”„ æ•°æ®é›†å·®å¼‚å¯¹æ¯”:\")\n",
        "        datasets_list = list(dataset_columns.keys())\n",
        "        has_pair_differences = False\n",
        "        \n",
        "        for i in range(len(datasets_list)):\n",
        "            for j in range(i+1, len(datasets_list)):\n",
        "                dataset1, dataset2 = datasets_list[i], datasets_list[j]\n",
        "                cols1, cols2 = dataset_columns[dataset1], dataset_columns[dataset2]\n",
        "                \n",
        "                only_in_1 = cols1 - cols2\n",
        "                only_in_2 = cols2 - cols1\n",
        "                \n",
        "                if only_in_1 or only_in_2:\n",
        "                    has_pair_differences = True\n",
        "                    print(f\"\\n  âš ï¸ {dataset1} vs {dataset2}:\")\n",
        "                    \n",
        "                    if only_in_1:\n",
        "                        print(f\"    ğŸ”¸ ä»…{dataset1}æœ‰({len(only_in_1)}): {sorted(list(only_in_1))}\")\n",
        "                        _show_unique_columns_source(dataset1, only_in_1, dataset_csv_info)\n",
        "                        \n",
        "                    if only_in_2:\n",
        "                        print(f\"    ğŸ”¸ ä»…{dataset2}æœ‰({len(only_in_2)}): {sorted(list(only_in_2))}\")\n",
        "                        _show_unique_columns_source(dataset2, only_in_2, dataset_csv_info)\n",
        "        \n",
        "        if not has_pair_differences:\n",
        "            print(\"  âœ… æ‰€æœ‰æ•°æ®é›†å®Œå…¨åŒ¹é…\")\n",
        "    \n",
        "    # æ¨èçš„ç»Ÿä¸€ç­–ç•¥\n",
        "    print(f\"\\nğŸ’¡ ç»Ÿä¸€ç­–ç•¥å»ºè®®:\")\n",
        "    if len(common_columns) == len(all_columns):\n",
        "        print(\"âœ… æ‰€æœ‰æ•°æ®é›†åˆ—å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€è°ƒæ•´\")\n",
        "    else:\n",
        "        print(f\"ğŸ”§ å»ºè®®ç»Ÿä¸€åˆ°å…±åŒåˆ—é›†åˆ ({len(common_columns)}åˆ—)\")\n",
        "        print(f\"   è¿™å°†ç¡®ä¿æ‰€æœ‰æ•°æ®é›†å…·æœ‰ç›¸åŒçš„ç‰¹å¾ç»“æ„\")\n",
        "        \n",
        "        lost_columns = all_columns - common_columns\n",
        "        if lost_columns:\n",
        "            print(f\"   âš ï¸ å°†ä¸¢å¤±çš„åˆ—: {sorted(list(lost_columns))}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "def _analyze_csv_files_columns(csv_files_info, app_type):\n",
        "    \"\"\"\n",
        "    åˆ†æå•ä¸ªAPPç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶çš„åˆ—å·®å¼‚ - ç²¾ç®€ç‰ˆï¼Œåªæ˜¾ç¤ºå…³é”®ä¸ä¸€è‡´ä¿¡æ¯\n",
        "    \"\"\"\n",
        "    if not csv_files_info:\n",
        "        return\n",
        "    \n",
        "    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„åˆ—ä¿¡æ¯\n",
        "    file_column_counts = {}\n",
        "    \n",
        "    for i, file_info in enumerate(csv_files_info):\n",
        "        filename = os.path.basename(file_info['file'])\n",
        "        file_columns = set(file_info['columns'])\n",
        "        col_count = len(file_columns)\n",
        "        \n",
        "        file_column_counts[filename] = {\n",
        "            'columns': file_columns,\n",
        "            'count': col_count\n",
        "        }\n",
        "    \n",
        "    # åˆ†æåˆ—æ•°åˆ†å¸ƒ\n",
        "    count_distribution = {}\n",
        "    for filename, info in file_column_counts.items():\n",
        "        count = info['count']\n",
        "        if count not in count_distribution:\n",
        "            count_distribution[count] = []\n",
        "        count_distribution[count].append(filename)\n",
        "    \n",
        "    # åªåœ¨å‘ç°ä¸ä¸€è‡´æ—¶è¾“å‡ºè¯¦ç»†ä¿¡æ¯\n",
        "    if len(count_distribution) > 1:\n",
        "        print(f\"      âŒ {app_type}åˆ—æ•°ä¸ä¸€è‡´:\")\n",
        "        for count in sorted(count_distribution.keys()):\n",
        "            files = count_distribution[count]\n",
        "            print(f\"        {count}åˆ—: {files}\")\n",
        "        \n",
        "        # è®¡ç®—å…±åŒåˆ—å¹¶åªæ˜¾ç¤ºæœ‰ç‹¬æœ‰åˆ—çš„æ–‡ä»¶\n",
        "        common_columns_in_app = set(csv_files_info[0]['columns'])\n",
        "        for file_info in csv_files_info[1:]:\n",
        "            common_columns_in_app = common_columns_in_app.intersection(set(file_info['columns']))\n",
        "        \n",
        "        # åªæ˜¾ç¤ºæœ‰ç‹¬æœ‰åˆ—çš„ç‰¹æ®Šæ–‡ä»¶\n",
        "        special_files = []\n",
        "        for filename, info in file_column_counts.items():\n",
        "            file_columns = info['columns']\n",
        "            unique_columns = file_columns - common_columns_in_app\n",
        "            if unique_columns:\n",
        "                special_files.append(f\"{filename}(+{sorted(list(unique_columns))})\")\n",
        "        \n",
        "        if special_files:\n",
        "            print(f\"      ğŸ”¸ ç‰¹æ®Šæ–‡ä»¶: {', '.join(special_files)}\")\n",
        "\n",
        "\n",
        "def _show_unique_columns_source(dataset_name, unique_columns, dataset_csv_info):\n",
        "    \"\"\"\n",
        "    è¾…åŠ©å‡½æ•°ï¼šç®€æ´æ˜¾ç¤ºç‹¬æœ‰åˆ—æ¥è‡ªå“ªä¸ªCSVæ–‡ä»¶\n",
        "    \"\"\"\n",
        "    csv_info = dataset_csv_info.get(dataset_name, {})\n",
        "    source_files = []\n",
        "    \n",
        "    # æ£€æŸ¥APP_0 (æ­£å¸¸æµé‡) çš„CSVæ–‡ä»¶\n",
        "    normal_meta = csv_info.get('normal_meta', {})\n",
        "    if 'original_columns_info' in normal_meta:\n",
        "        for file_info in normal_meta['original_columns_info']:\n",
        "            file_columns = set(file_info['columns'])\n",
        "            file_unique = file_columns.intersection(unique_columns)\n",
        "            if file_unique:\n",
        "                filename = os.path.basename(file_info['file'])\n",
        "                source_files.append(f\"APP_0/{filename}\")\n",
        "    \n",
        "    # æ£€æŸ¥APP_1 (PCDNæµé‡) çš„CSVæ–‡ä»¶\n",
        "    pcdn_meta = csv_info.get('pcdn_meta', {})\n",
        "    if 'original_columns_info' in pcdn_meta:\n",
        "        for file_info in pcdn_meta['original_columns_info']:\n",
        "            file_columns = set(file_info['columns'])\n",
        "            file_unique = file_columns.intersection(unique_columns)\n",
        "            if file_unique:\n",
        "                filename = os.path.basename(file_info['file'])\n",
        "                source_files.append(f\"APP_1/{filename}\")\n",
        "    \n",
        "    if source_files:\n",
        "        print(f\"      ğŸ“ æ¥æº: {', '.join(source_files)}\")\n",
        "\n",
        "print(\"âœ… æ•°æ®åŠ è½½å‡½æ•°å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ æ™ºèƒ½æ•°æ®åŠ è½½ï¼šå››çº§åˆ—ä¸€è‡´æ€§æ£€æŸ¥\n",
        "print(\"ğŸš€ å¼€å§‹æ™ºèƒ½æ•°æ®åŠ è½½...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ğŸ”§ æ•°æ®ç›®å½•é…ç½®\n",
        "data_dir = \"./pcdn_32_pkts_2class_feature_enhance_v17.4_dataset\"\n",
        "\n",
        "# æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨\n",
        "if not os.path.exists(data_dir):\n",
        "    print(f\"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}\")\n",
        "    print(\"è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å¤¹ä¸notebookåœ¨åŒä¸€ç›®å½•ä¸‹\")\n",
        "    print(\"å½“å‰å·¥ä½œç›®å½•:\", os.getcwd())\n",
        "    print(\"å½“å‰ç›®å½•å†…å®¹:\", [f for f in os.listdir('.') if not f.startswith('.')])\n",
        "    \n",
        "    # å°è¯•æŸ¥æ‰¾æ•°æ®ç›®å½•\n",
        "    possible_dirs = [d for d in os.listdir('.') if 'pcdn' in d.lower() and os.path.isdir(d)]\n",
        "    if possible_dirs:\n",
        "        print(f\"å‘ç°å¯èƒ½çš„æ•°æ®ç›®å½•: {possible_dirs}\")\n",
        "        data_dir = possible_dirs[0]\n",
        "        print(f\"âœ… ä½¿ç”¨æ•°æ®ç›®å½•: {data_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"æ‰¾ä¸åˆ°æ•°æ®ç›®å½•ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ä½ç½®\")\n",
        "else:\n",
        "    print(f\"âœ… æ‰¾åˆ°æ•°æ®ç›®å½•: {data_dir}\")\n",
        "\n",
        "# æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„åˆ—ä¿¡æ¯\n",
        "datasets_meta = {}\n",
        "\n",
        "# ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®\n",
        "print(f\"\\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...\")\n",
        "train_normal, train_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_0'), 0)\n",
        "train_pcdn, train_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Training_set', 'APP_1'), 1)\n",
        "\n",
        "# åˆå¹¶è®­ç»ƒæ•°æ®å¹¶è®°å½•å…ƒä¿¡æ¯\n",
        "train_data = pd.concat([train_normal, train_pcdn], ignore_index=True)\n",
        "datasets_meta['Training_set'] = {\n",
        "    'final_columns': list(train_data.columns),\n",
        "    'dataset_path': 'Training_set',\n",
        "    'shape': train_data.shape,\n",
        "    'normal_meta': train_normal_meta,\n",
        "    'pcdn_meta': train_pcdn_meta\n",
        "}\n",
        "\n",
        "print(f\"âœ… è®­ç»ƒæ•°æ®: {len(train_normal)}ä¸ªæ­£å¸¸ + {len(train_pcdn)}ä¸ªPCDN = {len(train_data)}æ€»æ ·æœ¬\")\n",
        "\n",
        "# ğŸ“‚ åŠ è½½éªŒè¯æ•°æ®\n",
        "print(f\"\\nğŸ“‚ åŠ è½½éªŒè¯æ•°æ®...\")\n",
        "val_normal, val_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_0'), 0)\n",
        "val_pcdn, val_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Validation_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥éªŒè¯æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(val_normal) == 0 and len(val_pcdn) == 0:\n",
        "    print(\"âš ï¸ éªŒè¯é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†\")\n",
        "    val_data = pd.DataFrame()\n",
        "    datasets_meta['Validation_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    val_data = pd.concat([val_normal, val_pcdn], ignore_index=True)\n",
        "    datasets_meta['Validation_set'] = {\n",
        "        'final_columns': list(val_data.columns),\n",
        "        'dataset_path': 'Validation_set',\n",
        "        'shape': val_data.shape,\n",
        "        'normal_meta': val_normal_meta,\n",
        "        'pcdn_meta': val_pcdn_meta\n",
        "    }\n",
        "    print(f\"âœ… éªŒè¯æ•°æ®: {len(val_normal)}ä¸ªæ­£å¸¸ + {len(val_pcdn)}ä¸ªPCDN = {len(val_data)}æ€»æ ·æœ¬\")\n",
        "\n",
        "# ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®\n",
        "print(f\"\\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...\")\n",
        "test_normal, test_normal_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_0'), 0)\n",
        "test_pcdn, test_pcdn_meta = load_data_from_directory(os.path.join(data_dir, 'Testing_set', 'APP_1'), 1)\n",
        "\n",
        "# æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦ä¸ºç©º\n",
        "if len(test_normal) == 0 and len(test_pcdn) == 0:\n",
        "    print(\"âš ï¸ æµ‹è¯•é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†\")\n",
        "    test_data = pd.DataFrame()\n",
        "    datasets_meta['Testing_set'] = {'final_columns': [], 'shape': (0, 0), 'status': 'empty'}\n",
        "else:\n",
        "    test_data = pd.concat([test_normal, test_pcdn], ignore_index=True)\n",
        "    datasets_meta['Testing_set'] = {\n",
        "        'final_columns': list(test_data.columns),\n",
        "        'dataset_path': 'Testing_set', \n",
        "        'shape': test_data.shape,\n",
        "        'normal_meta': test_normal_meta,\n",
        "        'pcdn_meta': test_pcdn_meta\n",
        "    }\n",
        "    print(f\"âœ… æµ‹è¯•æ•°æ®: {len(test_normal)}ä¸ªæ­£å¸¸ + {len(test_pcdn)}ä¸ªPCDN = {len(test_data)}æ€»æ ·æœ¬\")\n",
        "\n",
        "# ğŸ“Š æ‰§è¡Œè·¨æ•°æ®é›†åˆ—ç»“æ„åˆ†æ\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ” å¼€å§‹è·¨æ•°æ®é›†åˆ—ç»“æ„åˆ†æ...\")\n",
        "analyze_cross_dataset_columns(datasets_meta)\n",
        "\n",
        "# ğŸ“‹ æ•°æ®åŠ è½½æ€»ç»“\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“‹ æ•°æ®åŠ è½½æ€»ç»“\")\n",
        "print(\"=\"*80)\n",
        "for dataset_name, meta in datasets_meta.items():\n",
        "    if 'status' in meta and meta['status'] == 'empty':\n",
        "        print(f\"ğŸ“‚ {dataset_name}: ç©ºæ•°æ®é›†\")\n",
        "    else:\n",
        "        print(f\"ğŸ“‚ {dataset_name}: {meta['shape'][0]}è¡Œ x {meta['shape'][1]}åˆ—\")\n",
        "\n",
        "print(\"ğŸ‰ æ™ºèƒ½æ•°æ®åŠ è½½å’Œè·¨æ•°æ®é›†åˆ†æå®Œæˆï¼\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š CSVæ ¼å¼å’Œå­—æ®µæ¢ç´¢åˆ†æ\n",
        "\n",
        "print(\"ğŸ“Š å¼€å§‹CSVæ ¼å¼å’Œå­—æ®µåˆ†æ...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. åŸºæœ¬æ•°æ®ä¿¡æ¯\n",
        "print(\"ğŸ” **æ•°æ®é›†åŸºæœ¬ä¿¡æ¯**:\")\n",
        "print(f\"  è®­ç»ƒé›†å½¢çŠ¶: {train_data.shape}\")\n",
        "if len(val_data) > 0:\n",
        "    print(f\"  éªŒè¯é›†å½¢çŠ¶: {val_data.shape}\")\n",
        "else:\n",
        "    print(f\"  éªŒè¯é›†: ç©º (å°†ä»è®­ç»ƒé›†åˆ†å‰²)\")\n",
        "if len(test_data) > 0:\n",
        "    print(f\"  æµ‹è¯•é›†å½¢çŠ¶: {test_data.shape}\")\n",
        "else:\n",
        "    print(f\"  æµ‹è¯•é›†: ç©º (å°†ä»è®­ç»ƒé›†åˆ†å‰²)\")\n",
        "\n",
        "# 2. CSVæ–‡ä»¶è¯¦ç»†ä¿¡æ¯å±•ç¤º\n",
        "print(f\"\\nğŸ“ **CSVæ–‡ä»¶è¯¦ç»†ä¿¡æ¯**:\")\n",
        "if 'Training_set' in datasets_meta:\n",
        "    train_meta = datasets_meta['Training_set']\n",
        "    print(f\"  è®­ç»ƒé›†æ–‡ä»¶åˆ†å¸ƒ:\")\n",
        "    \n",
        "    # æ˜¾ç¤ºAPP_0æ–‡ä»¶ä¿¡æ¯\n",
        "    if 'normal_meta' in train_meta and 'original_columns_info' in train_meta['normal_meta']:\n",
        "        normal_files = train_meta['normal_meta']['original_columns_info']\n",
        "        print(f\"    ğŸ“‚ APP_0 (æ­£å¸¸æµé‡): {len(normal_files)}ä¸ªCSVæ–‡ä»¶\")\n",
        "        for i, file_info in enumerate(normal_files):\n",
        "            filename = os.path.basename(file_info['file'])\n",
        "            col_count = file_info['count']\n",
        "            print(f\"      æ–‡ä»¶{i+1}: {filename} ({col_count}åˆ—)\")\n",
        "    \n",
        "    # æ˜¾ç¤ºAPP_1æ–‡ä»¶ä¿¡æ¯  \n",
        "    if 'pcdn_meta' in train_meta and 'original_columns_info' in train_meta['pcdn_meta']:\n",
        "        pcdn_files = train_meta['pcdn_meta']['original_columns_info']\n",
        "        print(f\"    ğŸ“‚ APP_1 (PCDNæµé‡): {len(pcdn_files)}ä¸ªCSVæ–‡ä»¶\")\n",
        "        for i, file_info in enumerate(pcdn_files):\n",
        "            filename = os.path.basename(file_info['file'])\n",
        "            col_count = file_info['count']\n",
        "            print(f\"      æ–‡ä»¶{i+1}: {filename} ({col_count}åˆ—)\")\n",
        "\n",
        "# 3. åˆ—åå’Œæ•°æ®ç±»å‹åˆ†æ\n",
        "print(f\"\\nğŸ“‹ **åˆ—åå’Œæ•°æ®ç±»å‹** (è®­ç»ƒé›†ç¤ºä¾‹):\")\n",
        "print(f\"  æ€»åˆ—æ•°: {len(train_data.columns)}\")\n",
        "print(f\"  æ•°æ®ç±»å‹åˆ†å¸ƒ:\")\n",
        "dtype_counts = train_data.dtypes.value_counts()\n",
        "for dtype, count in dtype_counts.items():\n",
        "    print(f\"    {dtype}: {count}åˆ—\")\n",
        "\n",
        "print(f\"\\nğŸ“ **æ‰€æœ‰åˆ—ååˆ—è¡¨**:\")\n",
        "all_columns = [col for col in train_data.columns if col not in ['label', 'source_file']]\n",
        "print(f\"  åŸå§‹ç‰¹å¾åˆ—æ•°: {len(all_columns)}\")\n",
        "\n",
        "# åˆ†ç»„æ˜¾ç¤ºåˆ—å\n",
        "chunk_size = 10\n",
        "for i in range(0, len(all_columns), chunk_size):\n",
        "    chunk = all_columns[i:i+chunk_size]\n",
        "    print(f\"  {i+1:2d}-{min(i+chunk_size, len(all_columns)):2d}: {chunk}\")\n",
        "\n",
        "# 4. æ•°æ®æ ·æœ¬å±•ç¤º\n",
        "print(f\"\\nğŸ“– **æ•°æ®æ ·æœ¬å±•ç¤º** (å‰5è¡Œ):\")\n",
        "display_columns = [col for col in train_data.columns if col not in ['source_file']][:15]  # åªæ˜¾ç¤ºå‰15åˆ—é¿å…è¿‡å®½\n",
        "print(\"é€‰æ‹©æ˜¾ç¤ºå‰15åˆ—:\", display_columns)\n",
        "print(train_data[display_columns].head())\n",
        "\n",
        "# 5. æ ‡ç­¾åˆ†å¸ƒåˆ†æ\n",
        "print(f\"\\nğŸ·ï¸ **æ ‡ç­¾åˆ†å¸ƒåˆ†æ**:\")\n",
        "label_counts = train_data['label'].value_counts().sort_index()\n",
        "total_samples = len(train_data)\n",
        "\n",
        "for label, count in label_counts.items():\n",
        "    label_name = \"æ­£å¸¸æµé‡\" if label == 0 else \"PCDNæµé‡\"\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"  æ ‡ç­¾ {label} ({label_name}): {count:,} æ ·æœ¬ ({percentage:.1f}%)\")\n",
        "\n",
        "# 6. ç¼ºå¤±å€¼åˆ†æ\n",
        "print(f\"\\nğŸ” **ç¼ºå¤±å€¼åˆ†æ**:\")\n",
        "missing_counts = train_data.isnull().sum()\n",
        "missing_columns = missing_counts[missing_counts > 0]\n",
        "\n",
        "if len(missing_columns) > 0:\n",
        "    print(f\"  å‘ç° {len(missing_columns)} åˆ—æœ‰ç¼ºå¤±å€¼:\")\n",
        "    for col, count in missing_columns.items():\n",
        "        percentage = (count / len(train_data)) * 100\n",
        "        print(f\"    {col}: {count} ç¼ºå¤±å€¼ ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"  âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼\")\n",
        "\n",
        "# 7. æ•°å€¼å‹ç‰¹å¾ç»Ÿè®¡æè¿°\n",
        "print(f\"\\nğŸ“ˆ **æ•°å€¼å‹ç‰¹å¾ç»Ÿè®¡æè¿°**:\")\n",
        "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols = [col for col in numeric_cols if col != 'label'][:10]  # é€‰æ‹©å‰10ä¸ªæ•°å€¼åˆ—\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    print(f\"  é€‰æ‹©å‰10ä¸ªæ•°å€¼å‹ç‰¹å¾è¿›è¡Œç»Ÿè®¡:\")\n",
        "    stats_df = train_data[numeric_cols].describe()\n",
        "    print(stats_df)\n",
        "else:\n",
        "    print(\"  æ²¡æœ‰å‘ç°æ•°å€¼å‹ç‰¹å¾\")\n",
        "\n",
        "# 8. åºåˆ—ç‰¹å¾åˆ†æ (å¦‚æœå­˜åœ¨)\n",
        "sequence_features = ['ip_direction', 'pkt_len', 'iat']\n",
        "existing_seq_features = [col for col in sequence_features if col in train_data.columns]\n",
        "\n",
        "if existing_seq_features:\n",
        "    print(f\"\\nğŸ”„ **åºåˆ—ç‰¹å¾åˆ†æ**:\")\n",
        "    print(f\"  å‘ç°åºåˆ—ç‰¹å¾: {existing_seq_features}\")\n",
        "    \n",
        "    for col in existing_seq_features[:2]:  # åªåˆ†æå‰2ä¸ªé¿å…è¾“å‡ºè¿‡é•¿\n",
        "        print(f\"\\n  ğŸ“Š {col} ç‰¹å¾æ ·æœ¬:\")\n",
        "        # æ˜¾ç¤ºå‡ ä¸ªåºåˆ—æ ·æœ¬\n",
        "        for i in range(min(3, len(train_data))):\n",
        "            sample_value = train_data[col].iloc[i]\n",
        "            print(f\"    æ ·æœ¬{i+1}: {str(sample_value)[:100]}{'...' if len(str(sample_value)) > 100 else ''}\")\n",
        "else:\n",
        "    print(f\"\\nğŸ”„ **åºåˆ—ç‰¹å¾**: æœªå‘ç°åºåˆ—ç‰¹å¾ {sequence_features}\")\n",
        "\n",
        "# 9. æ•°æ®è´¨é‡æ£€æŸ¥\n",
        "print(f\"\\nğŸ” **æ•°æ®è´¨é‡æ£€æŸ¥**:\")\n",
        "print(f\"  æ— ç©·å€¼æ£€æŸ¥:\")\n",
        "inf_cols = []\n",
        "for col in train_data.select_dtypes(include=[np.number]).columns:\n",
        "    inf_count = np.isinf(train_data[col]).sum()\n",
        "    if inf_count > 0:\n",
        "        inf_cols.append((col, inf_count))\n",
        "\n",
        "if inf_cols:\n",
        "    print(f\"    å‘ç°æ— ç©·å€¼: {len(inf_cols)} åˆ—\")\n",
        "    for col, count in inf_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª\n",
        "        print(f\"      {col}: {count} ä¸ªæ— ç©·å€¼\")\n",
        "else:\n",
        "    print(\"    âœ… æ²¡æœ‰å‘ç°æ— ç©·å€¼\")\n",
        "\n",
        "# 10. åˆ›å»ºæ•°æ®æ¦‚è§ˆå¯è§†åŒ–\n",
        "print(f\"\\nğŸ“Š ç”Ÿæˆæ•°æ®æ¦‚è§ˆå›¾è¡¨...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Dataset Overview Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 10.1 æ ‡ç­¾åˆ†å¸ƒé¥¼å›¾\n",
        "ax1 = axes[0, 0]\n",
        "label_names = ['Normal Traffic', 'PCDN Traffic']\n",
        "label_values = [label_counts.get(0, 0), label_counts.get(1, 0)]\n",
        "colors = ['lightblue', 'lightcoral']\n",
        "ax1.pie(label_values, labels=label_names, autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "ax1.set_title('Label Distribution')\n",
        "\n",
        "# 10.2 æ•°æ®ç±»å‹åˆ†å¸ƒ\n",
        "ax2 = axes[0, 1]\n",
        "dtype_names = [str(dtype) for dtype in dtype_counts.index]\n",
        "ax2.bar(range(len(dtype_names)), dtype_counts.values, color='lightgreen')\n",
        "ax2.set_xticks(range(len(dtype_names)))\n",
        "ax2.set_xticklabels(dtype_names, rotation=45)\n",
        "ax2.set_title('Data Types Distribution')\n",
        "ax2.set_ylabel('Number of Columns')\n",
        "\n",
        "# 10.3 ç¼ºå¤±å€¼çƒ­å›¾ (å¦‚æœæœ‰ç¼ºå¤±å€¼)\n",
        "ax3 = axes[1, 0]\n",
        "if len(missing_columns) > 0:\n",
        "    missing_data = train_data[missing_columns.index[:10]].isnull()  # æœ€å¤šæ˜¾ç¤º10åˆ—\n",
        "    sns.heatmap(missing_data, ax=ax3, cbar=True, yticklabels=False, cmap='viridis')\n",
        "    ax3.set_title('Missing Values Pattern')\n",
        "else:\n",
        "    ax3.text(0.5, 0.5, 'No Missing Values Found', transform=ax3.transAxes, \n",
        "             fontsize=14, ha='center', va='center')\n",
        "    ax3.set_title('Missing Values Check')\n",
        "    ax3.set_xticks([])\n",
        "    ax3.set_yticks([])\n",
        "\n",
        "# 10.4 æ•°å€¼ç‰¹å¾åˆ†å¸ƒæ ·æœ¬\n",
        "ax4 = axes[1, 1]\n",
        "if len(numeric_cols) >= 2:\n",
        "    # é€‰æ‹©ä¸¤ä¸ªæ•°å€¼ç‰¹å¾ç»˜åˆ¶æ•£ç‚¹å›¾\n",
        "    col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "    normal_data = train_data[train_data['label'] == 0]\n",
        "    pcdn_data = train_data[train_data['label'] == 1]\n",
        "    \n",
        "    ax4.scatter(normal_data[col1], normal_data[col2], alpha=0.6, label='Normal', s=20)\n",
        "    ax4.scatter(pcdn_data[col1], pcdn_data[col2], alpha=0.6, label='PCDN', s=20)\n",
        "    ax4.set_xlabel(col1)\n",
        "    ax4.set_ylabel(col2)\n",
        "    ax4.set_title(f'Feature Scatter: {col1} vs {col2}')\n",
        "    ax4.legend()\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'Insufficient Numeric Features', transform=ax4.transAxes, \n",
        "             fontsize=12, ha='center', va='center')\n",
        "    ax4.set_title('Feature Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nâœ… CSVæ ¼å¼å’Œå­—æ®µåˆ†æå®Œæˆï¼\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼šåˆ é™¤æŒ‡å®šåˆ—ã€å¤„ç†ç¼ºå¤±å€¼ç­‰\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # åˆ é™¤æŒ‡å®šçš„ä¸éœ€è¦çš„åˆ—\n",
        "    columns_to_drop = [\n",
        "        'source_file',  # æºæ–‡ä»¶ä¿¡æ¯ï¼ˆæ·»åŠ çš„è¾…åŠ©åˆ—ï¼‰\n",
        "        'frame.number', 'frame.time_relative', 'ip.version', 'ip.ttl', 'ip.src', 'ip.dst',\n",
        "        'ipv6.plen', 'ipv6.nxt', 'ipv6.src', 'ipv6.dst', '_ws.col.Protocol', \n",
        "        'ssl.handshake.extensions_server_name', 'eth.src', 'pcap_duration', 'app', 'os', 'date',\n",
        "        'flow_id', 'dpi_file_name', 'dpi_five_tuple', 'dpi_rule_result', 'dpi_label', \n",
        "        'ulProtoID', 'dpi_rule_pkt', 'dpi_packets', 'dpi_bytes', 'label_source', 'id', 'category'\n",
        "    ]\n",
        "    \n",
        "    # åˆ é™¤å­˜åœ¨çš„åˆ—\n",
        "    existing_columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    if existing_columns_to_drop:\n",
        "        df_processed = df_processed.drop(columns=existing_columns_to_drop)\n",
        "        print(f\"âœ… åˆ é™¤äº† {len(existing_columns_to_drop)} ä¸ªæŒ‡å®šåˆ—\")\n",
        "        \n",
        "    # å¤„ç†ç¼ºå¤±å€¼å’Œæ— ç©·å€¼\n",
        "    df_processed = df_processed.fillna(0)\n",
        "    df_processed = df_processed.replace([np.inf, -np.inf], 0)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "\n",
        "def process_sequence_features(df, enable_sequence_processing=True):\n",
        "    \"\"\"\n",
        "    å¤„ç†åºåˆ—ç±»å‹çš„ç‰¹å¾ (pkt_len, ip_direction, iat)\n",
        "    enable_sequence_processing: True=è½¬æ¢ä¸ºç»Ÿè®¡ç‰¹å¾, False=ç›´æ¥åˆ é™¤\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    sequence_columns = ['pkt_len', 'ip_direction', 'iat']\n",
        "    \n",
        "    if not enable_sequence_processing:\n",
        "        # ç®€å•æ¨¡å¼ï¼šç›´æ¥åˆ é™¤åºåˆ—ç‰¹å¾\n",
        "        print(\"ğŸ—‘ï¸ åˆ é™¤åºåˆ—ç‰¹å¾æ¨¡å¼...\")\n",
        "        for col in sequence_columns:\n",
        "            if col in df_processed.columns:\n",
        "                df_processed = df_processed.drop(columns=[col])\n",
        "        print(f\"âœ… å·²åˆ é™¤åºåˆ—ç‰¹å¾: {sequence_columns}\")\n",
        "        return df_processed\n",
        "    \n",
        "    # å¤æ‚æ¨¡å¼ï¼šåºåˆ—ç‰¹å¾å·¥ç¨‹\n",
        "    print(\"ğŸ”§ åºåˆ—ç‰¹å¾å·¥ç¨‹æ¨¡å¼...\")\n",
        "    \n",
        "    for col in sequence_columns:\n",
        "        if col in df_processed.columns:\n",
        "            print(f\"å¤„ç†åºåˆ—ç‰¹å¾: {col}\")\n",
        "            \n",
        "            # å®‰å…¨åœ°è§£æåºåˆ—æ•°æ®\n",
        "            sequences = []\n",
        "            for idx, value in df_processed[col].items():\n",
        "                try:\n",
        "                    if pd.isna(value) or value == '' or value == 'nan':\n",
        "                        sequences.append([])\n",
        "                    else:\n",
        "                        # ä½¿ç”¨ ast.literal_eval å®‰å…¨è§£æ\n",
        "                        if isinstance(value, str):\n",
        "                            parsed = ast.literal_eval(value)\n",
        "                            sequences.append(parsed if isinstance(parsed, list) else [])\n",
        "                        else:\n",
        "                            sequences.append([])\n",
        "                except:\n",
        "                    sequences.append([])\n",
        "            \n",
        "            # æå–ç»Ÿè®¡ç‰¹å¾\n",
        "            feature_dict = {}\n",
        "            \n",
        "            for i, seq in enumerate(sequences):\n",
        "                if len(seq) > 0:\n",
        "                    # åŸºç¡€ç»Ÿè®¡ç‰¹å¾\n",
        "                    feature_dict.setdefault(f'{col}_mean', []).append(np.mean(seq))\n",
        "                    feature_dict.setdefault(f'{col}_std', []).append(np.std(seq))\n",
        "                    feature_dict.setdefault(f'{col}_min', []).append(np.min(seq))\n",
        "                    feature_dict.setdefault(f'{col}_max', []).append(np.max(seq))\n",
        "                    feature_dict.setdefault(f'{col}_median', []).append(np.median(seq))\n",
        "                    feature_dict.setdefault(f'{col}_range', []).append(np.max(seq) - np.min(seq))\n",
        "                    feature_dict.setdefault(f'{col}_q25', []).append(np.percentile(seq, 25))\n",
        "                    feature_dict.setdefault(f'{col}_q75', []).append(np.percentile(seq, 75))\n",
        "                    feature_dict.setdefault(f'{col}_iqr', []).append(np.percentile(seq, 75) - np.percentile(seq, 25))\n",
        "                    feature_dict.setdefault(f'{col}_len', []).append(len(seq))\n",
        "                    \n",
        "                    # å˜å¼‚ç³»æ•°\n",
        "                    cv = np.std(seq) / np.mean(seq) if np.mean(seq) != 0 else 0\n",
        "                    feature_dict.setdefault(f'{col}_cv', []).append(cv)\n",
        "                    \n",
        "                    # ååº¦å’Œå³°åº¦\n",
        "                    try:\n",
        "                        feature_dict.setdefault(f'{col}_skew', []).append(stats.skew(seq))\n",
        "                        feature_dict.setdefault(f'{col}_kurtosis', []).append(stats.kurtosis(seq))\n",
        "                    except:\n",
        "                        feature_dict.setdefault(f'{col}_skew', []).append(0)\n",
        "                        feature_dict.setdefault(f'{col}_kurtosis', []).append(0)\n",
        "                    \n",
        "                    # åºåˆ—ç‰¹å®šç‰¹å¾\n",
        "                    if col == 'ip_direction':\n",
        "                        # å‡ºæ–¹å‘æ¯”ä¾‹\n",
        "                        out_ratio = sum(1 for x in seq if x == 1) / len(seq)\n",
        "                        feature_dict.setdefault(f'{col}_out_ratio', []).append(out_ratio)\n",
        "                    elif col == 'pkt_len':\n",
        "                        # å°åŒ…æ¯”ä¾‹ (<=64å­—èŠ‚)\n",
        "                        small_pkt_ratio = sum(1 for x in seq if x <= 64) / len(seq)\n",
        "                        feature_dict.setdefault(f'{col}_small_pkt_ratio', []).append(small_pkt_ratio)\n",
        "                    elif col == 'iat':\n",
        "                        # çªå‘æ¯”ä¾‹ (é—´éš”<=0.1ç§’)\n",
        "                        burst_ratio = sum(1 for x in seq if x <= 0.1) / len(seq)\n",
        "                        feature_dict.setdefault(f'{col}_burst_ratio', []).append(burst_ratio)\n",
        "                else:\n",
        "                    # å¤„ç†ç©ºåºåˆ—\n",
        "                    for feature_name in [f'{col}_mean', f'{col}_std', f'{col}_min', f'{col}_max', \n",
        "                                       f'{col}_median', f'{col}_range', f'{col}_q25', f'{col}_q75', \n",
        "                                       f'{col}_iqr', f'{col}_len', f'{col}_cv', f'{col}_skew', f'{col}_kurtosis']:\n",
        "                        feature_dict.setdefault(feature_name, []).append(0)\n",
        "                    \n",
        "                    # åºåˆ—ç‰¹å®šç‰¹å¾çš„é»˜è®¤å€¼\n",
        "                    if col == 'ip_direction':\n",
        "                        feature_dict.setdefault(f'{col}_out_ratio', []).append(0)\n",
        "                    elif col == 'pkt_len':\n",
        "                        feature_dict.setdefault(f'{col}_small_pkt_ratio', []).append(0)\n",
        "                    elif col == 'iat':\n",
        "                        feature_dict.setdefault(f'{col}_burst_ratio', []).append(0)\n",
        "            \n",
        "            # æ·»åŠ æ–°ç‰¹å¾åˆ°DataFrame\n",
        "            for feature_name, feature_values in feature_dict.items():\n",
        "                df_processed[feature_name] = feature_values\n",
        "            \n",
        "            # åˆ é™¤åŸå§‹åºåˆ—åˆ—\n",
        "            df_processed = df_processed.drop(columns=[col])\n",
        "            \n",
        "            print(f\"  âœ… {col} -> ç”Ÿæˆäº† {len(feature_dict)} ä¸ªç»Ÿè®¡ç‰¹å¾\")\n",
        "    \n",
        "    # å¤„ç†éæ•°å€¼åˆ—\n",
        "    print(\"\\nğŸ”§ å¤„ç†éæ•°å€¼ç‰¹å¾...\")\n",
        "    label_encoders = {}\n",
        "    \n",
        "    for col in df_processed.columns:\n",
        "        if col != 'label' and df_processed[col].dtype == 'object':\n",
        "            try:\n",
        "                le = LabelEncoder()\n",
        "                df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
        "                label_encoders[col] = le\n",
        "                print(f\"  âœ… æ ‡ç­¾ç¼–ç : {col}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  âš ï¸ æ— æ³•ç¼–ç  {col}, åˆ é™¤è¯¥åˆ—: {e}\")\n",
        "                df_processed = df_processed.drop(columns=[col])\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "\n",
        "# ğŸ”§ æ‰§è¡Œæ•°æ®é¢„å¤„ç†\n",
        "print(\"å¼€å§‹æ•°æ®é¢„å¤„ç†...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# é¢„å¤„ç†è®­ç»ƒæ•°æ®\n",
        "print(\"ğŸ“Š å¤„ç†è®­ç»ƒæ•°æ®...\")\n",
        "train_processed = preprocess_data(train_data)\n",
        "train_processed = process_sequence_features(train_processed, ENABLE_SEQUENCE_FEATURES)\n",
        "\n",
        "# é¢„å¤„ç†éªŒè¯æ•°æ®\n",
        "if len(val_data) > 0:\n",
        "    print(\"\\nğŸ“Š å¤„ç†éªŒè¯æ•°æ®...\")\n",
        "    val_processed = preprocess_data(val_data)\n",
        "    val_processed = process_sequence_features(val_processed, ENABLE_SEQUENCE_FEATURES)\n",
        "else:\n",
        "    val_processed = pd.DataFrame()\n",
        "\n",
        "# é¢„å¤„ç†æµ‹è¯•æ•°æ®\n",
        "if len(test_data) > 0:\n",
        "    print(\"\\nğŸ“Š å¤„ç†æµ‹è¯•æ•°æ®...\")\n",
        "    test_processed = preprocess_data(test_data)\n",
        "    test_processed = process_sequence_features(test_processed, ENABLE_SEQUENCE_FEATURES)\n",
        "else:\n",
        "    test_processed = pd.DataFrame()\n",
        "\n",
        "print(\"\\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ æœ€ç»ˆç‰¹å¾å¯¹é½å’Œè®­ç»ƒæ•°æ®å‡†å¤‡\n",
        "\n",
        "print(\"ğŸ”§ æ‰§è¡Œæœ€ç»ˆç‰¹å¾å¯¹é½...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. æ£€æŸ¥é¢„å¤„ç†åçš„æ•°æ®é›†çŠ¶æ€\n",
        "print(\"ğŸ“Š é¢„å¤„ç†åæ•°æ®é›†çŠ¶æ€:\")\n",
        "train_cols = list(train_processed.columns)\n",
        "val_cols = list(val_processed.columns) if len(val_processed) > 0 else []\n",
        "test_cols = list(test_processed.columns) if len(test_processed) > 0 else []\n",
        "\n",
        "print(f\"  è®­ç»ƒé›†: {train_processed.shape} (åˆ—æ•°: {len(train_cols)})\")\n",
        "if len(val_processed) > 0:\n",
        "    print(f\"  éªŒè¯é›†: {val_processed.shape} (åˆ—æ•°: {len(val_cols)})\")\n",
        "else:\n",
        "    print(f\"  éªŒè¯é›†: ç©ºæ•°æ®é›†\")\n",
        "if len(test_processed) > 0:\n",
        "    print(f\"  æµ‹è¯•é›†: {test_processed.shape} (åˆ—æ•°: {len(test_cols)})\")\n",
        "else:\n",
        "    print(f\"  æµ‹è¯•é›†: ç©ºæ•°æ®é›†\")\n",
        "\n",
        "# 2. ç‰¹å¾å¯¹é½\n",
        "active_datasets = [train_processed]\n",
        "active_col_sets = [set(train_cols)]\n",
        "\n",
        "if len(val_processed) > 0:\n",
        "    active_datasets.append(val_processed)\n",
        "    active_col_sets.append(set(val_cols))\n",
        "if len(test_processed) > 0:\n",
        "    active_datasets.append(test_processed)\n",
        "    active_col_sets.append(set(test_cols))\n",
        "\n",
        "# è®¡ç®—å…±åŒç‰¹å¾\n",
        "if len(active_col_sets) > 1:\n",
        "    common_features = set.intersection(*active_col_sets)\n",
        "    common_feature_list = sorted(list(common_features))\n",
        "    \n",
        "    print(f\"\\nğŸ”§ ç‰¹å¾å¯¹é½ç»“æœ:\")\n",
        "    print(f\"  å…±åŒç‰¹å¾æ•°: {len(common_feature_list)}\")\n",
        "    \n",
        "    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¹é½\n",
        "    needs_alignment = False\n",
        "    for i, dataset in enumerate(active_datasets):\n",
        "        current_cols = set(dataset.columns)\n",
        "        if len(current_cols) != len(common_features):\n",
        "            needs_alignment = True\n",
        "            unique_cols = current_cols - common_features\n",
        "            if unique_cols:\n",
        "                dataset_name = [\"è®­ç»ƒé›†\", \"éªŒè¯é›†\", \"æµ‹è¯•é›†\"][i]\n",
        "                print(f\"  {dataset_name}ç‹¬æœ‰åˆ—: {sorted(list(unique_cols))}\")\n",
        "    \n",
        "    if needs_alignment:\n",
        "        print(f\"\\nğŸ”§ æ‰§è¡Œç‰¹å¾å¯¹é½...\")\n",
        "        train_processed = train_processed[common_feature_list]\n",
        "        if len(val_processed) > 0:\n",
        "            val_processed = val_processed[common_feature_list]\n",
        "        if len(test_processed) > 0:\n",
        "            test_processed = test_processed[common_feature_list]\n",
        "        print(\"âœ… ç‰¹å¾å¯¹é½å®Œæˆ\")\n",
        "    else:\n",
        "        print(\"âœ… æ‰€æœ‰æ•°æ®é›†ç‰¹å¾å·²ç»ä¸€è‡´\")\n",
        "else:\n",
        "    common_feature_list = train_cols\n",
        "    print(\"âœ… åªæœ‰è®­ç»ƒé›†ï¼Œæ— éœ€å¯¹é½\")\n",
        "\n",
        "# 3. å¤„ç†ç©ºçš„éªŒè¯/æµ‹è¯•é›†\n",
        "if len(val_processed) == 0 or len(test_processed) == 0:\n",
        "    print(f\"\\nğŸ”§ å¤„ç†ç©ºæ•°æ®é›†...\")\n",
        "    \n",
        "    # ä»è®­ç»ƒé›†ä¸­åˆ†å‰²å‡ºéªŒè¯é›†å’Œæµ‹è¯•é›†\n",
        "    X_temp = train_processed.drop(columns=['label'])\n",
        "    y_temp = train_processed['label']\n",
        "    \n",
        "    if len(val_processed) == 0 and len(test_processed) == 0:\n",
        "        # ä¸¤ä¸ªéƒ½ä¸ºç©ºï¼Œåˆ†å‰²ä¸º 70% è®­ç»ƒ, 15% éªŒè¯, 15% æµ‹è¯•\n",
        "        X_train_split, X_temp_split, y_train_split, y_temp_split = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp)\n",
        "        X_val_split, X_test_split, y_val_split, y_test_split = train_test_split(\n",
        "            X_temp_split, y_temp_split, test_size=0.5, random_state=42, stratify=y_temp_split)\n",
        "        \n",
        "        train_processed = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "        val_processed = pd.concat([X_val_split, y_val_split], axis=1)\n",
        "        test_processed = pd.concat([X_test_split, y_test_split], axis=1)\n",
        "        \n",
        "        print(f\"  âœ… åˆ†å‰²æ•°æ®: è®­ç»ƒ{len(train_processed)} éªŒè¯{len(val_processed)} æµ‹è¯•{len(test_processed)}\")\n",
        "        \n",
        "    elif len(val_processed) == 0:\n",
        "        # åªæœ‰éªŒè¯é›†ä¸ºç©ºï¼Œåˆ†å‰²ä¸º 80% è®­ç»ƒ, 20% éªŒè¯\n",
        "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "        \n",
        "        train_processed = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "        val_processed = pd.concat([X_val_split, y_val_split], axis=1)\n",
        "        \n",
        "        print(f\"  âœ… ç”ŸæˆéªŒè¯é›†: è®­ç»ƒ{len(train_processed)} éªŒè¯{len(val_processed)}\")\n",
        "        \n",
        "    elif len(test_processed) == 0:\n",
        "        # åªæœ‰æµ‹è¯•é›†ä¸ºç©ºï¼Œåˆ†å‰²ä¸º 80% è®­ç»ƒ, 20% æµ‹è¯•\n",
        "        X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "        \n",
        "        train_processed = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "        test_processed = pd.concat([X_test_split, y_test_split], axis=1)\n",
        "        \n",
        "        print(f\"  âœ… ç”Ÿæˆæµ‹è¯•é›†: è®­ç»ƒ{len(train_processed)} æµ‹è¯•{len(test_processed)}\")\n",
        "\n",
        "# 4. å‡†å¤‡æœ€ç»ˆè®­ç»ƒæ•°æ®\n",
        "feature_columns = [col for col in train_processed.columns if col != 'label']\n",
        "print(f\"\\nğŸ“ˆ æœ€ç»ˆæ•°æ®å‡†å¤‡:\")\n",
        "print(f\"  ç‰¹å¾æ•°é‡: {len(feature_columns)}\")\n",
        "\n",
        "X_train = train_processed[feature_columns]\n",
        "y_train = train_processed['label']\n",
        "\n",
        "X_val = val_processed[feature_columns]\n",
        "y_val = val_processed['label']\n",
        "\n",
        "X_test = test_processed[feature_columns]\n",
        "y_test = test_processed['label']\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æ•°å€¼å‹\n",
        "X_train = X_train.select_dtypes(include=[np.number])\n",
        "X_val = X_val.select_dtypes(include=[np.number])\n",
        "X_test = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\nğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶:\")\n",
        "print(f\"  è®­ç»ƒé›†: {X_train.shape}\")\n",
        "print(f\"  éªŒè¯é›†: {X_val.shape}\")\n",
        "print(f\"  æµ‹è¯•é›†: {X_test.shape}\")\n",
        "\n",
        "# 5. æ•°æ®è´¨é‡æ£€æŸ¥\n",
        "print(f\"\\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:\")\n",
        "print(f\"  ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§: {X_train.shape[1] == X_val.shape[1] == X_test.shape[1]}\")\n",
        "print(f\"  è®­ç»ƒé›†NaN: {X_train.isnull().any().any()}\")\n",
        "print(f\"  éªŒè¯é›†NaN: {X_val.isnull().any().any()}\")\n",
        "print(f\"  æµ‹è¯•é›†NaN: {X_test.isnull().any().any()}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:\")\n",
        "print(f\"  è®­ç»ƒé›†: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"  éªŒè¯é›†: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"  æµ‹è¯•é›†: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "if X_train.shape[1] == X_val.shape[1] == X_test.shape[1]:\n",
        "    print(f\"\\nğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼æ‰€æœ‰æ•°æ®é›†ç‰¹å¾ç»´åº¦ä¸€è‡´: {X_train.shape[1]}ä¸ªç‰¹å¾\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ä¿å­˜ç‰¹å¾åç§°ä»¥ä¾›åç»­ä½¿ç”¨\n",
        "feature_names = list(X_train.columns)\n",
        "print(f\"ğŸ’¾ å·²ä¿å­˜ {len(feature_names)} ä¸ªç‰¹å¾åç§°\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¤– XGBoostæ¨¡å‹è®­ç»ƒï¼ˆç‰ˆæœ¬å…¼å®¹ï¼‰\n",
        "\n",
        "print(\"ğŸ¤– å¼€å§‹XGBoostæ¨¡å‹è®­ç»ƒ...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# æ£€æŸ¥XGBoostç‰ˆæœ¬\n",
        "import xgboost\n",
        "print(f\"XGBoostç‰ˆæœ¬: {xgboost.__version__}\")\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "output_dir = \"output\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}\")\n",
        "\n",
        "# åˆ›å»ºXGBooståˆ†ç±»å™¨ - ä½¿ç”¨ä¿å®ˆå‚æ•°ç¡®ä¿ç¨³å®šæ€§\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    verbosity=0  # å‡å°‘è¾“å‡º\n",
        ")\n",
        "\n",
        "# ğŸ”§ ç‰ˆæœ¬å…¼å®¹çš„è®­ç»ƒç­–ç•¥ï¼ˆä¸‰çº§å›é€€æœºåˆ¶ï¼‰\n",
        "training_success = False\n",
        "training_method = \"\"\n",
        "\n",
        "print(\"\\nğŸ”§ å¼€å§‹æ™ºèƒ½è®­ç»ƒï¼ˆç‰ˆæœ¬å…¼å®¹ï¼‰...\")\n",
        "\n",
        "try:\n",
        "    # æ–¹æ³•1: æ–°ç‰ˆæœ¬XGBoostæ–¹å¼ï¼ˆæ¨èï¼‰\n",
        "    print(\"ğŸ”„ å°è¯•æ–¹æ³•1: æ–°ç‰ˆæœ¬è®­ç»ƒæ–¹å¼...\")\n",
        "    xgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "    training_success = True\n",
        "    training_method = \"æ–°ç‰ˆæœ¬æ–¹å¼\"\n",
        "    print(\"âœ… æ–¹æ³•1æˆåŠŸ\")\n",
        "    \n",
        "except TypeError as e:\n",
        "    if \"early_stopping_rounds\" in str(e):\n",
        "        print(\"âš ï¸ æ–¹æ³•1å¤±è´¥ï¼šearly_stopping_roundså‚æ•°é—®é¢˜\")\n",
        "        \n",
        "        try:\n",
        "            # æ–¹æ³•2: åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶è®¾ç½®early_stopping_rounds\n",
        "            print(\"ğŸ”„ å°è¯•æ–¹æ³•2: åˆå§‹åŒ–æ—¶è®¾ç½®early_stopping...\")\n",
        "            xgb_model = xgb.XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                eval_metric='logloss',\n",
        "                early_stopping_rounds=10,\n",
        "                verbosity=0\n",
        "            )\n",
        "            xgb_model.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "            training_success = True\n",
        "            training_method = \"åˆå§‹åŒ–early_stoppingæ–¹å¼\"\n",
        "            print(\"âœ… æ–¹æ³•2æˆåŠŸ\")\n",
        "            \n",
        "        except Exception as e2:\n",
        "            print(f\"âš ï¸ æ–¹æ³•2å¤±è´¥: {e2}\")\n",
        "            \n",
        "            try:\n",
        "                # æ–¹æ³•3: ç¦ç”¨early stoppingï¼Œå¢åŠ è®­ç»ƒè½®æ•°\n",
        "                print(\"ğŸ”„ å°è¯•æ–¹æ³•3: ç¦ç”¨early stopping...\")\n",
        "                xgb_model = xgb.XGBClassifier(\n",
        "                    n_estimators=150,  # å¢åŠ è½®æ•°è¡¥å¿\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42,\n",
        "                    eval_metric='logloss',\n",
        "                    verbosity=0\n",
        "                )\n",
        "                xgb_model.fit(X_train, y_train)\n",
        "                training_success = True\n",
        "                training_method = \"æ— early stoppingæ–¹å¼\"\n",
        "                print(\"âœ… æ–¹æ³•3æˆåŠŸ\")\n",
        "                \n",
        "            except Exception as e3:\n",
        "                print(f\"âŒ æ–¹æ³•3ä¹Ÿå¤±è´¥: {e3}\")\n",
        "                training_success = False\n",
        "    else:\n",
        "        print(f\"âŒ è®­ç»ƒå¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯: {e}\")\n",
        "        training_success = False\n",
        "\n",
        "if training_success:\n",
        "    print(f\"\\nğŸ‰ æ¨¡å‹è®­ç»ƒæˆåŠŸï¼\")\n",
        "    print(f\"ğŸ“‹ ä½¿ç”¨æ–¹æ³•: {training_method}\")\n",
        "    print(f\"ğŸ“Š è®­ç»ƒæ•°æ®: {X_train.shape[0]}æ ·æœ¬, {X_train.shape[1]}ç‰¹å¾\")\n",
        "    print(f\"ğŸ“Š éªŒè¯æ•°æ®: {X_val.shape[0]}æ ·æœ¬\")\n",
        "    print(f\"ğŸ“Š æµ‹è¯•æ•°æ®: {X_test.shape[0]}æ ·æœ¬\")\n",
        "else:\n",
        "    raise RuntimeError(\"âŒ æ‰€æœ‰è®­ç»ƒæ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥XGBoostç‰ˆæœ¬å’Œæ•°æ®\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½åˆ†æ\n",
        "\n",
        "print(\"ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# æ¨¡å‹é¢„æµ‹\n",
        "print(\"ğŸ”„ æ‰§è¡Œé¢„æµ‹...\")\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_train_pred_proba = xgb_model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "y_val_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "y_test_pred = xgb_model.predict(X_test)\n",
        "y_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"âœ… é¢„æµ‹å®Œæˆ\")\n",
        "\n",
        "# è®¡ç®—æ€§èƒ½æŒ‡æ ‡\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_pred_proba, dataset_name):\n",
        "    \"\"\"è®¡ç®—å¹¶æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡\"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # åŸºç¡€åˆ†ç±»æŒ‡æ ‡\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred)\n",
        "    metrics['recall'] = recall_score(y_true, y_pred)\n",
        "    metrics['f1'] = f1_score(y_true, y_pred)\n",
        "    metrics['auc'] = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    print(f\"\\nğŸ“‹ {dataset_name}æ€§èƒ½æŒ‡æ ‡:\")\n",
        "    print(f\"  å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.4f}\")\n",
        "    print(f\"  å¬å›ç‡ (Recall):    {metrics['recall']:.4f}\")\n",
        "    print(f\"  F1åˆ†æ•° (F1-Score):  {metrics['f1']:.4f}\")\n",
        "    print(f\"  AUCåˆ†æ•°:           {metrics['auc']:.4f}\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# è®¡ç®—å„æ•°æ®é›†çš„æ€§èƒ½æŒ‡æ ‡\n",
        "train_metrics = calculate_metrics(y_train, y_train_pred, y_train_pred_proba, \"è®­ç»ƒé›†\")\n",
        "val_metrics = calculate_metrics(y_val, y_val_pred, y_val_pred_proba, \"éªŒè¯é›†\")\n",
        "test_metrics = calculate_metrics(y_test, y_test_pred, y_test_pred_proba, \"æµ‹è¯•é›†\")\n",
        "\n",
        "# æ£€æŸ¥è¿‡æ‹Ÿåˆæƒ…å†µ\n",
        "print(f\"\\nğŸ” è¿‡æ‹Ÿåˆæ£€æŸ¥:\")\n",
        "train_test_auc_diff = train_metrics['auc'] - test_metrics['auc']\n",
        "train_test_acc_diff = train_metrics['accuracy'] - test_metrics['accuracy']\n",
        "\n",
        "print(f\"  è®­ç»ƒé›† vs æµ‹è¯•é›† AUCå·®å¼‚: {train_test_auc_diff:.4f}\")\n",
        "print(f\"  è®­ç»ƒé›† vs æµ‹è¯•é›† å‡†ç¡®ç‡å·®å¼‚: {train_test_acc_diff:.4f}\")\n",
        "\n",
        "if train_test_auc_diff > 0.1 or train_test_acc_diff > 0.1:\n",
        "    print(\"  âš ï¸ å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®è°ƒæ•´æ¨¡å‹å‚æ•°\")\n",
        "elif train_test_auc_diff < 0.02 and train_test_acc_diff < 0.02:\n",
        "    print(\"  âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½\")\n",
        "else:\n",
        "    print(\"  âœ… æ¨¡å‹æ€§èƒ½æ­£å¸¸\")\n",
        "\n",
        "# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
        "print(f\"\\nğŸ“ˆ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (æµ‹è¯•é›†):\")\n",
        "test_report = classification_report(y_test, y_test_pred, target_names=['æ­£å¸¸æµé‡', 'PCDNæµé‡'])\n",
        "print(test_report)\n",
        "\n",
        "# åˆ›å»ºæ€§èƒ½æ€»ç»“å­—å…¸\n",
        "performance_summary = {\n",
        "    'training_method': training_method,\n",
        "    'feature_count': X_train.shape[1],\n",
        "    'train_samples': X_train.shape[0],\n",
        "    'val_samples': X_val.shape[0],\n",
        "    'test_samples': X_test.shape[0],\n",
        "    'train_metrics': train_metrics,\n",
        "    'val_metrics': val_metrics,\n",
        "    'test_metrics': test_metrics,\n",
        "    'sequence_features_enabled': ENABLE_SEQUENCE_FEATURES\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ’¾ æ€§èƒ½æ€»ç»“å·²ä¿å­˜åˆ°å˜é‡ 'performance_summary'\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ ç‰¹å¾é‡è¦æ€§åˆ†æå’Œå¯è§†åŒ–\n",
        "\n",
        "print(\"ğŸ¯ å¼€å§‹ç‰¹å¾é‡è¦æ€§åˆ†æ...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# è·å–ç‰¹å¾é‡è¦æ€§\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"ğŸ“Š æ€»ç‰¹å¾æ•°: {len(feature_names)}\")\n",
        "print(f\"ğŸ“ˆ é‡è¦æ€§åˆ†æå®Œæˆ\")\n",
        "\n",
        "# æ˜¾ç¤ºå‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾\n",
        "print(f\"\\nğŸ” å‰20ä¸ªæœ€é‡è¦ç‰¹å¾:\")\n",
        "top_features = feature_importance_df.head(20)\n",
        "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
        "    print(f\"  {i:2d}. {row['feature']:30} | é‡è¦æ€§: {row['importance']:.4f}\")\n",
        "\n",
        "# åˆ†æåºåˆ—ç‰¹å¾vséåºåˆ—ç‰¹å¾çš„é‡è¦æ€§\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(f\"\\nğŸ” åºåˆ—ç‰¹å¾ vs åŸå§‹ç‰¹å¾é‡è¦æ€§åˆ†æ:\")\n",
        "    \n",
        "    sequence_keywords = ['pkt_len_', 'ip_direction_', 'iat_']\n",
        "    sequence_features = []\n",
        "    original_features = []\n",
        "    \n",
        "    for _, row in feature_importance_df.iterrows():\n",
        "        is_sequence = any(keyword in row['feature'] for keyword in sequence_keywords)\n",
        "        if is_sequence:\n",
        "            sequence_features.append(row['importance'])\n",
        "        else:\n",
        "            original_features.append(row['importance'])\n",
        "    \n",
        "    seq_avg_importance = np.mean(sequence_features) if sequence_features else 0\n",
        "    orig_avg_importance = np.mean(original_features) if original_features else 0\n",
        "    \n",
        "    print(f\"  åºåˆ—ç‰¹å¾æ•°é‡: {len(sequence_features)}\")\n",
        "    print(f\"  åŸå§‹ç‰¹å¾æ•°é‡: {len(original_features)}\")\n",
        "    print(f\"  åºåˆ—ç‰¹å¾å¹³å‡é‡è¦æ€§: {seq_avg_importance:.4f}\")\n",
        "    print(f\"  åŸå§‹ç‰¹å¾å¹³å‡é‡è¦æ€§: {orig_avg_importance:.4f}\")\n",
        "    \n",
        "    if seq_avg_importance > orig_avg_importance:\n",
        "        print(f\"  ğŸ’¡ åºåˆ—ç‰¹å¾å·¥ç¨‹æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼\")\n",
        "    else:\n",
        "        print(f\"  ğŸ’¡ åŸå§‹ç‰¹å¾ä»ç„¶æ˜¯ä¸»è¦çš„é¢„æµ‹å› å­\")\n",
        "\n",
        "# åˆ›å»ºç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\n",
        "print(f\"\\nğŸ“Š ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾è¡¨...\")\n",
        "\n",
        "# è®¾ç½®å›¾è¡¨\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "fig.suptitle('XGBoost Model Analysis - Feature Importance & Performance', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. å‰15ä¸ªæœ€é‡è¦ç‰¹å¾æ¡å½¢å›¾\n",
        "ax1 = axes[0, 0]\n",
        "top_15_features = feature_importance_df.head(15)\n",
        "bars = ax1.barh(range(len(top_15_features)), top_15_features['importance'], color='skyblue')\n",
        "ax1.set_yticks(range(len(top_15_features)))\n",
        "ax1.set_yticklabels(top_15_features['feature'], fontsize=9)\n",
        "ax1.set_xlabel('Feature Importance')\n",
        "ax1.set_title('Top 15 Most Important Features')\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
        "             f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
        "\n",
        "# 2. ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒç›´æ–¹å›¾\n",
        "ax2 = axes[0, 1]\n",
        "ax2.hist(feature_importance, bins=30, color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "ax2.set_xlabel('Feature Importance')\n",
        "ax2.set_ylabel('Number of Features')\n",
        "ax2.set_title('Distribution of Feature Importance')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
        "mean_importance = np.mean(feature_importance)\n",
        "median_importance = np.median(feature_importance)\n",
        "ax2.axvline(mean_importance, color='red', linestyle='--', label=f'Mean: {mean_importance:.4f}')\n",
        "ax2.axvline(median_importance, color='blue', linestyle='--', label=f'Median: {median_importance:.4f}')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. åºåˆ—ç‰¹å¾vsåŸå§‹ç‰¹å¾å¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨äº†åºåˆ—ç‰¹å¾ï¼‰\n",
        "ax3 = axes[1, 0]\n",
        "if ENABLE_SEQUENCE_FEATURES and sequence_features and original_features:\n",
        "    # ç®±çº¿å›¾æ¯”è¾ƒ\n",
        "    data_to_plot = [sequence_features, original_features]\n",
        "    bp = ax3.boxplot(data_to_plot, labels=['Sequence Features', 'Original Features'], patch_artist=True)\n",
        "    bp['boxes'][0].set_facecolor('lightcoral')\n",
        "    bp['boxes'][1].set_facecolor('lightblue')\n",
        "    ax3.set_ylabel('Feature Importance')\n",
        "    ax3.set_title('Sequence vs Original Features Importance')\n",
        "    ax3.grid(alpha=0.3)\n",
        "else:\n",
        "    # å¦‚æœæ²¡æœ‰åºåˆ—ç‰¹å¾ï¼Œæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾çš„é¥¼å›¾\n",
        "    top_10 = feature_importance_df.head(10)\n",
        "    other_importance = feature_importance_df.iloc[10:]['importance'].sum()\n",
        "    \n",
        "    labels = list(top_10['feature']) + ['Others']\n",
        "    sizes = list(top_10['importance']) + [other_importance]\n",
        "    \n",
        "    ax3.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "    ax3.set_title('Top 10 Features Contribution')\n",
        "\n",
        "# 4. ç´¯ç§¯é‡è¦æ€§æ›²çº¿\n",
        "ax4 = axes[1, 1]\n",
        "cumulative_importance = np.cumsum(feature_importance_df['importance'])\n",
        "ax4.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\n",
        "ax4.set_xlabel('Number of Features')\n",
        "ax4.set_ylabel('Cumulative Importance')\n",
        "ax4.set_title('Cumulative Feature Importance')\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "# æ·»åŠ å…³é”®é˜ˆå€¼çº¿\n",
        "threshold_80 = np.where(cumulative_importance >= 0.8)[0]\n",
        "threshold_90 = np.where(cumulative_importance >= 0.9)[0]\n",
        "\n",
        "if len(threshold_80) > 0:\n",
        "    ax4.axvline(threshold_80[0] + 1, color='orange', linestyle='--', \n",
        "                label=f'80% importance: {threshold_80[0] + 1} features')\n",
        "if len(threshold_90) > 0:\n",
        "    ax4.axvline(threshold_90[0] + 1, color='red', linestyle='--', \n",
        "                label=f'90% importance: {threshold_90[0] + 1} features')\n",
        "\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ä¿å­˜ç‰¹å¾é‡è¦æ€§æ•°æ®\n",
        "feature_importance_summary = {\n",
        "    'top_20_features': top_features.to_dict('records'),\n",
        "    'total_features': len(feature_names),\n",
        "    'sequence_features_count': len(sequence_features) if ENABLE_SEQUENCE_FEATURES else 0,\n",
        "    'original_features_count': len(original_features) if ENABLE_SEQUENCE_FEATURES else len(feature_names),\n",
        "    'mean_importance': float(mean_importance),\n",
        "    'median_importance': float(median_importance)\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ’¾ ç‰¹å¾é‡è¦æ€§åˆ†æå·²ä¿å­˜åˆ°å˜é‡ 'feature_importance_summary'\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“ˆ ROCæ›²çº¿å’Œæ··æ·†çŸ©é˜µå¯è§†åŒ–\n",
        "\n",
        "print(\"ğŸ“ˆ ç”ŸæˆROCæ›²çº¿å’Œæ··æ·†çŸ©é˜µ...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨\n",
        "fig, axes = plt.subplots(2, 3, figsize=(21, 14))\n",
        "fig.suptitle('XGBoost Model Performance Visualization', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. ROCæ›²çº¿ - è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†å¯¹æ¯”\n",
        "ax1 = axes[0, 0]\n",
        "\n",
        "# è®¡ç®—ROCæ›²çº¿æ•°æ®\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
        "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_pred_proba)\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_proba)\n",
        "\n",
        "# ç»˜åˆ¶ROCæ›²çº¿\n",
        "ax1.plot(fpr_train, tpr_train, 'b-', label=f'Training (AUC = {train_metrics[\"auc\"]:.3f})', linewidth=2)\n",
        "ax1.plot(fpr_val, tpr_val, 'g-', label=f'Validation (AUC = {val_metrics[\"auc\"]:.3f})', linewidth=2)\n",
        "ax1.plot(fpr_test, tpr_test, 'r-', label=f'Test (AUC = {test_metrics[\"auc\"]:.3f})', linewidth=2)\n",
        "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC = 0.5)')\n",
        "\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate')\n",
        "ax1.set_title('ROC Curves Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. è®­ç»ƒé›†æ··æ·†çŸ©é˜µ\n",
        "ax2 = axes[0, 1]\n",
        "train_cm = confusion_matrix(y_train, y_train_pred)\n",
        "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
        "            xticklabels=['Normal', 'PCDN'], yticklabels=['Normal', 'PCDN'])\n",
        "ax2.set_title('Training Set - Confusion Matrix')\n",
        "ax2.set_xlabel('Predicted')\n",
        "ax2.set_ylabel('Actual')\n",
        "\n",
        "# 3. éªŒè¯é›†æ··æ·†çŸ©é˜µ\n",
        "ax3 = axes[0, 2]\n",
        "val_cm = confusion_matrix(y_val, y_val_pred)\n",
        "sns.heatmap(val_cm, annot=True, fmt='d', cmap='Greens', ax=ax3,\n",
        "            xticklabels=['Normal', 'PCDN'], yticklabels=['Normal', 'PCDN'])\n",
        "ax3.set_title('Validation Set - Confusion Matrix')\n",
        "ax3.set_xlabel('Predicted')\n",
        "ax3.set_ylabel('Actual')\n",
        "\n",
        "# 4. æµ‹è¯•é›†æ··æ·†çŸ©é˜µ\n",
        "ax4 = axes[1, 0]\n",
        "test_cm = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Reds', ax=ax4,\n",
        "            xticklabels=['Normal', 'PCDN'], yticklabels=['Normal', 'PCDN'])\n",
        "ax4.set_title('Test Set - Confusion Matrix')\n",
        "ax4.set_xlabel('Predicted')\n",
        "ax4.set_ylabel('Actual')\n",
        "\n",
        "# 5. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”\n",
        "ax5 = axes[1, 1]\n",
        "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
        "train_values = [train_metrics['accuracy'], train_metrics['precision'], \n",
        "                train_metrics['recall'], train_metrics['f1'], train_metrics['auc']]\n",
        "val_values = [val_metrics['accuracy'], val_metrics['precision'], \n",
        "              val_metrics['recall'], val_metrics['f1'], val_metrics['auc']]\n",
        "test_values = [test_metrics['accuracy'], test_metrics['precision'], \n",
        "               test_metrics['recall'], test_metrics['f1'], test_metrics['auc']]\n",
        "\n",
        "x = np.arange(len(metrics_names))\n",
        "width = 0.25\n",
        "\n",
        "ax5.bar(x - width, train_values, width, label='Training', color='skyblue', alpha=0.8)\n",
        "ax5.bar(x, val_values, width, label='Validation', color='lightgreen', alpha=0.8)\n",
        "ax5.bar(x + width, test_values, width, label='Test', color='lightcoral', alpha=0.8)\n",
        "\n",
        "ax5.set_xlabel('Metrics')\n",
        "ax5.set_ylabel('Score')\n",
        "ax5.set_title('Performance Metrics Comparison')\n",
        "ax5.set_xticks(x)\n",
        "ax5.set_xticklabels(metrics_names, rotation=45)\n",
        "ax5.legend()\n",
        "ax5.grid(axis='y', alpha=0.3)\n",
        "ax5.set_ylim(0, 1.1)\n",
        "\n",
        "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
        "for i, (train_val, val_val, test_val) in enumerate(zip(train_values, val_values, test_values)):\n",
        "    ax5.text(i - width, train_val + 0.01, f'{train_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    ax5.text(i, val_val + 0.01, f'{val_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    ax5.text(i + width, test_val + 0.01, f'{test_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 6. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n",
        "ax6 = axes[1, 2]\n",
        "\n",
        "# åˆ†åˆ«è·å–æ­£å¸¸æµé‡å’ŒPCDNæµé‡çš„é¢„æµ‹æ¦‚ç‡\n",
        "normal_proba = y_test_pred_proba[y_test == 0]\n",
        "pcdn_proba = y_test_pred_proba[y_test == 1]\n",
        "\n",
        "ax6.hist(normal_proba, bins=30, alpha=0.6, label='Normal Traffic', color='blue', density=True)\n",
        "ax6.hist(pcdn_proba, bins=30, alpha=0.6, label='PCDN Traffic', color='red', density=True)\n",
        "ax6.set_xlabel('Predicted Probability (PCDN)')\n",
        "ax6.set_ylabel('Density')\n",
        "ax6.set_title('Prediction Probability Distribution')\n",
        "ax6.legend()\n",
        "ax6.grid(alpha=0.3)\n",
        "\n",
        "# æ·»åŠ å†³ç­–é˜ˆå€¼çº¿ï¼ˆé»˜è®¤0.5ï¼‰\n",
        "ax6.axvline(0.5, color='green', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# è®¡ç®—è¯¦ç»†çš„æ··æ·†çŸ©é˜µç»Ÿè®¡\n",
        "def analyze_confusion_matrix(cm, dataset_name):\n",
        "    \"\"\"åˆ†ææ··æ·†çŸ©é˜µå¹¶è¿”å›è¯¦ç»†ç»Ÿè®¡\"\"\"\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    # è®¡ç®—å„ç§ç‡\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # çœŸæ­£ç‡/å¬å›ç‡\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # çœŸè´Ÿç‡\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0         # ç²¾ç¡®ç‡/é˜³æ€§é¢„æµ‹å€¼\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0         # é˜´æ€§é¢„æµ‹å€¼\n",
        "    \n",
        "    return {\n",
        "        'dataset': dataset_name,\n",
        "        'true_negative': int(tn),\n",
        "        'false_positive': int(fp),\n",
        "        'false_negative': int(fn),\n",
        "        'true_positive': int(tp),\n",
        "        'sensitivity_recall': sensitivity,\n",
        "        'specificity': specificity,\n",
        "        'precision_ppv': ppv,\n",
        "        'negative_predictive_value': npv\n",
        "    }\n",
        "\n",
        "# åˆ†æå„æ•°æ®é›†çš„æ··æ·†çŸ©é˜µ\n",
        "print(f\"\\nğŸ“Š è¯¦ç»†æ··æ·†çŸ©é˜µåˆ†æ:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cm_analysis = {\n",
        "    'train': analyze_confusion_matrix(train_cm, \"è®­ç»ƒé›†\"),\n",
        "    'val': analyze_confusion_matrix(val_cm, \"éªŒè¯é›†\"),\n",
        "    'test': analyze_confusion_matrix(test_cm, \"æµ‹è¯•é›†\")\n",
        "}\n",
        "\n",
        "for dataset, analysis in cm_analysis.items():\n",
        "    print(f\"\\nğŸ“‹ {analysis['dataset']}:\")\n",
        "    print(f\"  çœŸè´Ÿä¾‹ (TN): {analysis['true_negative']:4d} | å‡æ­£ä¾‹ (FP): {analysis['false_positive']:4d}\")\n",
        "    print(f\"  å‡è´Ÿä¾‹ (FN): {analysis['false_negative']:4d} | çœŸæ­£ä¾‹ (TP): {analysis['true_positive']:4d}\")\n",
        "    print(f\"  æ•æ„Ÿæ€§/å¬å›ç‡: {analysis['sensitivity_recall']:.4f}\")\n",
        "    print(f\"  ç‰¹å¼‚æ€§:       {analysis['specificity']:.4f}\")\n",
        "    print(f\"  ç²¾ç¡®ç‡:       {analysis['precision_ppv']:.4f}\")\n",
        "    print(f\"  é˜´æ€§é¢„æµ‹å€¼:   {analysis['negative_predictive_value']:.4f}\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ’¾ ç»“æœä¿å­˜å’Œé¡¹ç›®æ€»ç»“\n",
        "\n",
        "print(\"ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œåˆ†æç»“æœ...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# åˆ›å»ºæ—¶é—´æˆ³\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 1. ä¿å­˜XGBoostæ¨¡å‹\n",
        "try:\n",
        "    model_path = os.path.join(output_dir, f\"xgboost_model_{timestamp}.pkl\")\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(xgb_model, f)\n",
        "    print(f\"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# 2. ä¿å­˜ç‰¹å¾é‡è¦æ€§\n",
        "try:\n",
        "    feature_importance_path = os.path.join(output_dir, f\"feature_importance_{timestamp}.csv\")\n",
        "    feature_importance_df.to_csv(feature_importance_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {feature_importance_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ç‰¹å¾é‡è¦æ€§ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# 3. ä¿å­˜å®Œæ•´æ€§èƒ½æŠ¥å‘Š\n",
        "performance_report = {\n",
        "    'timestamp': timestamp,\n",
        "    'experiment_info': {\n",
        "        'sequence_features_enabled': ENABLE_SEQUENCE_FEATURES,\n",
        "        'training_method': training_method,\n",
        "        'xgboost_version': xgboost.__version__,\n",
        "        'feature_count': len(feature_names),\n",
        "        'data_samples': {\n",
        "            'train': X_train.shape[0],\n",
        "            'validation': X_val.shape[0],\n",
        "            'test': X_test.shape[0]\n",
        "        }\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'train': train_metrics,\n",
        "        'validation': val_metrics,\n",
        "        'test': test_metrics\n",
        "    },\n",
        "    'feature_analysis': feature_importance_summary,\n",
        "    'confusion_matrix_analysis': cm_analysis,\n",
        "    'model_hyperparameters': {\n",
        "        'n_estimators': xgb_model.n_estimators,\n",
        "        'max_depth': xgb_model.max_depth,\n",
        "        'learning_rate': xgb_model.learning_rate,\n",
        "        'subsample': xgb_model.subsample,\n",
        "        'colsample_bytree': xgb_model.colsample_bytree\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    report_path = os.path.join(output_dir, f\"performance_report_{timestamp}.json\")\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(performance_report, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"âœ… æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ æ€§èƒ½æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# 4. ä¿å­˜é¢„æµ‹ç»“æœ\n",
        "try:\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'actual': y_test,\n",
        "        'predicted': y_test_pred,\n",
        "        'probability_pcdn': y_test_pred_proba,\n",
        "        'correct': y_test == y_test_pred\n",
        "    })\n",
        "    predictions_path = os.path.join(output_dir, f\"test_predictions_{timestamp}.csv\")\n",
        "    predictions_df.to_csv(predictions_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²ä¿å­˜: {predictions_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ é¢„æµ‹ç»“æœä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "# 5. ç”Ÿæˆé¡¹ç›®æ€»ç»“æŠ¥å‘Š\n",
        "print(f\"\\nğŸ“‹ é¡¹ç›®æ‰§è¡Œæ€»ç»“:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"ğŸ¯ **å®éªŒé…ç½®**\")\n",
        "print(f\"  åºåˆ—ç‰¹å¾å¤„ç†: {'å¯ç”¨' if ENABLE_SEQUENCE_FEATURES else 'ç¦ç”¨'}\")\n",
        "print(f\"  XGBoostç‰ˆæœ¬: {xgboost.__version__}\")\n",
        "print(f\"  è®­ç»ƒæ–¹å¼: {training_method}\")\n",
        "print(f\"  ç‰¹å¾æ€»æ•°: {len(feature_names)}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š **æ•°æ®ç»Ÿè®¡**\")\n",
        "print(f\"  è®­ç»ƒæ ·æœ¬: {X_train.shape[0]:,}\")\n",
        "print(f\"  éªŒè¯æ ·æœ¬: {X_val.shape[0]:,}\")\n",
        "print(f\"  æµ‹è¯•æ ·æœ¬: {X_test.shape[0]:,}\")\n",
        "\n",
        "print(f\"\\nğŸ† **æœ€ä½³æ€§èƒ½ (æµ‹è¯•é›†)**\")\n",
        "print(f\"  å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  ç²¾ç¡®ç‡: {test_metrics['precision']:.4f}\")\n",
        "print(f\"  å¬å›ç‡: {test_metrics['recall']:.4f}\")\n",
        "print(f\"  F1åˆ†æ•°: {test_metrics['f1']:.4f}\")\n",
        "print(f\"  AUCåˆ†æ•°: {test_metrics['auc']:.4f}\")\n",
        "\n",
        "if ENABLE_SEQUENCE_FEATURES:\n",
        "    print(f\"\\nğŸ”§ **åºåˆ—ç‰¹å¾å·¥ç¨‹æ•ˆæœ**\")\n",
        "    print(f\"  ç”Ÿæˆåºåˆ—ç‰¹å¾æ•°: {feature_importance_summary['sequence_features_count']}\")\n",
        "    print(f\"  åŸå§‹ç‰¹å¾æ•°: {feature_importance_summary['original_features_count']}\")\n",
        "    \n",
        "    # ç»Ÿè®¡å‰10é‡è¦ç‰¹å¾ä¸­åºåˆ—ç‰¹å¾çš„æ¯”ä¾‹\n",
        "    top_10_features = feature_importance_df.head(10)['feature'].tolist()\n",
        "    sequence_in_top10 = sum(1 for feat in top_10_features \n",
        "                           if any(keyword in feat for keyword in ['pkt_len_', 'ip_direction_', 'iat_']))\n",
        "    print(f\"  å‰10é‡è¦ç‰¹å¾ä¸­åºåˆ—ç‰¹å¾å æ¯”: {sequence_in_top10}/10 ({sequence_in_top10*10:.0f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ“ **è¾“å‡ºæ–‡ä»¶**\")\n",
        "output_files = [f for f in os.listdir(output_dir) if timestamp in f]\n",
        "for file in sorted(output_files):\n",
        "    print(f\"  {file}\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ **å…³é”®å‘ç°**\")\n",
        "# è‡ªåŠ¨ç”Ÿæˆå…³é”®å‘ç°\n",
        "findings = []\n",
        "\n",
        "# æ€§èƒ½åˆ†æ\n",
        "if test_metrics['auc'] >= 0.95:\n",
        "    findings.append(\"ğŸŒŸ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ (AUC â‰¥ 0.95)\")\n",
        "elif test_metrics['auc'] >= 0.90:\n",
        "    findings.append(\"âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ (AUC â‰¥ 0.90)\")\n",
        "elif test_metrics['auc'] >= 0.80:\n",
        "    findings.append(\"ğŸ“ˆ æ¨¡å‹æ€§èƒ½ä¸­ç­‰ (AUC â‰¥ 0.80)\")\n",
        "else:\n",
        "    findings.append(\"âš ï¸ æ¨¡å‹æ€§èƒ½éœ€è¦æ”¹è¿› (AUC < 0.80)\")\n",
        "\n",
        "# æ³›åŒ–èƒ½åŠ›åˆ†æ\n",
        "train_test_auc_diff = train_metrics['auc'] - test_metrics['auc']\n",
        "if train_test_auc_diff <= 0.02:\n",
        "    findings.append(\"ğŸ¯ æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼ºï¼Œæ— è¿‡æ‹Ÿåˆ\")\n",
        "elif train_test_auc_diff <= 0.05:\n",
        "    findings.append(\"âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½\")\n",
        "else:\n",
        "    findings.append(\"âš ï¸ å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ï¼Œå»ºè®®è°ƒæ•´æ¨¡å‹å¤æ‚åº¦\")\n",
        "\n",
        "# åºåˆ—ç‰¹å¾ä»·å€¼åˆ†æ\n",
        "if ENABLE_SEQUENCE_FEATURES and sequence_in_top10 >= 5:\n",
        "    findings.append(\"ğŸš€ åºåˆ—ç‰¹å¾å·¥ç¨‹éå¸¸æœ‰æ•ˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½\")\n",
        "elif ENABLE_SEQUENCE_FEATURES and sequence_in_top10 >= 3:\n",
        "    findings.append(\"ğŸ“Š åºåˆ—ç‰¹å¾å·¥ç¨‹æœ‰æ•ˆï¼Œä¸ºæ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯\")\n",
        "\n",
        "# æ•°æ®è´¨é‡åˆ†æ\n",
        "total_samples = X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
        "if total_samples >= 10000:\n",
        "    findings.append(\"ğŸ“ˆ æ•°æ®æ ·æœ¬å……è¶³ï¼Œæ¨¡å‹è®­ç»ƒç¨³å®š\")\n",
        "elif total_samples >= 5000:\n",
        "    findings.append(\"ğŸ“Š æ•°æ®æ ·æœ¬é€‚ä¸­ï¼Œç»“æœå¯ä¿¡åº¦è‰¯å¥½\")\n",
        "else:\n",
        "    findings.append(\"âš ï¸ æ•°æ®æ ·æœ¬è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æ•°æ®é‡\")\n",
        "\n",
        "for i, finding in enumerate(findings, 1):\n",
        "    print(f\"  {i}. {finding}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ **é¡¹ç›®å®ŒæˆçŠ¶æ€**\")\n",
        "print(\"âœ… æ•°æ®åŠ è½½å’Œç‰¹å¾å·¥ç¨‹å®Œæˆ\")\n",
        "print(\"âœ… æ¨¡å‹è®­ç»ƒå’Œç‰ˆæœ¬å…¼å®¹å¤„ç†å®Œæˆ\")\n",
        "print(\"âœ… æ€§èƒ½è¯„ä¼°å’Œå¯è§†åŒ–å®Œæˆ\")\n",
        "print(\"âœ… ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆ\")\n",
        "print(\"âœ… ç»“æœä¿å­˜å’ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ\")\n",
        "\n",
        "print(f\"\\nğŸ’¼ **åç»­å»ºè®®**\")\n",
        "print(\"1. å¯ä»¥å°è¯•è°ƒæ•´XGBoostè¶…å‚æ•°è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½\")\n",
        "print(\"2. è€ƒè™‘é›†æˆå…¶ä»–ç®—æ³•(å¦‚Random Forest, LightGBM)è¿›è¡Œæ¨¡å‹èåˆ\")\n",
        "print(\"3. æ·±å…¥åˆ†æé”™è¯¯é¢„æµ‹æ¡ˆä¾‹ï¼Œä¼˜åŒ–ç‰¹å¾å·¥ç¨‹\")\n",
        "if not ENABLE_SEQUENCE_FEATURES:\n",
        "    print(\"4. å»ºè®®å°è¯•å¯ç”¨åºåˆ—ç‰¹å¾å·¥ç¨‹ä»¥æå‡æ€§èƒ½\")\n",
        "print(\"5. åœ¨æ›´å¤§çš„æ•°æ®é›†ä¸ŠéªŒè¯æ¨¡å‹çš„ç¨³å®šæ€§\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸŠ PCDN vs Normal Traffic Classification é¡¹ç›®æ‰§è¡Œå®Œæˆï¼\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bysj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
